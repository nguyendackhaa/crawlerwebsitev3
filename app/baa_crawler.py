import os
import shutil
import time
import zipfile
from datetime import datetime
from app.crawler import (
    is_category_url, is_product_url, extract_product_urls, extract_product_info,
    download_baa_product_images_fixed, get_html_content
)
from app import socketio
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import re
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import requests
import traceback
from queue import Queue

def get_category_vn_name(url):
    html = get_html_content(url)
    if html:
        soup = BeautifulSoup(html, 'html.parser')
        # ∆Øu ti√™n l·∫•y t·ª´ breadcrumb
        breadcrumb = soup.select_one('.breadcrumb li.active, .breadcrumb-item.active, .breadcrumb li:last-child, .breadcrumb-item:last-child')
        if breadcrumb and breadcrumb.text.strip():
            return re.sub(r'[\\/:*?"<>|]', '_', breadcrumb.text.strip())
        # Ho·∫∑c l·∫•y t·ª´ th·∫ª h1
        h1 = soup.select_one('h1, .category-title, .page-title')
        if h1 and h1.text.strip():
            return re.sub(r'[\\/:*?"<>|]', '_', h1.text.strip())
    # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c th√¨ fallback sang t√™n t·ª´ URL
    parsed = urlparse(url)
    path = parsed.path.strip('/').split('/')
    if path:
        last = path[-1]
        name = re.sub(r'_\d+$', '', last)
        return name
    return 'danh_muc'

def detect_pagination(url):
    """Ph√°t hi·ªán s·ªë trang t·ªëi ƒëa cho danh m·ª•c"""
    try:
        html = get_html_content(url)
        if not html:
            return 1
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # T√¨m th√†nh ph·∫ßn ph√¢n trang
        pagination = soup.select_one('.pagination, .page-list, nav[aria-label="Page navigation"]')
        if not pagination:
            return 1
        
        # T√¨m s·ªë trang l·ªõn nh·∫•t
        max_page = 1
        page_links = pagination.select('a.page-link')
        
        for link in page_links:
            text = link.get_text(strip=True)
            # Th·ª≠ chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh s·ªë
            try:
                page_num = int(text)
                max_page = max(max_page, page_num)
            except ValueError:
                pass
        
        return max(max_page, 1)
    except Exception as e:
        print(f"L·ªói khi ph√°t hi·ªán s·ªë trang: {str(e)}")
        return 1

def make_pagination_url(base_url, page_number):
    """T·∫°o URL ph√¢n trang cho BAA.vn"""
    if page_number <= 1:
        return base_url
        
    try:
        # Ph√¢n t√≠ch URL g·ªëc
        parsed_url = urlparse(base_url)
        path = parsed_url.path
        query = parsed_url.query
        
        # Ki·ªÉm tra xem URL ƒë√£ c√≥ tham s·ªë page ch∆∞a
        params = {}
        if query:
            for param in query.split('&'):
                if '=' in param:
                    key, value = param.split('=', 1)
                    params[key] = value
        
        # C·∫≠p nh·∫≠t tham s·ªë page
        params['page'] = str(page_number)
        
        # T·∫°o query string m·ªõi
        new_query = '&'.join([f"{key}={value}" for key, value in params.items()])
        
        # T·∫°o URL m·ªõi
        new_url = f"{parsed_url.scheme}://{parsed_url.netloc}{path}?{new_query}"
        return new_url
    except Exception as e:
        print(f"L·ªói khi t·∫°o URL ph√¢n trang: {str(e)}")
        # N·∫øu c√≥ l·ªói, th·ª≠ th√™m ?page=X v√†o cu·ªëi URL
        if '?' in base_url:
            return f"{base_url}&page={page_number}"
        else:
            return f"{base_url}?page={page_number}"

def extract_product_series(url):
    """
    Tr√≠ch xu·∫•t series s·∫£n ph·∫©m t·ª´ URL - CH·ªà √ÅP D·ª§NG CHO M·ªòT DANH M·ª§C C·ª§ TH·ªÇ
    Ch·ªâ th·ª±c hi·ªán ph√¢n lo·∫°i chi ti·∫øt cho: https://baa.vn/vn/Category/cong-tac-den-bao-coi-bao-qlight_F_782/
    C√°c URL kh√°c s·∫Ω tr·∫£ v·ªÅ None (kh√¥ng ph√¢n lo·∫°i series)
    """
    if not url:
        return None
    
    # CH·ªà √°p d·ª•ng logic ph√¢n lo·∫°i chi ti·∫øt cho link danh m·ª•c QLIGHT c·ª• th·ªÉ
    target_url = "https://baa.vn/vn/Category/cong-tac-den-bao-coi-bao-qlight_F_782/"
    if url.strip().rstrip('/') != target_url.strip().rstrip('/'):
        return None
    
    # Logic ph√¢n lo·∫°i chi ti·∫øt CH·ªà cho URL QLIGHT
    print(f"üîç ƒêang ph√¢n lo·∫°i series cho URL QLIGHT: {url}")
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Danh s√°ch c√°c brands ƒë∆∞·ª£c h·ªó tr·ª£
        supported_brands = [
            'qlight', 'autonics', 'sick', 'omron', 'keyence', 'pepperl', 'fuchs',
            'balluff', 'turck', 'banner', 'contrinex', 'schneider', 'siemens', 
            'abb', 'mitsubishi', 'panasonic', 'hanyoung', 'fotek', 'idec',
            'phoenix', 'weidmuller', 'pilz', 'ifm', 'leuze', 'wenglor',
            'baumer', 'datalogic', 'cognex', 'keyence', 'festo'
        ]
        
        # Method 1: Ki·ªÉm tra HTML structure cho s·∫£n ph·∫©m ƒë∆°n l·∫ª
        product_symbol = soup.find('div', class_='product__symbol-header')
        if product_symbol:
            text = product_symbol.get_text(strip=True).upper()
            for brand in supported_brands:
                if brand.upper() in text:
                    # T√¨m series pattern sau brand name
                    import re
                    pattern = rf'{brand.upper()}[\s\-]*([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
                    match = re.search(pattern, text)
                    if match:
                        series = match.group(1)
                        if len(series) >= 2:
                            print(f"‚úÖ Method 1 - T√¨m th·∫•y series t·ª´ product symbol: {series}_series")
                            return f"{series}_series"
            
        # Method 2: Tr√≠ch xu·∫•t t·ª´ ti√™u ƒë·ªÅ h1
        h1 = soup.find('h1')
        if h1:
            title = h1.get_text(strip=True).upper()
            # Pattern 1: "BRAND MODEL series" format
            for brand in supported_brands:
                if brand.upper() in title:
                    import re
                    # T√¨m pattern sau brand name
                    pattern = rf'{brand.upper()}[\s\-]*([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
                    match = re.search(pattern, title)
                    if match:
                        series = match.group(1)
                        if len(series) >= 2 and not series.isdigit():
                            print(f"‚úÖ Method 2a - T√¨m th·∫•y series t·ª´ h1 title: {series}_series")
                            return f"{series}_series"
                    
                    # Pattern 2: T√¨m series v·ªõi "SERIES" keyword
                    series_pattern = r'(\w+)[\s\-]*SERIES'
                    series_match = re.search(series_pattern, title)
                    if series_match:
                        series = series_match.group(1)
                        if len(series) >= 2:
                            print(f"‚úÖ Method 2b - T√¨m th·∫•y series t·ª´ 'SERIES' keyword: {series}_series")
                            return f"{series}_series"
            
        # Method 3: Tr√≠ch xu·∫•t t·ª´ breadcrumb
        breadcrumb = soup.find('nav', class_='breadcrumb') or soup.find('ol', class_='breadcrumb')
        if breadcrumb:
            breadcrumb_text = breadcrumb.get_text(strip=True).upper()
            for brand in supported_brands:
                if brand.upper() in breadcrumb_text:
                    import re
                    pattern = rf'{brand.upper()}[\s\-]*([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
                    match = re.search(pattern, breadcrumb_text)
                    if match:
                        series = match.group(1)
                        if len(series) >= 2:
                            print(f"‚úÖ Method 3 - T√¨m th·∫•y series t·ª´ breadcrumb: {series}_series")
                            return f"{series}_series"
        
        # Method 4: Tr√≠ch xu·∫•t t·ª´ URL patterns
        import re
        url_upper = url.upper()
        
        # Pattern 1: /brand-series_number/ ho·∫∑c /brand-series/
        for brand in supported_brands:
            pattern1 = rf'/{brand.upper()}-([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)(?:_|\b)'
            match1 = re.search(pattern1, url_upper)
            if match1:
                series = match1.group(1)
                if len(series) >= 2:
                    print(f"‚úÖ Method 4a - T√¨m th·∫•y series t·ª´ URL pattern 1: {series}_series")
                    return f"{series}_series"
            
            # Pattern 2: brand_series ho·∫∑c brand-series
            pattern2 = rf'{brand.upper()}[_\-]([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
            match2 = re.search(pattern2, url_upper)
            if match2:
                series = match2.group(1)
                if len(series) >= 2 and not series.isdigit():
                    print(f"‚úÖ Method 4b - T√¨m th·∫•y series t·ª´ URL pattern 2: {series}_series")
                    return f"{series}_series"
            
            # Pattern 3: series-brand format
            pattern3 = rf'([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)-{brand.upper()}'
            match3 = re.search(pattern3, url_upper)
            if match3:
                series = match3.group(1)
                if len(series) >= 2:
                    print(f"‚úÖ Method 4c - T√¨m th·∫•y series t·ª´ URL pattern 3: {series}_series")
                    return f"{series}_series"
        
        # Method 5: Ki·ªÉm tra meta tags
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            content = meta.get('content', '').upper()
            if content:
                for brand in supported_brands:
                    if brand.upper() in content:
                        import re
                        pattern = rf'{brand.upper()}[\s\-]*([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
                        match = re.search(pattern, content)
                        if match:
                            series = match.group(1)
                            if len(series) >= 2:
                                print(f"‚úÖ Method 5 - T√¨m th·∫•y series t·ª´ meta tags: {series}_series")
                                return f"{series}_series"
        
        # Method 6: Fallback - t√¨m trong to√†n b·ªô page content
        page_text = soup.get_text().upper()
        series_candidates = {}
        
        for brand in supported_brands:
            if brand.upper() in page_text:
                import re
                pattern = rf'{brand.upper()}[\s\-]*([A-Z0-9]+(?:[A-Z0-9\-]*[A-Z0-9])?)'
                matches = re.findall(pattern, page_text)
                for match in matches:
                    if len(match) >= 2 and not match.isdigit():
                        series_candidates[match] = series_candidates.get(match, 0) + 1
        
        if series_candidates:
            # Ch·ªçn series xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
            most_common_series = max(series_candidates.items(), key=lambda x: x[1])
            series = most_common_series[0]
            print(f"‚úÖ Method 6 - T√¨m th·∫•y series t·ª´ page content (frequency: {most_common_series[1]}): {series}_series")
            return f"{series}_series"
        
        # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát cho URL QLIGHT
        if 'QLIGHT' in url_upper or 'qlight' in url.lower():
            print("‚úÖ Fallback - Ph√°t hi·ªán QLIGHT trong URL: QLIGHT_series")
            return "QLIGHT_series"
            
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t series: {e}")
    
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y series c·ª• th·ªÉ cho URL ƒë·∫∑c bi·ªát n√†y")
    return None

def sanitize_folder_name(name):
    """
    L√†m s·∫°ch t√™n th∆∞ m·ª•c ƒë·ªÉ tr√°nh k√Ω t·ª± kh√¥ng h·ª£p l·ªá
    
    Args:
        name (str): T√™n c·∫ßn l√†m s·∫°ch
        
    Returns:
        str: T√™n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
    """
    if not name:
        return 'Khac'
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá cho t√™n th∆∞ m·ª•c
    clean_name = re.sub(r'[\\/:*?"<>|]', '_', name)
    clean_name = re.sub(r'\s+', '_', clean_name)
    clean_name = clean_name.strip('_.')
    
    return clean_name if clean_name else 'Khac'

class BaaProductCrawler:
    def __init__(self, output_root=None, max_workers=8, max_retries=3):
        self.output_root = output_root or os.path.join(os.getcwd(), "output_baa")
        self.max_workers = max_workers
        self.max_retries = max_retries
        os.makedirs(self.output_root, exist_ok=True)

    def crawl_products(self, input_urls):
        """
        Nh·∫≠n v√†o danh s√°ch URL (danh m·ª•c ho·∫∑c s·∫£n ph·∫©m), tr·∫£ v·ªÅ tuple (list dict s·∫£n ph·∫©m, ƒë∆∞·ªùng d·∫´n folder k·∫øt qu·∫£)
        C·∫£i ti·∫øn v·ªõi m√¥ h√¨nh ƒëa lu·ªìng theo ch·ª©c nƒÉng:
        - Lu·ªìng 1: Ph√¢n t√≠ch v√† ph√¢n lo·∫°i URL
        - Lu·ªìng 2: Thu th·∫≠p URL s·∫£n ph·∫©m
        - Lu·ªìng 3: X·ª≠ l√Ω th√¥ng tin s·∫£n ph·∫©m
        - Lu·ªìng 4: T·∫£i ·∫£nh s·∫£n ph·∫©m
        """
        # ƒêo th·ªùi gian th·ª±c hi·ªán
        start_time = time.time()
        
        # Th·ªëng k√™ hi·ªáu su·∫•t
        stats = {
            "urls_processed": 0,
            "products_found": 0,
            "products_processed": 0,
            "products_skipped": 0,  # S·∫£n ph·∫©m b·ªè qua v√¨ kh√¥ng c√≥ gi√°
            "images_downloaded": 0,
            "categories": 0,
            "single_products": 0,
            "failed_products": 0,
            "failed_images": 0
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = os.path.join(self.output_root, f"Baa_ngay{timestamp}")
        os.makedirs(result_dir, exist_ok=True)
        all_products = []
        category_folders = []
        
        # Thu th·∫≠p d·ªØ li·ªáu b√°o c√°o ·∫£nh t·ª´ t·∫•t c·∫£ danh m·ª•c
        all_image_report_data = []
        
        # Th√¥ng b√°o b·∫Øt ƒë·∫ßu
        socketio.emit('progress_update', {
            'percent': 0, 
            'message': f'B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(input_urls)} URL t·ª´ BAA.vn',
            'detail': 'ƒêang ph√¢n t√≠ch v√† ph√¢n lo·∫°i c√°c URL...'
        })
        
        # S·ª≠ d·ª•ng Queue ƒë·ªÉ truy·ªÅn d·ªØ li·ªáu gi·ªØa c√°c lu·ªìng x·ª≠ l√Ω
        category_queue = Queue()  # H√†ng ƒë·ª£i cho c√°c danh m·ª•c ƒë√£ ph√¢n lo·∫°i
        product_url_queue = Queue()  # H√†ng ƒë·ª£i cho c√°c URL s·∫£n ph·∫©m t·ª´ danh m·ª•c
        
        # 1. Ph√¢n lo·∫°i URL v√† t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
        def classify_urls_worker():
            """Ph√¢n lo·∫°i URL th√†nh danh m·ª•c v√† s·∫£n ph·∫©m ƒë∆°n l·∫ª"""
            category_map = {}  # L∆∞u √°nh x·∫° t√™n danh m·ª•c -> danh s√°ch URL
            single_products = []  # L∆∞u danh s√°ch URL s·∫£n ph·∫©m ƒë∆°n l·∫ª
            
            for i, url in enumerate(input_urls):
                try:
                    stats["urls_processed"] += 1
                    if is_category_url(url):
                        cat_name = get_category_vn_name(url)
                        if cat_name not in category_map:
                            category_map[cat_name] = []
                            stats["categories"] += 1
                        category_map[cat_name].append(url)
                        socketio.emit('progress_update', {
                            'percent': 2 * i // len(input_urls), 
                            'message': f'ƒêang ph√¢n t√≠ch URL {i+1}/{len(input_urls)}',
                            'detail': f'ƒê√£ x√°c ƒë·ªãnh danh m·ª•c: {cat_name}'
                        })
                    elif is_product_url(url):
                        single_products.append(url)
                        stats["single_products"] += 1
                        socketio.emit('progress_update', {
                            'percent': 2 * i // len(input_urls), 
                            'message': f'ƒêang ph√¢n t√≠ch URL {i+1}/{len(input_urls)}',
                            'detail': f'ƒê√£ x√°c ƒë·ªãnh s·∫£n ph·∫©m ƒë∆°n l·∫ª'
                        })
                except Exception as e:
                    print(f"L·ªói khi ph√¢n t√≠ch URL {url}: {str(e)}")
            
            # ƒê·∫∑t c√°c danh m·ª•c v√†o h√†ng ƒë·ª£i
            for cat_name, cat_urls in category_map.items():
                category_queue.put((cat_name, cat_urls))
            
            # ƒê·∫∑t s·∫£n ph·∫©m ƒë∆°n l·∫ª v√†o h√†ng ƒë·ª£i n·∫øu c√≥
            if single_products:
                category_queue.put(('san_pham_le', single_products))
            
            return category_map, single_products
        
        # 2. T·∫°o c√°c th∆∞ m·ª•c b√°o c√°o
        report_dir = os.path.join(result_dir, "Bao_cao")
        os.makedirs(report_dir, exist_ok=True)
        
        # Th·ªëng k√™ t·ªïng s·ªë l∆∞·ª£ng v√† thi·∫øt l·∫≠p
        category_map, single_products = classify_urls_worker()
        total_categories = len(category_map)
        total_steps = total_categories + (1 if single_products else 0)
        
        # T·∫°o c√°c th∆∞ m·ª•c danh m·ª•c tr∆∞·ªõc
        for cat_name in category_map.keys():
            cat_dir = os.path.join(result_dir, cat_name)
            anh_dir = os.path.join(cat_dir, "Anh")
            os.makedirs(anh_dir, exist_ok=True)
            category_folders.append(cat_dir)
        
        # N·∫øu c√≥ s·∫£n ph·∫©m ƒë∆°n l·∫ª, t·∫°o th∆∞ m·ª•c ri√™ng
        if single_products:
            le_dir = os.path.join(result_dir, 'san_pham_le')
            anh_dir = os.path.join(le_dir, "Anh")
            os.makedirs(anh_dir, exist_ok=True)
            category_folders.append(le_dir)
        
        # 3. X·ª≠ l√Ω t·ª´ng danh m·ª•c v·ªõi m√¥ h√¨nh lu·ªìng theo ch·ª©c nƒÉng
        for cat_idx, (cat_name, cat_urls) in enumerate(category_map.items()):
            current_step = cat_idx + 1
            step_progress_base = 5 + (current_step * 90 // max(1, total_steps))
            
            cat_dir = os.path.join(result_dir, cat_name)
            anh_dir = os.path.join(cat_dir, "Anh")
            
            socketio.emit('progress_update', {
                'percent': step_progress_base, 
                'message': f'ƒêang x·ª≠ l√Ω danh m·ª•c [{current_step}/{total_steps}]: {cat_name}',
                'detail': f'T·∫°o th∆∞ m·ª•c v√† chu·∫©n b·ªã c√†o d·ªØ li·ªáu ({len(cat_urls)} URL ngu·ªìn)'
            })
            
            # Thu th·∫≠p URL s·∫£n ph·∫©m t·ª´ c√°c danh m·ª•c, bao g·ªìm x·ª≠ l√Ω ph√¢n trang
            # S·ª≠ d·ª•ng thread ri√™ng ƒë·ªÉ kh√¥ng ch·∫∑n lu·ªìng ch√≠nh
            product_urls_queue = Queue()  # H√†ng ƒë·ª£i l∆∞u URL s·∫£n ph·∫©m c·ªßa danh m·ª•c hi·ªán t·∫°i
            
            def collect_product_urls_thread():
                """Thread thu th·∫≠p URL s·∫£n ph·∫©m t·ª´ danh m·ª•c v√† ƒë·∫∑t v√†o h√†ng ƒë·ª£i"""
                product_urls = self._collect_product_urls_with_pagination(cat_urls)
                product_urls = list(dict.fromkeys(product_urls))  # Lo·∫°i b·ªè tr√πng l·∫∑p
                stats["products_found"] += len(product_urls)
                
                # ƒê·∫∑t m·ªói URL v√†o h√†ng ƒë·ª£i ƒë·ªÉ x·ª≠ l√Ω
                for url in product_urls:
                    product_urls_queue.put(url)
                
                # ƒê·∫∑t None ƒë·ªÉ b√°o hi·ªáu ƒë√£ h·∫øt URL
                product_urls_queue.put(None)
                
                # Ghi log chi ti·∫øt
                print(f"[{cat_name}] ƒê√£ thu th·∫≠p {len(product_urls)} URL s·∫£n ph·∫©m t·ª´ {len(cat_urls)} URL danh m·ª•c")
                return product_urls
            
            # Kh·ªüi ch·∫°y thread thu th·∫≠p URL
            from threading import Thread
            url_collector_thread = Thread(target=collect_product_urls_thread)
            url_collector_thread.start()
            url_collector_thread.join()  # ƒê·ª£i thread ho√†n th√†nh
            
            # L·∫•y danh s√°ch URL ƒë·ªÉ l∆∞u v√†o file v√† ti·∫øp t·ª•c x·ª≠ l√Ω
            # Sao ch√©p t·∫•t c·∫£ URL t·ª´ Queue (ƒë√£ c√≥ None ·ªü cu·ªëi)
            product_urls = []
            while True:
                url = product_urls_queue.get()
                if url is None:
                    break
                product_urls.append(url)
                product_urls_queue.put(url)  # ƒê·∫∑t l·∫°i URL ƒë·ªÉ x·ª≠ l√Ω s·∫£n ph·∫©m
            product_urls_queue.put(None)  # ƒê·∫∑t l·∫°i None v√†o cu·ªëi
            
            socketio.emit('progress_update', {
                'percent': step_progress_base + 2, 
                'message': f'[{cat_name}] ƒê√£ thu th·∫≠p {len(product_urls)} li√™n k·∫øt s·∫£n ph·∫©m',
                'detail': f'Chu·∫©n b·ªã tr√≠ch xu·∫•t th√¥ng tin v√† t·∫£i ·∫£nh s·∫£n ph·∫©m'
            })
            
            # L∆∞u danh s√°ch URL s·∫£n ph·∫©m
            urls_file = os.path.join(cat_dir, f"{cat_name}_urls.txt")
            with open(urls_file, 'w', encoding='utf-8') as f:
                for url in product_urls:
                    f.write(f"{url}\n")
            
            # T·∫°o file Du_lieu.xlsx
            data_excel = os.path.join(cat_dir, "Du_lieu.xlsx")
            
            # H√†ng ƒë·ª£i l∆∞u th√¥ng tin s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω v√† code-url ƒë·ªÉ t·∫£i ·∫£nh
            product_info_queue = Queue()
            image_task_queue = Queue()
            
            # Thread x·ª≠ l√Ω th√¥ng tin s·∫£n ph·∫©m
            def process_product_info_thread():
                """Thread x·ª≠ l√Ω th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m t·ª´ URL"""
                products = []
                code_url_map = {}
                series_products_map = {}  # Nh√≥m s·∫£n ph·∫©m theo series
                required_fields = ['STT', 'M√£ s·∫£n ph·∫©m', 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'T·ªïng quan', 'URL']
                batch_size = min(20, max(1, len(product_urls) // 5))
                
                # Theo d√µi ti·∫øn ƒë·ªô trong thread n√†y
                items_processed = 0
                batch_start_time = time.time()
                batch_success = 0
                batch_failure = 0
                batch_skipped = 0  # S·ªë s·∫£n ph·∫©m b·ªè qua v√¨ kh√¥ng c√≥ gi√°
                
                # Ph·∫ßn trƒÉm ti·∫øn ƒë·ªô cho batch n√†y
                batch_percent_start = step_progress_base + 5
                batch_percent_range = 30 // max(1, total_steps)
                
                # X·ª≠ l√Ω ƒëa lu·ªìng tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = {}
                    
                    # L·∫•y URL t·ª´ h√†ng ƒë·ª£i v√† x·ª≠ l√Ω
                    url_index = 0
                    while True:
                        url = product_urls_queue.get()
                        if url is None:
                            break
                        
                        future = executor.submit(extract_product_info, url, required_fields, url_index + 1)
                        futures[future] = url
                        url_index += 1
                    
                    # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
                    for future in as_completed(futures):
                        url = futures[future]  # L·∫•y URL t·ª´ mapping
                        try:
                            info = future.result()
                            if info:
                                # Tr√≠ch xu·∫•t th√¥ng tin series t·ª´ URL s·∫£n ph·∫©m
                                try:
                                    product_series = extract_product_series(url)
                                    # Ch·ªâ th√™m field Series n·∫øu c√≥ gi√° tr·ªã (kh√¥ng ph·∫£i None)
                                    if product_series:
                                        info['Series'] = product_series
                                        print(f"[{cat_name}] S·∫£n ph·∫©m {info.get('M√£ s·∫£n ph·∫©m', 'N/A')} thu·ªôc series: {product_series}")
                                    else:
                                        # Kh√¥ng th√™m field Series cho c√°c URL kh√¥ng h·ªó tr·ª£ ph√¢n lo·∫°i
                                        print(f"[{cat_name}] S·∫£n ph·∫©m {info.get('M√£ s·∫£n ph·∫©m', 'N/A')} kh√¥ng ƒë∆∞·ª£c ph√¢n lo·∫°i theo series")
                                except Exception as e:
                                    print(f"[{cat_name}] L·ªói khi tr√≠ch xu·∫•t series cho {url}: {str(e)}")
                                    # Kh√¥ng th√™m field Series khi c√≥ l·ªói
                                
                                # Ki·ªÉm tra xem s·∫£n ph·∫©m c√≥ gi√° kh√¥ng ƒë·ªÉ th·ªëng k√™
                                product_price = info.get('Gi√°', '').strip()
                                
                                # Chu·∫©n h√≥a th√¥ng s·ªë k·ªπ thu·∫≠t
                                info['T·ªïng quan'] = self._normalize_spec(info.get('T·ªïng quan', ''))
                                products.append(info)
                                
                                # Nh√≥m s·∫£n ph·∫©m theo series (ch·ªâ khi c√≥ Series)
                                series_name = info.get('Series')
                                if series_name:  # Ch·ªâ nh√≥m khi c√≥ series
                                    if series_name not in series_products_map:
                                        series_products_map[series_name] = []
                                    series_products_map[series_name].append(info)
                                
                                # L∆∞u m√£ s·∫£n ph·∫©m v√† URL ƒë·ªÉ t·∫£i ·∫£nh (nh√≥m theo series n·∫øu c√≥)
                                if info.get('M√£ s·∫£n ph·∫©m') and info.get('URL'):
                                    code_url_map[info['M√£ s·∫£n ph·∫©m']] = {
                                        'url': info['URL'],
                                        'series': series_name if series_name else None
                                    }
                                
                                if not product_price or product_price == '':
                                    # Th·ªëng k√™ s·∫£n ph·∫©m kh√¥ng c√≥ gi√° nh∆∞ng v·∫´n x·ª≠ l√Ω
                                    batch_skipped += 1
                                    stats["products_skipped"] += 1
                                    print(f"[{cat_name}] S·∫£n ph·∫©m kh√¥ng c√≥ gi√° (v·∫´n l∆∞u th√¥ng tin): {info.get('T√™n s·∫£n ph·∫©m', 'N/A')}")
                                else:
                                    batch_success += 1
                                
                                stats["products_processed"] += 1
                                
                                # Th√™m v√†o h√†ng ƒë·ª£i th√¥ng tin s·∫£n ph·∫©m
                                product_info_queue.put(info)
                            else:
                                batch_failure += 1
                                stats["failed_products"] += 1
                                print(f"[{cat_name}] Kh√¥ng th·ªÉ tr√≠ch xu·∫•t th√¥ng tin t·ª´ {url}")
                        except Exception as e:
                            batch_failure += 1
                            stats["failed_products"] += 1
                            print(f"[{cat_name}] L·ªói khi tr√≠ch xu·∫•t: {str(e)}")
                        
                        # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                        items_processed += 1
                        
                        # T√≠nh ti·∫øn ƒë·ªô chi ti·∫øt h∆°n
                        batch_progress = batch_percent_start + (items_processed * batch_percent_range // len(product_urls))
                        
                        # Hi·ªÉn th·ªã th√¥ng tin ti·∫øn ƒë·ªô
                        if items_processed % 5 == 0 or items_processed == len(product_urls):
                            # T√≠nh t·ªëc ƒë·ªô x·ª≠ l√Ω
                            elapsed = time.time() - batch_start_time
                            speed = items_processed / elapsed if elapsed > 0 else 0
                            remaining = (len(product_urls) - items_processed) / speed if speed > 0 else 0
                            
                            # Format th·ªùi gian c√≤n l·∫°i
                            remaining_info = ""
                            if remaining > 0:
                                if remaining < 60:
                                    remaining_info = f", c√≤n l·∫°i: {remaining:.1f}s"
                                else:
                                    remaining_info = f", c√≤n l·∫°i: {remaining/60:.1f}m"
                        
                            socketio.emit('progress_update', {
                                'percent': batch_progress, 
                                'message': f'[{cat_name}] ƒê√£ x·ª≠ l√Ω {items_processed}/{len(product_urls)} s·∫£n ph·∫©m ({batch_success} c√≥ gi√°, {batch_skipped} kh√¥ng c√≥ gi√°, {batch_failure} l·ªói)',
                                'detail': f'T·ªëc ƒë·ªô: {speed:.1f} sp/s{remaining_info}, ƒë√£ ph√°t hi·ªán {len(series_products_map)} series'
                            })
                
                # ƒê·∫∑t None v√†o cu·ªëi h√†ng ƒë·ª£i ƒë·ªÉ b√°o hi·ªáu ƒë√£ ho√†n th√†nh
                product_info_queue.put(None)
                
                # ƒê·∫∑t code_url_map v√† series_products_map v√†o image_task_queue ƒë·ªÉ t·∫£i ·∫£nh
                image_task_queue.put((code_url_map, series_products_map, anh_dir, cat_name, cat_idx, total_categories, step_progress_base + 40))
                
                print(f"[{cat_name}] K·∫øt qu·∫£ x·ª≠ l√Ω: {batch_success} s·∫£n ph·∫©m c√≥ gi√°, {batch_skipped} s·∫£n ph·∫©m kh√¥ng c√≥ gi√°, {batch_failure} l·ªói")
                print(f"[{cat_name}] ƒê√£ ph√°t hi·ªán {len(series_products_map)} series: {list(series_products_map.keys())}")
                
                return products, code_url_map, series_products_map
            
            # Thread l∆∞u th√¥ng tin s·∫£n ph·∫©m v√†o file Excel
            def save_product_info_thread():
                """Thread l∆∞u th√¥ng tin s·∫£n ph·∫©m v√†o file Excel theo series"""
                products = []
                series_products_map = {}  # Thu th·∫≠p s·∫£n ph·∫©m theo series
                
                # L·∫•y th√¥ng tin s·∫£n ph·∫©m t·ª´ h√†ng ƒë·ª£i
                while True:
                    info = product_info_queue.get()
                    if info is None:
                        break
                    products.append(info)
                    
                    # Nh√≥m s·∫£n ph·∫©m theo series (ch·ªâ khi c√≥ Series)
                    series_name = info.get('Series')
                    if series_name:  # Ch·ªâ nh√≥m khi c√≥ series
                        if series_name not in series_products_map:
                            series_products_map[series_name] = []
                        series_products_map[series_name].append(info)
                
                # T·∫°o th∆∞ m·ª•c cho t·ª´ng series v√† l∆∞u d·ªØ li·ªáu (ch·ªâ khi c√≥ series)
                if series_products_map:  # Ch·ªâ t·∫°o khi c√≥ series
                    for series_name, series_products in series_products_map.items():
                        # T·∫°o th∆∞ m·ª•c series
                        series_folder_name = sanitize_folder_name(series_name)
                        series_dir = os.path.join(cat_dir, series_folder_name)
                        series_anh_dir = os.path.join(series_dir, "Anh")
                        os.makedirs(series_anh_dir, exist_ok=True)
                        
                        # L∆∞u d·ªØ li·ªáu series v√†o file Excel ri√™ng
                        series_excel_file = os.path.join(series_dir, f"{series_folder_name}_Du_lieu.xlsx")
                        if series_products:
                            df = pd.DataFrame(series_products)
                            df.to_excel(series_excel_file, index=False)
                            print(f"[{cat_name}] ƒê√£ l∆∞u d·ªØ li·ªáu series '{series_name}': {len(series_products)} s·∫£n ph·∫©m v√†o {series_excel_file}")
                
                # L∆∞u file t·ªïng h·ª£p t·∫•t c·∫£ s·∫£n ph·∫©m (gi·ªØ nguy√™n ch·ª©c nƒÉng c≈©)
                if products:
                    df = pd.DataFrame(products)
                    df.to_excel(data_excel, index=False)
                    print(f"[{cat_name}] ƒê√£ l∆∞u d·ªØ li·ªáu t·ªïng h·ª£p: {len(products)} s·∫£n ph·∫©m")
                
                # Th√™m c√°c s·∫£n ph·∫©m v√†o danh s√°ch t·ªïng h·ª£p
                all_products.extend(products)
                
                if series_products_map:
                    print(f"[{cat_name}] ƒê√£ ph√¢n lo·∫°i {len(products)} s·∫£n ph·∫©m v√†o {len(series_products_map)} series")
                else:
                    print(f"[{cat_name}] Kh√¥ng c√≥ series n√†o ƒë∆∞·ª£c ph√°t hi·ªán, ch·ªâ l∆∞u d·ªØ li·ªáu t·ªïng h·ª£p")
                return products, series_products_map
            
            # Thread t·∫£i ·∫£nh s·∫£n ph·∫©m
            def download_images_thread():
                """Thread t·∫£i ·∫£nh s·∫£n ph·∫©m"""
                # L·∫•y th√¥ng tin t·∫£i ·∫£nh t·ª´ h√†ng ƒë·ª£i
                code_url_map, series_products_map, anh_dir, cat_name, cat_idx, total_categories, percent_base = image_task_queue.get()
                
                # T·∫£i ·∫£nh s·∫£n ph·∫©m
                img_map, image_report_data = self._download_product_images(code_url_map, series_products_map, anh_dir, cat_name, cat_idx, total_categories, percent_base)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                stats["images_downloaded"] += len(img_map)
                stats["failed_images"] += len(code_url_map) - len(img_map)
                
                # Thu th·∫≠p d·ªØ li·ªáu b√°o c√°o ·∫£nh ƒë·ªÉ h·ª£p nh·∫•t sau n√†y
                nonlocal all_image_report_data
                all_image_report_data.extend(image_report_data)
                
                return img_map
            
            # Kh·ªüi ch·∫°y c√°c thread x·ª≠ l√Ω
            product_processor_thread = Thread(target=process_product_info_thread)
            product_saver_thread = Thread(target=save_product_info_thread)
            image_downloader_thread = Thread(target=download_images_thread)
            
            # B·∫Øt ƒë·∫ßu thread x·ª≠ l√Ω s·∫£n ph·∫©m
            product_processor_thread.start()
            
            # B·∫Øt ƒë·∫ßu thread l∆∞u th√¥ng tin
            product_saver_thread.start()
            
            # B·∫Øt ƒë·∫ßu thread t·∫£i ·∫£nh
            image_downloader_thread.start()
            
            # ƒê·ª£i t·∫•t c·∫£ c√°c thread ho√†n th√†nh
            product_processor_thread.join()
            product_saver_thread.join()
            image_downloader_thread.join()
            
            # Th√¥ng b√°o ho√†n th√†nh danh m·ª•c
            socketio.emit('progress_update', {
                'percent': step_progress_base + 65, 
                'message': f'[{cat_name}] ƒê√£ ho√†n th√†nh x·ª≠ l√Ω danh m·ª•c',
                'detail': f'ƒê√£ x·ª≠ l√Ω {len(product_urls)} s·∫£n ph·∫©m'
            })
        
        # T·∫°o file t·ªïng h·ª£p ngo√†i c√πng v·ªõi nhi·ªÅu sheet
        report_path = os.path.join(result_dir, 'Bao_cao_tong_hop.xlsx')
        
        # Thu th·∫≠p th·ªëng k√™ series t·ª´ t·∫•t c·∫£ s·∫£n ph·∫©m
        series_stats = {}
        for product in all_products:
            series_name = product.get('Series')
            # Ch·ªâ th·ªëng k√™ khi c√≥ series
            if series_name:
                if series_name not in series_stats:
                    series_stats[series_name] = {
                        'So_luong': 0,
                        'Co_gia': 0,
                        'Khong_gia': 0,
                        'Danh_muc': set()
                    }
                
                series_stats[series_name]['So_luong'] += 1
                
                # Th·ªëng k√™ theo gi√°
                product_price = product.get('Gi√°', '').strip()
                if product_price:
                    series_stats[series_name]['Co_gia'] += 1
                else:
                    series_stats[series_name]['Khong_gia'] += 1
                
                # Thu th·∫≠p danh m·ª•c ch·ª©a series n√†y
                # C√≥ th·ªÉ l·∫•y t·ª´ URL ho·∫∑c th√¥ng tin kh√°c
                product_url = product.get('URL', '')
                if product_url:
                    # Tr√≠ch xu·∫•t t√™n danh m·ª•c t·ª´ URL ho·∫∑c context
                    for cat_name in category_map.keys():
                        series_stats[series_name]['Danh_muc'].add(cat_name)
                        break  # Ch·ªâ c·∫ßn 1 danh m·ª•c ƒë·∫°i di·ªán
        
        # Chuy·ªÉn ƒë·ªïi set th√†nh string cho vi·ªác l∆∞u tr·ªØ
        series_summary_data = []
        if series_stats:  # Ch·ªâ t·∫°o khi c√≥ series
            for series_name, stats in series_stats.items():
                series_summary_data.append({
                    'Series': series_name,
                    'T·ªïng s·ªë s·∫£n ph·∫©m': stats['So_luong'],
                    'S·∫£n ph·∫©m c√≥ gi√°': stats['Co_gia'],
                    'S·∫£n ph·∫©m kh√¥ng c√≥ gi√°': stats['Khong_gia'],
                    'T·ª∑ l·ªá c√≥ gi√° (%)': f"{(stats['Co_gia'] * 100 / stats['So_luong']):.1f}" if stats['So_luong'] > 0 else "0.0",
                    'C√°c danh m·ª•c': ', '.join(stats['Danh_muc']) if stats['Danh_muc'] else 'N/A'
                })
        
        # T·∫°o m·ªôt ExcelWriter ƒë·ªÉ ghi nhi·ªÅu sheet v√†o c√πng m·ªôt file Excel
        with pd.ExcelWriter(report_path, engine='openpyxl') as writer:
            # Sheet d·ªØ li·ªáu s·∫£n ph·∫©m
            if all_products:
                df = pd.DataFrame(all_products)
                df.to_excel(writer, sheet_name='Du_lieu_san_pham', index=False)
            
            # Sheet th·ªëng k√™ series (ch·ªâ t·∫°o khi c√≥ series)
            if series_summary_data:
                series_df = pd.DataFrame(series_summary_data)
                # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng s·∫£n ph·∫©m gi·∫£m d·∫ßn
                series_df = series_df.sort_values('T·ªïng s·ªë s·∫£n ph·∫©m', ascending=False)
                series_df.to_excel(writer, sheet_name='Thong_ke_series', index=False)
            
            # Sheet th·ªëng k√™ t·ªïng quan
            stats_data = [
                {
                    'Ch·ªâ s·ªë': 'T·ªïng s·ªë URL x·ª≠ l√Ω',
                    'Gi√° tr·ªã': stats["urls_processed"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë danh m·ª•c',
                    'Gi√° tr·ªã': stats["categories"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë series ph√°t hi·ªán',
                    'Gi√° tr·ªã': len(series_stats)
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë s·∫£n ph·∫©m ƒë∆°n l·∫ª',
                    'Gi√° tr·ªã': stats["single_products"]
                },
                {
                    'Ch·ªâ s·ªë': 'T·ªïng s·ªë s·∫£n ph·∫©m t√¨m th·∫•y',
                    'Gi√° tr·ªã': stats["products_found"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë s·∫£n ph·∫©m x·ª≠ l√Ω th√†nh c√¥ng',
                    'Gi√° tr·ªã': stats["products_processed"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë s·∫£n ph·∫©m kh√¥ng c√≥ gi√°',
                    'Gi√° tr·ªã': stats["products_skipped"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë s·∫£n ph·∫©m l·ªói',
                    'Gi√° tr·ªã': stats["failed_products"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë ·∫£nh t·∫£i th√†nh c√¥ng',
                    'Gi√° tr·ªã': stats["images_downloaded"]
                },
                {
                    'Ch·ªâ s·ªë': 'S·ªë ·∫£nh t·∫£i th·∫•t b·∫°i',
                    'Gi√° tr·ªã': stats["failed_images"]
                },
                {
                    'Ch·ªâ s·ªë': 'T·ª∑ l·ªá th√†nh c√¥ng s·∫£n ph·∫©m',
                    'Gi√° tr·ªã': f"{stats['products_processed'] * 100 / max(1, stats['products_found']):.2f}%"
                },
                {
                    'Ch·ªâ s·ªë': 'T·ª∑ l·ªá th√†nh c√¥ng ·∫£nh',
                    'Gi√° tr·ªã': f"{stats['images_downloaded'] * 100 / max(1, stats['products_processed']):.2f}%"
                },
                {
                    'Ch·ªâ s·ªë': 'Th·ªùi gian x·ª≠ l√Ω',
                    'Gi√° tr·ªã': f"{(time.time() - start_time) / 60:.2f} ph√∫t"
                }
            ]
            stats_df = pd.DataFrame(stats_data)
            stats_df.to_excel(writer, sheet_name='Thong_ke_tong_quan', index=False)
            
            # Sheet b√°o c√°o t·∫£i ·∫£nh
            if all_image_report_data:
                image_df = pd.DataFrame(all_image_report_data)
                # S·∫Øp x·∫øp theo series v√† m√£ s·∫£n ph·∫©m (x·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ series)
                image_df = image_df.sort_values(['Series', 'M√£ s·∫£n ph·∫©m'], ascending=[True, True])
                image_df.to_excel(writer, sheet_name='Bao_cao_anh', index=False)
        
        # N√©n th∆∞ m·ª•c k·∫øt qu·∫£ th√†nh file ZIP
        socketio.emit('progress_update', {
            'percent': 95, 
            'message': f'ƒêang n√©n k·∫øt qu·∫£ th√†nh file ZIP...',
            'detail': f'ƒê√£ x·ª≠ l√Ω {len(all_products)} s·∫£n ph·∫©m, {stats["images_downloaded"]} ·∫£nh'
        })
        
        zip_path = result_dir + '.zip'
        zip_filename = os.path.basename(zip_path)
        
        try:
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for folder in category_folders:
                    for root, dirs, files in os.walk(folder):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, result_dir)
                            zipf.write(file_path, arcname)
                
                # Th√™m file b√°o c√°o duy nh·∫•t v√†o ZIP
                zipf.write(report_path, os.path.basename(report_path))
        except Exception as e:
            print(f"L·ªói khi n√©n th∆∞ m·ª•c: {str(e)}")
            socketio.emit('progress_update', {
                'percent': 100, 
                'message': f'Ho√†n th√†nh l·∫•y d·ªØ li·ªáu {len(all_products)} s·∫£n ph·∫©m (kh√¥ng n√©n ƒë∆∞·ª£c)',
                'detail': f'ƒê√£ x·∫£y ra l·ªói khi n√©n: {str(e)}',
                'completed': True,
                'download_ready': False
            })
        else:
            # T√≠nh to√°n t·ªïng th·ªùi gian v√† hi·ªáu su·∫•t
            total_time = time.time() - start_time
            products_per_second = stats["products_processed"] / total_time if total_time > 0 else 0
            
            # T·∫°o th√¥ng tin chi ti·∫øt cho download
            download_info = {
                'zip_path': zip_path,
                'zip_filename': zip_filename,
                'download_url': f'/download-baa-result/{zip_filename}',
                'total_products': len(all_products),
                'total_series': len(series_stats),
                'total_categories': stats["categories"],
                'processing_time': f"{total_time/60:.2f} ph√∫t",
                'success_rate': f"{stats['products_processed'] * 100 / max(1, stats['products_found']):.1f}%",
                'image_success_rate': f"{stats['images_downloaded'] * 100 / max(1, stats['products_processed']):.1f}%",
                'file_size': f"{os.path.getsize(zip_path) / (1024 * 1024):.2f} MB" if os.path.exists(zip_path) else "N/A"
            }
            
            # Th√¥ng b√°o k·∫øt qu·∫£ t√πy thu·ªôc v√†o vi·ªác c√≥ series hay kh√¥ng
            if series_stats:
                completion_message = f'üéâ Ho√†n th√†nh! ƒê√£ c√†o ƒë∆∞·ª£c {len(all_products)} s·∫£n ph·∫©m t·ª´ {len(series_stats)} series'
                detail_message = f'Th·ªùi gian: {total_time/60:.2f} ph√∫t ‚Ä¢ T·ªëc ƒë·ªô: {products_per_second:.2f} sp/s ‚Ä¢ File: {zip_filename}'
            else:
                completion_message = f'üéâ Ho√†n th√†nh! ƒê√£ c√†o ƒë∆∞·ª£c {len(all_products)} s·∫£n ph·∫©m'
                detail_message = f'Th·ªùi gian: {total_time/60:.2f} ph√∫t ‚Ä¢ T·ªëc ƒë·ªô: {products_per_second:.2f} sp/s ‚Ä¢ File: {zip_filename}'
            
            # Chu·∫©n b·ªã series_stats ƒë·ªÉ hi·ªÉn th·ªã (n·∫øu c√≥)
            series_display = []
            if series_stats:
                series_display = [
                    {
                        'series_name': series_name,
                        'product_count': stats_info['So_luong'],
                        'success_rate': f"{(stats_info['Co_gia'] * 100 / stats_info['So_luong']):.1f}%" if stats_info['So_luong'] > 0 else "0%"
                    }
                    for series_name, stats_info in sorted(series_stats.items(), key=lambda x: x[1]['So_luong'], reverse=True)[:10]  # Top 10 series
                ]
            
            socketio.emit('progress_update', {
                'percent': 100, 
                'message': completion_message,
                'detail': detail_message,
                'completed': True,
                'download_ready': True,
                'download_info': download_info,
                'series_stats': series_display
            })
        
        # Ghi log t·ªïng k·∫øt
        print(f"=== Th·ªëng k√™ c√†o d·ªØ li·ªáu BAA.vn ===")
        print(f"T·ªïng URL x·ª≠ l√Ω: {stats['urls_processed']}")
        print(f"S·ªë danh m·ª•c: {stats['categories']}")
        if series_stats:
            print(f"S·ªë series ph√°t hi·ªán: {len(series_stats)}")
        else:
            print(f"Kh√¥ng c√≥ series n√†o ƒë∆∞·ª£c ph√°t hi·ªán")
        print(f"S·ªë s·∫£n ph·∫©m ƒë∆°n l·∫ª: {stats['single_products']}")
        print(f"T·ªïng s·∫£n ph·∫©m t√¨m th·∫•y: {stats['products_found']}")
        print(f"S·∫£n ph·∫©m x·ª≠ l√Ω th√†nh c√¥ng: {stats['products_processed']}")
        print(f"S·∫£n ph·∫©m kh√¥ng c√≥ gi√°: {stats['products_skipped']}")
        print(f"S·∫£n ph·∫©m l·ªói: {stats['failed_products']}")
        print(f"·∫¢nh t·∫£i th√†nh c√¥ng: {stats['images_downloaded']}")
        print(f"·∫¢nh t·∫£i th·∫•t b·∫°i: {stats['failed_images']}")
        print(f"Th·ªùi gian x·ª≠ l√Ω: {total_time:.2f}s ({total_time/60:.2f} ph√∫t)")
        print(f"T·ªëc ƒë·ªô trung b√¨nh: {products_per_second:.2f} s·∫£n ph·∫©m/gi√¢y")
        
        # Log chi ti·∫øt v·ªÅ c√°c series (ch·ªâ khi c√≥)
        if series_stats:
            print(f"\n=== Chi ti·∫øt Series ph√°t hi·ªán ===")
            sorted_series = sorted(series_stats.items(), key=lambda x: x[1]['So_luong'], reverse=True)
            for series_name, stats_info in sorted_series:
                success_rate = (stats_info['Co_gia'] * 100 / stats_info['So_luong']) if stats_info['So_luong'] > 0 else 0
                print(f"- {series_name}: {stats_info['So_luong']} s·∫£n ph·∫©m " +
                      f"({stats_info['Co_gia']} c√≥ gi√°, {stats_info['Khong_gia']} kh√¥ng c√≥ gi√°, " +
                      f"t·ª∑ l·ªá c√≥ gi√°: {success_rate:.1f}%)")
        else:
            print(f"\n=== Kh√¥ng c√≥ series n√†o ƒë∆∞·ª£c ph√¢n lo·∫°i ===")
            print("Ch·ªâ ph√¢n lo·∫°i series cho URL ƒë·∫∑c bi·ªát: https://baa.vn/vn/Category/cong-tac-den-bao-coi-bao-qlight_F_782/")
        
        print(f"=======================================")
        
        return all_products, result_dir

    def _collect_product_urls_with_pagination(self, category_urls):
        """Thu th·∫≠p URL s·∫£n ph·∫©m t·ª´ danh m·ª•c, h·ªó tr·ª£ ph√¢n trang v·ªõi x·ª≠ l√Ω ƒëa lu·ªìng"""
        all_product_urls = []
        
        # Th√¥ng b√°o b·∫Øt ƒë·∫ßu v√† theo d√µi ti·∫øn ƒë·ªô
        total_categories = len(category_urls)
        start_time = time.time()
        
        # ƒê·∫øm s·ªë trang d·ª± ki·∫øn ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô
        total_pages_estimate = 0
        
        # Ki·ªÉm tra s·ªë trang cho t·ª´ng danh m·ª•c
        socketio.emit('progress_update', {
            'percent': 2,
            'message': f'ƒêang ph√°t hi·ªán s·ªë trang cho {len(category_urls)} danh m·ª•c',
            'detail': 'Ph√¢n t√≠ch c·∫•u tr√∫c ph√¢n trang...'
        })
        
        # L∆∞u th√¥ng tin s·ªë trang cho m·ªói danh m·ª•c
        category_pages = {}
        
        # Ph√°t hi·ªán s·ªë trang cho t·ª´ng danh m·ª•c tr∆∞·ªõc khi x·ª≠ l√Ω
        for idx, category_url in enumerate(category_urls):
            try:
                max_pages = detect_pagination(category_url)
                category_pages[category_url] = max_pages
                total_pages_estimate += max_pages
                
                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                socketio.emit('progress_update', {
                    'percent': 2 + (idx * 3 // total_categories),
                    'message': f'Ph√°t hi·ªán ph√¢n trang ({idx+1}/{total_categories})',
                    'detail': f'Danh m·ª•c: {category_url} - {max_pages} trang'
                })
                
                print(f"Danh m·ª•c {idx+1}/{total_categories}: {category_url} - Ph√°t hi·ªán {max_pages} trang")
            except Exception as e:
                print(f"L·ªói khi ph√°t hi·ªán s·ªë trang cho {category_url}: {str(e)}")
                category_pages[category_url] = 1
        
        if total_pages_estimate == 0:
            total_pages_estimate = len(category_urls)  # T·ªëi thi·ªÉu l√† 1 trang cho m·ªói danh m·ª•c
        
        # Theo d√µi ti·∫øn ƒë·ªô
        pages_processed = 0
        products_found = 0
        category_processed = 0
        
        # Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan
        print(f"T·ªïng s·ªë danh m·ª•c: {total_categories}, ∆∞·ªõc t√≠nh {total_pages_estimate} trang")
        socketio.emit('progress_update', {
            'percent': 5,
            'message': f'Chu·∫©n b·ªã thu th·∫≠p d·ªØ li·ªáu t·ª´ {total_pages_estimate} trang',
            'detail': f'S·ªë danh m·ª•c: {total_categories}'
        })
        
        # H√†m worker ƒë·ªÉ x·ª≠ l√Ω t·ª´ng trang
        def process_page(url, is_category=True):
            """X·ª≠ l√Ω m·ªôt trang danh m·ª•c ho·∫∑c trang ph√¢n trang c·ª• th·ªÉ"""
            try:
                product_urls = extract_product_urls(url)
                return url, product_urls, None
            except Exception as e:
                error_msg = str(e)
                print(f"L·ªói khi thu th·∫≠p URL t·ª´ {url}: {error_msg}")
                return url, [], error_msg
        
        # T·∫°o danh s√°ch c√°c task ph√¢n trang
        pagination_tasks = []
        
        for category_url in category_urls:
            max_pages = category_pages[category_url]
            
            # Th√™m trang ƒë·∫ßu ti√™n cho m·ªói danh m·ª•c
            pagination_tasks.append((category_url, True))
            
            # Th√™m c√°c trang ph√¢n trang kh√°c n·∫øu c√≥
            for page in range(2, max_pages + 1):
                page_url = make_pagination_url(category_url, page)
                pagination_tasks.append((page_url, False))
        
        # Theo d√µi ti·∫øn ƒë·ªô v√† x·ª≠ l√Ω ƒëa lu·ªìng
        batch_size = 10  # S·ªë l∆∞·ª£ng trang x·ª≠ l√Ω trong m·ªói batch
        batches = [pagination_tasks[i:i+batch_size] for i in range(0, len(pagination_tasks), batch_size)]
        
        for batch_idx, batch in enumerate(batches):
            # T√≠nh to√°n ph·∫ßn trƒÉm ti·∫øn ƒë·ªô
            batch_start_percent = 5 + (batch_idx * 10 // len(batches))
            
            # Th√¥ng b√°o b·∫Øt ƒë·∫ßu batch
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, len(pagination_tasks))
            
            socketio.emit('progress_update', {
                'percent': batch_start_percent,
                'message': f'ƒêang thu th·∫≠p batch {batch_idx+1}/{len(batches)}',
                'detail': f'X·ª≠ l√Ω trang {batch_start+1}-{batch_end}/{len(pagination_tasks)}'
            })
            
            # X·ª≠ l√Ω batch hi·ªán t·∫°i v·ªõi ƒëa lu·ªìng
            batch_start_time = time.time()
            batch_results = []
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(process_page, url, is_category) for url, is_category in batch]
                
                # Thu th·∫≠p k·∫øt qu·∫£ khi ho√†n th√†nh
                for future in as_completed(futures):
                    try:
                        url, product_urls, error = future.result()
                        batch_results.append((url, product_urls, error))
                        
                        # C·∫≠p nh·∫≠t th·ªëng k√™
                        pages_processed += 1
                        if product_urls:
                            products_found += len(product_urls)
                            
                        # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô chi ti·∫øt
                        progress_percent = batch_start_percent + (pages_processed * 10 // len(pagination_tasks))
                        
                        # T√≠nh t·ªëc ƒë·ªô x·ª≠ l√Ω
                        elapsed = time.time() - batch_start_time
                        pages_per_second = pages_processed / elapsed if elapsed > 0 else 0
                        est_remaining = (len(pagination_tasks) - pages_processed) / pages_per_second if pages_per_second > 0 else 0
                        
                        # Format th√¥ng b√°o th·ªùi gian c√≤n l·∫°i
                        remaining_info = ""
                        if est_remaining > 0:
                            if est_remaining < 60:
                                remaining_info = f", c√≤n l·∫°i: {est_remaining:.1f}s"
                            else:
                                remaining_info = f", c√≤n l·∫°i: {est_remaining/60:.1f}m"
                        
                        # C·∫≠p nh·∫≠t th√¥ng b√°o ti·∫øn ƒë·ªô
                        socketio.emit('progress_update', {
                            'percent': progress_percent,
                            'message': f'ƒê√£ x·ª≠ l√Ω {pages_processed}/{len(pagination_tasks)} trang',
                            'detail': f'ƒê√£ t√¨m th·∫•y {products_found} URL s·∫£n ph·∫©m{remaining_info}'
                        })
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω future: {str(e)}")
            
            # Ki·ªÉm tra k·∫øt qu·∫£ batch v√† th√™m v√†o danh s√°ch s·∫£n ph·∫©m
            batch_products = 0
            for url, product_urls, error in batch_results:
                if product_urls:
                    all_product_urls.extend(product_urls)
                    batch_products += len(product_urls)
            
            # Log hi·ªáu su·∫•t c·ªßa batch
            batch_elapsed = time.time() - batch_start_time
            batch_speed = batch_size / batch_elapsed if batch_elapsed > 0 else 0
            print(f"Batch {batch_idx+1}/{len(batches)} ho√†n th√†nh trong {batch_elapsed:.2f}s, " +
                  f"t·ªëc ƒë·ªô: {batch_speed:.2f} trang/s, t√¨m th·∫•y {batch_products} s·∫£n ph·∫©m")
        
        # Lo·∫°i b·ªè URL tr√πng l·∫∑p
        unique_product_urls = list(dict.fromkeys(all_product_urls))
        
        # Log th·ªëng k√™
        total_time = time.time() - start_time
        print(f"ƒê√£ thu th·∫≠p xong {len(unique_product_urls)} URL s·∫£n ph·∫©m (t·ª´ {len(all_product_urls)} URLs g·ªëc)")
        print(f"Th·ªùi gian x·ª≠ l√Ω: {total_time:.2f}s, t·ªëc ƒë·ªô: {pages_processed/total_time:.2f} trang/s")
        
        # Th√¥ng b√°o ho√†n th√†nh
        socketio.emit('progress_update', {
            'percent': 15,
            'message': f'ƒê√£ thu th·∫≠p xong {len(unique_product_urls)} URL s·∫£n ph·∫©m',
            'detail': f'ƒê√£ x·ª≠ l√Ω {pages_processed} trang t·ª´ {len(category_urls)} danh m·ª•c'
        })
        
        return unique_product_urls

    def _download_product_images(self, code_url_map, series_products_map, anh_dir, category_name, category_idx=0, total_categories=1, percent_base=50):
        """T·∫£i ·∫£nh s·∫£n ph·∫©m v·ªõi x·ª≠ l√Ω l·ªói v√† retry th√¥ng minh, t·∫°o m·ªôt b√°o c√°o duy nh·∫•t, nh√≥m theo series n·∫øu c√≥"""
        img_map = {}
        
        if not code_url_map:
            return img_map, []
        
        # T·∫°o th∆∞ m·ª•c h√¨nh ·∫£nh cho danh m·ª•c ch√≠nh n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(anh_dir, exist_ok=True)
        
        # T·∫°o th∆∞ m·ª•c ·∫£nh cho t·ª´ng series (ch·ªâ khi c√≥ series)
        series_img_dirs = {}
        if series_products_map:  # Ch·ªâ t·∫°o khi c√≥ series
            for series_name in series_products_map.keys():
                series_folder_name = sanitize_folder_name(series_name)
                series_dir = os.path.join(os.path.dirname(anh_dir), series_folder_name)
                series_img_dir = os.path.join(series_dir, "Anh")
                os.makedirs(series_img_dir, exist_ok=True)
                series_img_dirs[series_name] = series_img_dir
        
        # Log th√¥ng tin b·∫Øt ƒë·∫ßu
        if series_img_dirs:
            print(f"[{category_name}] B·∫Øt ƒë·∫ßu t·∫£i {len(code_url_map)} ·∫£nh s·∫£n ph·∫©m v√†o {len(series_img_dirs)} series")
        else:
            print(f"[{category_name}] B·∫Øt ƒë·∫ßu t·∫£i {len(code_url_map)} ·∫£nh s·∫£n ph·∫©m v√†o th∆∞ m·ª•c chung (kh√¥ng c√≥ series)")
        
        # Th·ªùi gian b·∫Øt ƒë·∫ßu
        start_time = time.time()
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu cho b√°o c√°o t·∫£i ·∫£nh (ƒë∆∞·ª£c h·ª£p nh·∫•t v·ªõi b√°o c√°o ch√≠nh)
        image_report_data = []
        
        def download_img_worker(item):
            """Worker function ƒë·ªÉ t·∫£i ·∫£nh v·ªõi retry th√¥ng minh v√† l∆∞u theo series n·∫øu c√≥"""
            code, url_info = item
            
            # X·ª≠ l√Ω url_info (c√≥ th·ªÉ l√† string ho·∫∑c dict)
            if isinstance(url_info, dict):
                url = url_info.get('url', '')
                series_name = url_info.get('series')
            else:
                url = url_info
                series_name = None
            
            if not url:
                return code, '', 'URL tr·ªëng', None
            
            # X√°c ƒë·ªãnh th∆∞ m·ª•c ·∫£nh theo series (n·∫øu c√≥) ho·∫∑c th∆∞ m·ª•c chung
            if series_name and series_name in series_img_dirs:
                target_img_dir = series_img_dirs[series_name]
            else:
                target_img_dir = anh_dir
                series_name = None  # ƒê·∫£m b·∫£o series_name l√† None n·∫øu kh√¥ng c√≥ series
            
            # Ghi nh·∫≠n tr·∫°ng th√°i kh√¥ng c·∫ßn file log ri√™ng
            success_msg = None
            error_msg = None
            download_time = 0
            
            try:
                # Ki·ªÉm tra xem ·∫£nh ƒë√£ t·ªìn t·∫°i ch∆∞a (tr√°nh t·∫£i l·∫°i)
                existing_image = os.path.join(target_img_dir, f"{code}.webp")
                if os.path.exists(existing_image) and os.path.getsize(existing_image) > 0:
                    return code, existing_image, 'ƒê√£ t·ªìn t·∫°i', {
                        'M√£ s·∫£n ph·∫©m': code,
                        'URL': url,
                        'Series': series_name if series_name else 'Kh√¥ng c√≥',
                        'Tr·∫°ng th√°i': 'ƒê√£ t·ªìn t·∫°i',
                        'ƒê∆∞·ªùng d·∫´n ·∫£nh': existing_image,
                        'K√≠ch th∆∞·ªõc (bytes)': os.path.getsize(existing_image),
                        'Th·ªùi gian t·∫£i (s)': 0
                    }
                
                # Retry v·ªõi backoff
                retry_delays = [0.5, 1, 2, 3, 5]  # ƒê·ªô tr·ªÖ tƒÉng d·∫ßn gi·ªØa c√°c l·∫ßn th·ª≠
                download_start = time.time()
                
                for retry in range(self.max_retries):
                    try:
                        # Th·ª≠ t·∫£i ·∫£nh v√†o th∆∞ m·ª•c series ho·∫∑c th∆∞ m·ª•c chung
                        result = download_baa_product_images_fixed([url], target_img_dir, create_report=False)
                        
                        if result and result.get('report_data'):
                            for r in result['report_data']:
                                if r.get('M√£ s·∫£n ph·∫©m') == code:
                                    image_path = r.get('ƒê∆∞·ªùng d·∫´n ·∫£nh', '')
                                    
                                    # Ki·ªÉm tra xem file ·∫£nh c√≥ t·ªìn t·∫°i v√† c√≥ k√≠ch th∆∞·ªõc > 0
                                    if image_path and os.path.exists(image_path) and os.path.getsize(image_path) > 0:
                                        # T√≠nh th·ªùi gian t·∫£i
                                        download_time = time.time() - download_start
                                        
                                        # Ghi nh·∫≠n th√¥ng tin th√†nh c√¥ng
                                        if series_name:
                                            success_msg = f"T·∫£i th√†nh c√¥ng v√†o series {series_name}, k√≠ch th∆∞·ªõc: {os.path.getsize(image_path)} bytes"
                                        else:
                                            success_msg = f"T·∫£i th√†nh c√¥ng v√†o th∆∞ m·ª•c chung, k√≠ch th∆∞·ªõc: {os.path.getsize(image_path)} bytes"
                                        
                                        # Tr·∫£ v·ªÅ th√¥ng tin ƒë·∫ßy ƒë·ªß
                                        return code, image_path, success_msg, {
                                            'M√£ s·∫£n ph·∫©m': code,
                                            'URL': url,
                                            'Series': series_name if series_name else 'Kh√¥ng c√≥',
                                            'Tr·∫°ng th√°i': 'Th√†nh c√¥ng',
                                            'ƒê∆∞·ªùng d·∫´n ·∫£nh': image_path,
                                            'K√≠ch th∆∞·ªõc (bytes)': os.path.getsize(image_path),
                                            'Th·ªùi gian t·∫£i (s)': round(download_time, 2),
                                            'S·ªë l·∫ßn th·ª≠': retry + 1
                                        }
                                    else:
                                        error_msg = "C√≥ ƒë∆∞·ªùng d·∫´n ·∫£nh nh∆∞ng file kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng"
                        else:
                            error_msg = "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi h·ª£p l·ªá"
                        
                        # N·∫øu kh√¥ng th√†nh c√¥ng, ch·ªù v√† th·ª≠ l·∫°i
                        if retry < self.max_retries - 1:
                            # Ch·ªù theo th·ªùi gian retry v·ªõi backoff
                            time.sleep(retry_delays[min(retry, len(retry_delays)-1)])
                    except Exception as e:
                        error_msg = str(e)
                        
                        if retry < self.max_retries - 1:
                            time.sleep(retry_delays[min(retry, len(retry_delays)-1)])
                
                # T√≠nh th·ªùi gian t·∫£i t·ªïng c·ªông
                download_time = time.time() - download_start
                
                return code, '', f"L·ªói sau {self.max_retries} l·∫ßn th·ª≠: {error_msg}", {
                    'M√£ s·∫£n ph·∫©m': code,
                    'URL': url,
                    'Series': series_name if series_name else 'Kh√¥ng c√≥',
                    'Tr·∫°ng th√°i': 'Th·∫•t b·∫°i',
                    'ƒê∆∞·ªùng d·∫´n ·∫£nh': '',
                    'K√≠ch th∆∞·ªõc (bytes)': 0,
                    'Th·ªùi gian t·∫£i (s)': round(download_time, 2),
                    'L·ªói': error_msg,
                    'S·ªë l·∫ßn th·ª≠': self.max_retries
                }
                
            except Exception as e:
                error_msg = str(e)
                download_time = time.time() - download_start
                
                return code, '', f"L·ªói ngo√†i: {error_msg}", {
                    'M√£ s·∫£n ph·∫©m': code,
                    'URL': url,
                    'Series': series_name if series_name else 'Kh√¥ng c√≥',
                    'Tr·∫°ng th√°i': 'L·ªói',
                    'ƒê∆∞·ªùng d·∫´n ·∫£nh': '',
                    'K√≠ch th∆∞·ªõc (bytes)': 0,
                    'Th·ªùi gian t·∫£i (s)': round(download_time, 2),
                    'L·ªói': error_msg,
                    'S·ªë l·∫ßn th·ª≠': 1
                }
        
        # T√≠nh to√°n ph·∫ßn trƒÉm cho ph·∫ßn t·∫£i ·∫£nh
        percent_range = 30 // max(1, total_categories)
        percent_start = percent_base + (category_idx * 90 // max(1, total_categories))
        
        # Theo d√µi ti·∫øn ƒë·ªô v√† hi·ªáu su·∫•t
        total_images = len(code_url_map)
        success_count = 0
        fail_count = 0
        start_batch_time = time.time()
        batch_size = 20  # K√≠ch th∆∞·ªõc batch cho vi·ªác b√°o c√°o hi·ªáu su·∫•t
        
        # D·ªØ li·ªáu cho b√°o c√°o t·∫£i ·∫£nh
        image_report_data = []
        
        # Th·ªëng k√™ theo series
        series_stats = {}
        for series_name in series_products_map.keys():
            series_stats[series_name] = {'success': 0, 'fail': 0}
        
        # X·ª≠ l√Ω ƒëa lu·ªìng t·∫£i ·∫£nh v·ªõi theo d√µi ti·∫øn ƒë·ªô
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # T·∫°o c√°c future cho vi·ªác t·∫£i ·∫£nh
            img_futures = {executor.submit(download_img_worker, item): item[0] 
                          for item in code_url_map.items()}
            
            # X·ª≠ l√Ω t·ª´ng future khi ho√†n th√†nh
            for idx, future in enumerate(as_completed(img_futures)):
                try:
                    code, img_path, status, report_entry = future.result()
                    
                    if report_entry:
                        image_report_data.append(report_entry)
                        series_name = report_entry.get('Series', 'Kh√¥ng c√≥')
                    
                    if code:
                        if img_path:
                            img_map[code] = img_path
                            success_count += 1
                            if series_name in series_stats:
                                series_stats[series_name]['success'] += 1
                        else:
                            fail_count += 1
                            if series_name in series_stats:
                                series_stats[series_name]['fail'] += 1
                            print(f"[{category_name}] Kh√¥ng th·ªÉ t·∫£i ·∫£nh cho {code}: {status}")
                except Exception as e:
                    fail_count += 1
                    print(f"[{category_name}] L·ªói x·ª≠ l√Ω future: {str(e)}")
                
                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                items_done = idx + 1
                
                # T√≠nh t·ªëc ƒë·ªô v√† ∆∞·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i
                elapsed = time.time() - start_time
                current_speed = items_done / elapsed if elapsed > 0 else 0
                est_remaining = (total_images - items_done) / current_speed if current_speed > 0 else 0
                
                # B√°o c√°o hi·ªáu su·∫•t theo batch
                if items_done % batch_size == 0 or items_done == total_images:
                    batch_elapsed = time.time() - start_batch_time
                    batch_speed = batch_size / batch_elapsed if batch_elapsed > 0 else 0
                    print(f"[{category_name}] Ti·∫øn ƒë·ªô t·∫£i ·∫£nh: {items_done}/{total_images}, " +
                          f"batch speed: {batch_speed:.2f} img/s, total speed: {current_speed:.2f} img/s")
                    start_batch_time = time.time()
                
                # Format th√¥ng b√°o ti·∫øn ƒë·ªô
                remaining_info = ""
                if est_remaining > 0:
                    if est_remaining < 60:
                        remaining_info = f", c√≤n l·∫°i: {est_remaining:.1f}s"
                    else:
                        remaining_info = f", c√≤n l·∫°i: {est_remaining/60:.1f}m"
                
                # T√≠nh ph·∫ßn trƒÉm ti·∫øn ƒë·ªô
                percent = percent_start + (items_done * percent_range // total_images)
                
                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô l√™n giao di·ªán
                socketio.emit('progress_update', {
                    'percent': percent, 
                    'message': f'[{category_name}] ƒê√£ t·∫£i ·∫£nh {items_done}/{total_images} ' +
                              f'(th√†nh c√¥ng: {success_count}, th·∫•t b·∫°i: {fail_count})',
                    'detail': f'T·ªëc ƒë·ªô: {current_speed:.1f} ·∫£nh/s{remaining_info}, {len(series_img_dirs)} series'
                })
        
        # T·∫°o b√°o c√°o Excel cho vi·ªác t·∫£i ·∫£nh (l∆∞u v√†o danh s√°ch d·ªØ li·ªáu, s·∫Ω ƒë∆∞·ª£c h·ª£p nh·∫•t v√†o b√°o c√°o ch√≠nh)
        if image_report_data:
            # S·∫Øp x·∫øp d·ªØ li·ªáu b√°o c√°o theo series v√† m√£ s·∫£n ph·∫©m ƒë·ªÉ d·ªÖ tra c·ª©u
            try:
                sorted_data = sorted(image_report_data, key=lambda x: (x.get('Series', ''), x.get('M√£ s·∫£n ph·∫©m', '')))
                print(f"[{category_name}] ƒê√£ thu th·∫≠p {len(sorted_data)} b·∫£n ghi b√°o c√°o t·∫£i ·∫£nh")
                
                # Log th·ªëng k√™ theo series
                print(f"[{category_name}] Th·ªëng k√™ t·∫£i ·∫£nh theo series:")
                for series_name, stats in series_stats.items():
                    total_series = stats['success'] + stats['fail']
                    if total_series > 0:
                        success_rate = (stats['success'] * 100) / total_series
                        print(f"  - {series_name}: {stats['success']}/{total_series} th√†nh c√¥ng ({success_rate:.1f}%)")
                
                return img_map, sorted_data
            except Exception as e:
                print(f"[{category_name}] L·ªói khi s·∫Øp x·∫øp b√°o c√°o t·∫£i ·∫£nh: {str(e)}")
        
        # B√°o c√°o k·∫øt qu·∫£ cu·ªëi c√πng
        total_time = time.time() - start_time
        avg_time_per_image = total_time / total_images if total_images > 0 else 0
        images_per_second = total_images / total_time if total_time > 0 else 0
        
        print(f"[{category_name}] Ho√†n t·∫•t t·∫£i ·∫£nh: {success_count}/{total_images} th√†nh c√¥ng, " +
              f"{fail_count} th·∫•t b·∫°i, t·ªëc ƒë·ªô: {images_per_second:.2f} ·∫£nh/s")
        
        return img_map, image_report_data

    def _normalize_spec(self, spec_html):
        """
        Chuy·ªÉn ƒë·ªïi m√£ s·∫£n ph·∫©m trong b·∫£ng th√¥ng s·ªë k·ªπ thu·∫≠t th√†nh ch·ªØ hoa.
        """
        if not spec_html:
            return spec_html
        
        # T√¨m v√† chuy·ªÉn ƒë·ªïi m√£ s·∫£n ph·∫©m th√†nh ch·ªØ hoa
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(spec_html, 'html.parser')
        for td in soup.find_all('td'):
            if td.text and any(keyword in td.text.lower() for keyword in ['m√£', 'model', 'part no']):
                td.string = td.text.upper()
            
        return str(soup)