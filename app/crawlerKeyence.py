"""
Keyence Product Crawler - C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ website keyence.com.vn
Phi√™n b·∫£n: 1.0
T√°c gi·∫£: Auto-generated based on OmronCrawler
Ng√†y t·∫°o: $(date)
"""

import os
import time
import requests
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import concurrent.futures
from queue import Queue
import pandas as pd
from PIL import Image, ImageEnhance
from io import BytesIO
import json
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from app.webp_converter import WebPConverter
import threading

# Selenium imports for dynamic content
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def sanitize_folder_name(name):
    """L√†m s·∫°ch t√™n folder ƒë·ªÉ ph√π h·ª£p v·ªõi h·ªá ƒëi·ªÅu h√†nh"""
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá
    clean_name = re.sub(r'[<>:"/\\|?*]', '_', name)
    clean_name = re.sub(r'[^\w\s-]', '', clean_name).strip()
    clean_name = re.sub(r'[-\s]+', '_', clean_name)
    return clean_name if clean_name else 'Unknown_Category'

def standardize_filename_keyence(code):
    """
    Chu·∫©n h√≥a m√£ s·∫£n ph·∫©m Keyence th√†nh t√™n file h·ª£p l·ªá
    Chuy·ªÉn t·ª´ JavaScript logic sang Python
    """
    if not code:
        return ""
    
    # Ki·ªÉm tra add-on kit
    had_add_on_kit = re.search(r'add[\s\-]*on[\s\-]*kit', code, re.IGNORECASE)
    
    # L√†m s·∫°ch chu·ªói lo·∫°i b·ªè ghi ch√∫ coating, addon...
    clean_code = re.sub(r'\(with special coating\)', '', code, flags=re.IGNORECASE)
    clean_code = re.sub(r'\[with special coating\]', '', clean_code, flags=re.IGNORECASE)
    clean_code = re.sub(r'add[\s\-]*on[\s\-]*kit', '', clean_code, flags=re.IGNORECASE).strip()
    
    # Chu·∫©n h√≥a nh∆∞ standardize_filename
    def standardize(s):
        result = re.sub(r'[\\/:*?"<>|,=\s]', '-', s)  # Replace invalid chars with dash
        result = re.sub(r'-+', '-', result)  # Replace multiple dashes with single dash
        result = re.sub(r'[^a-zA-Z0-9\-_]', '', result)  # Keep only valid chars
        return result.strip('-')
    
    result = standardize(clean_code.upper())
    if had_add_on_kit:
        result += "-ADK"
    
    return result

class KeyenceCrawler:
    """
    Crawler chuy√™n d·ª•ng cho website keyence.com.vn
    C√†o d·ªØ li·ªáu s·∫£n ph·∫©m v·ªõi x·ª≠ l√Ω ƒëa lu·ªìng v√† discontinued products
    """
    
    def __init__(self, output_root=None, max_workers=8, max_retries=3, socketio=None):
        """
        Kh·ªüi t·∫°o KeyenceCrawler
        
        Args:
            output_root: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u k·∫øt qu·∫£
            max_workers: S·ªë lu·ªìng t·ªëi ƒëa
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i khi request th·∫•t b·∫°i
            socketio: Socket.IO instance ƒë·ªÉ emit ti·∫øn tr√¨nh
        """
        self.output_root = output_root or os.path.join(os.getcwd(), "output_keyence")
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.socketio = socketio
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(self.output_root, exist_ok=True)
        
        # Base URLs
        self.base_url = "https://www.keyence.com.vn"
        
        # C·∫•u h√¨nh session v·ªõi retry strategy
        self.session = requests.Session()
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers m√¥ ph·ªèng tr√¨nh duy·ªát th·∫≠t
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # C·∫•u h√¨nh Selenium WebDriver cho Keyence
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # Disable c√°c features c√≥ th·ªÉ g√¢y overlay/popup
        self.chrome_options.add_argument('--disable-extensions')
        self.chrome_options.add_argument('--disable-plugins')
        self.chrome_options.add_argument('--disable-javascript-dialogs')
        self.chrome_options.add_argument('--disable-notifications')
        self.chrome_options.add_argument('--disable-infobars')
        self.chrome_options.add_argument('--disable-popup-blocking')
        self.chrome_options.add_argument('--disable-default-apps')
        self.chrome_options.add_argument('--no-first-run')
        self.chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        # Disable logging ƒë·ªÉ gi·∫£m noise
        self.chrome_options.add_argument('--log-level=3')
        self.chrome_options.add_argument('--disable-logging')
        
        # Preferences ƒë·ªÉ disable tutorial v√† overlay
        prefs = {
            "profile.default_content_setting_values": {
                "notifications": 2,
                "popups": 2
            },
            "profile.managed_default_content_settings": {
                "images": 2
            }
        }
        self.chrome_options.add_experimental_option("prefs", prefs)
        
        # Th·ªëng k√™
        self.stats = {
            "categories_processed": 0,
            "series_found": 0,
            "products_found": 0,
            "products_processed": 0,
            "images_downloaded": 0,
            "failed_requests": 0,
            "failed_images": 0
        }
        
        logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o KeyenceCrawler v·ªõi Selenium support")
    
    def get_driver(self):
        """T·∫°o m·ªôt WebDriver Chrome m·ªõi"""
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(10)
            return driver
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver: {e}")
            raise
    
    def close_driver(self, driver):
        """ƒê√≥ng WebDriver"""
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"L·ªói khi ƒë√≥ng WebDriver: {e}")
    
    def _parse_keyence_specs(self, soup):
        """
        Parse th√¥ng s·ªë k·ªπ thu·∫≠t Keyence theo chu·∫©n h√£ng v·ªõi BeautifulSoup
        √Åp d·ª•ng logic ƒë∆°n gi·∫£n v√† ch√≠nh x√°c theo y√™u c·∫ßu user
        
        Args:
            soup: BeautifulSoup object c·ªßa product page
            
        Returns:
            list: Danh s√°ch specs items [{"key": str, "value": str, "attributeid": str}]
        """
        try:
            # T√¨m t·∫•t c·∫£ rows trong specs table
            rows = soup.select('div.specTable-block table tr')
            
            items = []
            for tr in rows:
                key_main = tr.select_one('td.specTable-clm-0')
                key_sub = tr.select_one('td.specTable-clm-1')
                val_td = tr.select_one('td.specTable-clm-4')  # c·ªôt gi√° tr·ªã
                
                # N·∫øu kh√¥ng c√≥ c·ªôt gi√° tr·ªã, th·ª≠ t√¨m trong c√°c c·ªôt kh√°c (PS-26 format)
                if not val_td:
                    val_td = tr.select_one('td.specTable-clm-2')  # PS-26 style
                    if not val_td:
                        val_td = tr.select_one('td.specTable-clm-1')  # 2-column simple
                
                # B·ªè c√°c h√†ng kh√¥ng c√≥ √¥ key ch√≠nh ho·∫∑c √¥ gi√° tr·ªã
                if not key_main or not val_td:
                    continue
                
                # Text g·ªçn + x·ª≠ l√Ω xu·ªëng d√≤ng <br>
                def text_of(el):
                    if not el:
                        return ""
                    # Join c√°c text fragments v·ªõi ' ; ' 
                    return ' ; '.join([s.strip() for s in el.stripped_strings])
                
                key = text_of(key_main)
                if key_sub and key_sub.get_text(strip=True):
                    key = f"{key} ‚Äî {text_of(key_sub)}"
                
                value = text_of(val_td)
                # Chuy·ªÉn ƒë·ªïi c√°c k√Ω t·ª± tr·ªëng th√†nh empty string
                if value in ["‚Äï", "‚Äî", "-"]:
                    value = ""
                
                # Ch·ªâ th√™m n·∫øu c√≥ key v√† value h·ª£p l·ªá
                if key and key != value:
                    items.append({
                        "key": key,
                        "value": value,
                        "attributeid": val_td.get("attributeid", "")  # gi·ªØ l·∫°i id n·∫øu c·∫ßn map
                    })
            
            logger.info(f"‚úÖ Parsed {len(items)} specification items theo chu·∫©n h√£ng")
            return items
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi parse Keyence specs: {str(e)}")
            return []
    
    def _parse_keyence_footnotes(self, soup):
        """
        Parse footnotes t·ª´ specTable-foot rows
        
        Args:
            soup: BeautifulSoup object c·ªßa product page
            
        Returns:
            dict: Footnotes {attributeid: content}
        """
        try:
            footnotes = {}
            footer_rows = soup.select('div.specTable-block table tr.specTable-foot')
            
            for footer_row in footer_rows:
                cells = footer_row.find_all('td')
                if cells:
                    footnote_content = ' ; '.join([s.strip() for s in cells[0].stripped_strings])
                    attributeid = cells[0].get('attributeid', 'footnotes')
                    footnotes[attributeid] = footnote_content
            
            return footnotes
            
        except Exception as e:
            logger.debug(f"L·ªói khi parse footnotes: {str(e)}")
            return {}

    def _extract_original_specs_html(self, soup):
        """
        Tr√≠ch xu·∫•t nguy√™n kh·ªëi HTML ph·∫ßn Th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ trang Keyence.
        ƒê·ªìng th·ªùi ti√™m inline-style t·ªëi thi·ªÉu ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªÉn th·ªã ƒë·∫πp khi kh√¥ng c√≥ CSS c·ªßa Keyence.

        Args:
            soup: BeautifulSoup c·ªßa product page

        Returns:
            str: HTML ph·∫ßn specs ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a; r·ªóng n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            specs_div = soup.find('div', class_='prd-specsTable')
            if not specs_div:
                return ''

            # L·∫•y section ch·ª©a specs n·∫øu c√≥
            section = specs_div.find_parent('section') or specs_div

            # X√°c th·ª±c ti√™u ƒë·ªÅ l√† "Th√¥ng s·ªë k·ªπ thu·∫≠t" n·∫øu t·ªìn t·∫°i
            h2 = section.find('h2') if hasattr(section, 'find') else None
            if h2 and 'th√¥ng s·ªë' not in h2.get_text(strip=True).lower():
                # Kh√¥ng ƒë√∫ng section specs
                return ''

            # Ti√™m inline styles t·ªëi thi·ªÉu ƒë·ªÉ nh√¨n gi·ªëng website khi thi·∫øu CSS g·ªëc
            self._inject_inline_styles_to_keyence_specs(section)

            return str(section)
        except Exception:
            return ''

    def _inject_inline_styles_to_keyence_specs(self, section):
        """
        Th√™m inline-style c∆° b·∫£n v√†o table/td/th ƒë·ªÉ hi·ªÉn th·ªã gi·ªëng website trong m√¥i tr∆∞·ªùng kh√¥ng c√≥ CSS g·ªëc.
        Modify in-place.
        """
        try:
            table = section.find('table') if hasattr(section, 'find') else None
            if not table:
                return

            # Th√™m style cho table
            table_attrs = table.get('style', '')
            add_table_style = 'border-collapse:collapse;width:100%;'
            if add_table_style not in table_attrs:
                table['style'] = (table_attrs + ';' + add_table_style).strip(';')

            # Th√™m border/padding cho t·∫•t c·∫£ √¥
            for cell in table.find_all(['td', 'th']):
                cell_style = cell.get('style', '')
                add_cell_style = 'border:1px solid #e5e5e5;padding:8px;vertical-align:top;'
                if add_cell_style not in cell_style:
                    cell['style'] = (cell_style + ';' + add_cell_style).strip(';')

            # L√†m n·ªïi b·∫≠t header n·∫øu c√≥ thead
            thead = table.find('thead')
            if thead:
                for th in thead.find_all('th'):
                    th_style = th.get('style', '')
                    add_th_style = 'background:#f7f7f7;font-weight:600;'
                    if add_th_style not in th_style:
                        th['style'] = (th_style + ';' + add_th_style).strip(';')
        except Exception:
            # An to√†n: kh√¥ng ch·∫∑n workflow n·∫øu styling th·∫•t b·∫°i
            pass
    
    def clean_specs(self, html: str) -> str:
        """
        L√†m s·∫°ch HTML th√¥ng s·ªë k·ªπ thu·∫≠t theo y√™u c·∫ßu:
        - Ch·ªâ gi·ªØ l·∫°i duy nh·∫•t "rowspan" v√† "colspan" tr√™n m·ªçi th·∫ª trong b·∫£ng
        - X√≥a to√†n b·ªô <col>/<colgroup>
        - T√°i t·∫°o t·ªëi thi·ªÉu: <section><h2>Th√¥ng s·ªë k·ªπ thu·∫≠t</h2><table>...</table></section>
        - Ch√®n m·ªôt h√†ng b·∫£n quy·ªÅn ngay tr∆∞·ªõc c√°c h√†ng footnotes (tr.specTable-foot) n·∫øu c√≥,
          ho·∫∑c ·ªü cu·ªëi b·∫£ng n·∫øu kh√¥ng c√≥ footnotes.
        - Sau khi l√†m s·∫°ch, ti√™m inline-style t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ªùng k·∫ª l√† n√©t li·ªÅn.
        """
        try:
            if not html:
                return ""

            soup = BeautifulSoup(html, "html.parser")

            # T√¨m ti√™u ƒë·ªÅ H2 v√† b·∫£ng ƒë·∫ßu ti√™n ngay sau ƒë√≥
            h2 = soup.find("h2", string=lambda s: s and "Th√¥ng s·ªë k·ªπ thu·∫≠t" in s)
            table = h2.find_next("table") if h2 else None
            if not (h2 and table):
                return ""

            # Ghi nh·∫≠n v·ªã tr√≠ c·ªßa h√†ng foot ƒë·∫ßu ti√™n ƒë·ªÉ ch√®n b·∫£n quy·ªÅn ph√≠a tr√™n
            all_rows = table.find_all("tr")
            foot_index = None
            for idx, row in enumerate(all_rows):
                classes = row.get("class", []) or []
                if any(cls == "specTable-foot" for cls in classes):
                    foot_index = idx
                    break

            # Lo·∫°i b·ªè <col> v√† <colgroup>
            for col in table.find_all(["col", "colgroup"]):
                col.decompose()

            # H√†m lo·∫°i b·ªè m·ªçi attribute tr·ª´ rowspan/colspan
            def strip_attrs(tag):
                if not getattr(tag, "attrs", None):
                    return
                keep = {}
                for key in ("rowspan", "colspan"):
                    if key in tag.attrs:
                        keep[key] = tag.attrs[key]
                tag.attrs = keep

            # Strip attributes tr√™n table v√† to√†n b·ªô con
            strip_attrs(table)
            for t in table.find_all(True):
                strip_attrs(t)

            # T·∫°o soup t·ªëi thi·ªÉu v√† g·∫Øn table ƒë√£ l√†m s·∫°ch
            out = BeautifulSoup(features="html.parser")
            sec = out.new_tag("section")
            h2_min = out.new_tag("h2")
            h2_min.string = h2.get_text(strip=True)
            sec.append(h2_min)

            # Sao ch√©p table sang out-soup
            table_copy_soup = BeautifulSoup(str(table), "html.parser")
            table_copy = table_copy_soup.find("table")

            # Ch√®n h√†ng b·∫£n quy·ªÅn
            # T√¨m tbody ho·∫∑c d√πng table n·∫øu kh√¥ng c√≥
            tbody = table_copy.find("tbody")
            if not tbody:
                tbody = table_copy

            # T·∫°o <tr> b·∫£n quy·ªÅn v·ªõi style ƒë·∫≠m cho √¥ ƒë·∫ßu ti√™n
            tr_c = table_copy_soup.new_tag("tr")
            td_label = table_copy_soup.new_tag("td")
            td_label["style"] = "font-weight: bold;"
            td_label.string = "Copyright"
            td_value = table_copy_soup.new_tag("td")
            td_value.string = "Haiphongtech.vn"
            tr_c.append(td_label)
            tr_c.append(td_value)

            # X√°c ƒë·ªãnh v·ªã tr√≠ ch√®n: tr∆∞·ªõc h√†ng foot ƒë·∫ßu ti√™n n·∫øu c√≥
            rows_copy = table_copy.find_all("tr")
            if foot_index is not None and foot_index < len(rows_copy):
                # T√¨m ƒë√∫ng node h√†ng theo index ƒë·ªÉ ch√®n tr∆∞·ªõc
                target_row = rows_copy[foot_index]
                target_row.insert_before(tr_c)
            else:
                tbody.append(tr_c)

            sec.append(table_copy)
            out.append(sec)

            # Ti√™m inline styles t·ªëi thi·ªÉu sau khi l√†m s·∫°ch v√† ch√®n b·∫£n quy·ªÅn
            styled_soup = BeautifulSoup(out.prettify(), "html.parser")
            section_tag = styled_soup.find("section") or styled_soup
            self._inject_inline_styles_to_keyence_specs(section_tag)
            return str(styled_soup)

        except Exception as e:
            logger.debug(f"L·ªói khi clean specs HTML: {e}")
            return ""
    
    def emit_progress(self, percent, message, detail=""):
        """Emit ti·∫øn tr√¨nh qua Socket.IO"""
        if self.socketio:
            self.socketio.emit('progress_update', {
                'percent': percent,
                'message': message,
                'detail': detail
            })
        else:
            print(f"[{percent}%] {message} - {detail}")
    


    def get_html_content(self, url, timeout=30):
        """L·∫•y n·ªôi dung HTML t·ª´ URL"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"L·ªói khi l·∫•y HTML t·ª´ {url}: {str(e)}")
            return None

    def extract_series_from_category(self, category_url):
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ series t·ª´ trang category Keyence v·ªõi Selenium
        Bao g·ªìm c·∫£ discontinued series b·∫±ng c√°ch click switch
        
        Args:
            category_url: URL c·ªßa trang category
            
        Returns:
            list: Danh s√°ch c√°c series URLs v√† metadata
        """
        driver = None
        series_data = []
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"üîÑ Th·ª≠ l·∫ßn {attempt + 1}/{max_retries} extract series t·ª´ {category_url}")
                
                # T·∫°o driver m·ªõi cho m·ªói attempt
                if driver:
                    self.close_driver(driver)
                driver = self.get_driver()
                
                self.emit_progress(
                    10, 
                    f"ƒêang thu th·∫≠p series t·ª´ category (l·∫ßn th·ª≠ {attempt + 1})",
                    f"URL: {category_url}"
                )
                
                # Load trang category v·ªõi timeout
                driver.set_page_load_timeout(30)
                driver.get(category_url)
                
                # ƒê·ª£i trang load xong
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".prd-seriesCard-link"))
                    )
                except TimeoutException:
                    logger.warning(f"‚è∞ Timeout khi ƒë·ª£i page load, th·ª≠ fallback method")
                    return self.extract_series_fallback(category_url)
                
                # T√¨m t·∫•t c·∫£ normal series links
                normal_series_links = driver.find_elements(By.CSS_SELECTOR, "a.prd-seriesCard-link")
                
                for link_element in normal_series_links:
                    try:
                        href = link_element.get_attribute('href')
                        
                        # Extract series name t·ª´ linkLabel
                        link_label = link_element.find_element(By.CSS_SELECTOR, ".prd-seriesCard-linkLabel")
                        series_name = link_label.text.strip()
                        
                        if href and series_name:
                            # Convert relative URL th√†nh absolute URL
                            full_url = urljoin(self.base_url, href)
                            
                            # Tr√°nh tr√πng l·∫∑p
                            if not any(s['url'] == full_url for s in series_data):
                                series_data.append({
                                    'name': series_name,
                                    'url': full_url,
                                    'type': 'normal'
                                })
                                logger.debug(f"‚úÖ T√¨m th·∫•y series: {series_name} - {full_url}")
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è L·ªói khi extract normal series link: {str(e)}")
                        continue
                
                # Click discontinued switch ƒë·ªÉ hi·ªÉn th·ªã discontinued series
                try:
                    discontinued_switch = driver.find_element(
                        By.CSS_SELECTOR, 
                        "button[data-controller*='switch-discontinued']"
                    )
                    if discontinued_switch:
                        # Scroll ƒë·∫øn switch
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", discontinued_switch)
                        time.sleep(1)
                        
                        # Click switch ƒë·ªÉ hi·ªÉn th·ªã discontinued series
                        driver.execute_script("arguments[0].click();", discontinued_switch)
                        logger.info("‚úÖ ƒê√£ click switch ƒë·ªÉ hi·ªÉn th·ªã discontinued series")
                        time.sleep(3)  # ƒê·ª£i content load
                        
                        # T√¨m discontinued series links
                        discontinued_series_links = driver.find_elements(By.CSS_SELECTOR, "a.prd-seriesCardDiscontinued")
                        
                        for link_element in discontinued_series_links:
                            try:
                                href = link_element.get_attribute('href')
                                
                                # Extract series name t·ª´ title
                                title_element = link_element.find_element(By.CSS_SELECTOR, ".prd-seriesCardDiscontinued-title")
                                series_name = title_element.text.strip()
                                
                                if href and series_name:
                                    # Convert relative URL th√†nh absolute URL
                                    full_url = urljoin(self.base_url, href)
                                    
                                    # Tr√°nh tr√πng l·∫∑p
                                    if not any(s['url'] == full_url for s in series_data):
                                        series_data.append({
                                            'name': series_name + " (Ng∆∞ng s·∫£n xu·∫•t)",
                                            'url': full_url,
                                            'type': 'discontinued'
                                        })
                                        logger.debug(f"‚úÖ T√¨m th·∫•y discontinued series: {series_name}")
                                
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è L·ªói khi extract discontinued series link: {str(e)}")
                                continue
                        
                        logger.info(f"‚úÖ T√¨m th·∫•y {len(discontinued_series_links)} discontinued series")
                        
                except NoSuchElementException:
                    logger.info("Kh√¥ng t√¨m th·∫•y discontinued switch")
                except Exception as e:
                    logger.warning(f"L·ªói khi x·ª≠ l√Ω discontinued switch: {str(e)}")
                
                # N·∫øu t√¨m th·∫•y series, break kh·ªèi retry loop
                if series_data:
                    break
                    
                # N·∫øu kh√¥ng t√¨m th·∫•y series, th·ª≠ l·∫°i
                if attempt < max_retries - 1:
                    logger.warning(f"üîÑ Kh√¥ng t√¨m th·∫•y series, th·ª≠ l·∫°i sau 2 gi√¢y...")
                    time.sleep(2)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói l·∫ßn th·ª≠ {attempt + 1} khi extract series t·ª´ {category_url}: {str(e)}")
                if attempt < max_retries - 1:
                    logger.info("üîÑ S·∫Ω th·ª≠ l·∫°i...")
                    time.sleep(3)
                    continue
                else:
                    logger.error("üí• ƒê√£ h·∫øt s·ªë l·∫ßn th·ª≠, fallback sang requests method")
                    return self.extract_series_fallback(category_url)
            finally:
                # ƒê·∫£m b·∫£o driver ƒë∆∞·ª£c ƒë√≥ng sau m·ªói attempt
                if driver and attempt == max_retries - 1:  # Ch·ªâ ƒë√≥ng ·ªü attempt cu·ªëi
                    self.close_driver(driver)
                    driver = None
        
        # Cleanup final driver n·∫øu c√≤n
        if driver:
            self.close_driver(driver)
        
        self.stats["series_found"] += len(series_data)
        logger.info(f"üéØ T·ªïng c·ªông t√¨m th·∫•y {len(series_data)} series t·ª´ {category_url}")
        
        return series_data

    def extract_series_fallback(self, category_url):
        """
        Fallback method ƒë·ªÉ extract series s·ª≠ d·ª•ng requests + BeautifulSoup
        Khi Selenium g·∫∑p v·∫•n ƒë·ªÅ
        
        Args:
            category_url: URL c·ªßa category page
            
        Returns:
            list: Danh s√°ch series URLs v√† metadata
        """
        try:
            logger.info(f"üîÑ S·ª≠ d·ª•ng fallback method cho category: {category_url}")
            
            html = self.get_html_content(category_url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            series_data = []
            
            # T√¨m normal series links
            normal_series = soup.find_all('a', class_='prd-seriesCard-link')
            for link in normal_series:
                try:
                    href = link.get('href')
                    link_label = link.find(class_='prd-seriesCard-linkLabel')
                    series_name = link_label.text.strip() if link_label else ""
                    
                    if href and series_name:
                        full_url = urljoin(self.base_url, href)
                        series_data.append({
                            'name': series_name,
                            'url': full_url,
                            'type': 'normal'
                        })
                        logger.debug(f"üîç Fallback t√¨m th·∫•y: {series_name} - {full_url}")
                        
                except Exception as e:
                    logger.debug(f"L·ªói khi x·ª≠ l√Ω normal series fallback: {str(e)}")
                    continue
            
            # T√¨m discontinued series links
            discontinued_series = soup.find_all('a', class_='prd-seriesCardDiscontinued')
            for link in discontinued_series:
                try:
                    href = link.get('href')
                    title_element = link.find(class_='prd-seriesCardDiscontinued-title')
                    series_name = title_element.text.strip() if title_element else ""
                    
                    if href and series_name:
                        full_url = urljoin(self.base_url, href)
                        series_data.append({
                            'name': series_name + " (Ng∆∞ng s·∫£n xu·∫•t)",
                            'url': full_url,
                            'type': 'discontinued'
                        })
                        logger.debug(f"üîç Fallback t√¨m th·∫•y discontinued: {series_name}")
                        
                except Exception as e:
                    logger.debug(f"L·ªói khi x·ª≠ l√Ω discontinued series fallback: {str(e)}")
                    continue
            
            logger.info(f"üéØ Fallback method t√¨m th·∫•y {len(series_data)} series")
            return series_data
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong fallback method: {str(e)}")
            return []

    def extract_products_from_series(self, series_url):
        """
        Extract t·∫•t c·∫£ products t·ª´ series models page v·ªõi x·ª≠ l√Ω discontinued
        S·ª≠ d·ª•ng Selenium ƒë·ªÉ click n√∫t show discontinued models
        
        Args:
            series_url: URL c·ªßa series page
            
        Returns:
            list: Danh s√°ch product URLs v√† metadata
        """
        driver = None
        products_data = []
        max_retries = 2
        
        # T·∫°o models URL b·∫±ng c√°ch th√™m /models/ v√†o cu·ªëi
        models_url = series_url.rstrip('/') + '/models/'
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"üîÑ Th·ª≠ l·∫ßn {attempt + 1}/{max_retries} extract products t·ª´ {models_url}")
                
                # T·∫°o driver m·ªõi cho m·ªói attempt
                if driver:
                    self.close_driver(driver)
                driver = self.get_driver()
                
                self.emit_progress(
                    20, 
                    f"ƒêang thu th·∫≠p s·∫£n ph·∫©m t·ª´ series (l·∫ßn th·ª≠ {attempt + 1})",
                    f"URL: {models_url}"
                )
                
                # Load trang models v·ªõi timeout
                driver.set_page_load_timeout(45)
                driver.get(models_url)
                
                # ƒê·ª£i trang load ho√†n to√†n
                time.sleep(3)
                
                # ƒê·ª£i cho models page load
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".prd-layout-modelIndexHeader"))
                    )
                except TimeoutException:
                    logger.warning(f"‚è∞ Timeout khi ƒë·ª£i models page load, th·ª≠ fallback method")
                    return self.extract_products_fallback(models_url)
                
                # Click discontinued models switch ƒë·ªÉ hi·ªÉn th·ªã t·∫•t c·∫£ models
                try:
                    discontinued_switch = driver.find_element(
                        By.CSS_SELECTOR, 
                        ".prd-layout-modelIndexHeader button[data-controller*='switch-discontinued']"
                    )
                    if discontinued_switch:
                        # Ki·ªÉm tra switch c√≥ ƒëang OFF kh√¥ng (aria-checked="false")
                        is_checked = discontinued_switch.get_attribute('aria-checked')
                        if is_checked == 'false':
                            # Scroll ƒë·∫øn switch
                            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", discontinued_switch)
                            time.sleep(1)
                            
                            # Click switch ƒë·ªÉ hi·ªÉn th·ªã discontinued models
                            driver.execute_script("arguments[0].click();", discontinued_switch)
                            logger.info("‚úÖ ƒê√£ click switch ƒë·ªÉ hi·ªÉn th·ªã discontinued models")
                            time.sleep(4)  # ƒê·ª£i content load
                        else:
                            logger.info("‚úÖ Switch ƒë√£ ƒë∆∞·ª£c b·∫≠t s·∫µn")
                            
                except NoSuchElementException:
                    logger.info("Kh√¥ng t√¨m th·∫•y discontinued models switch")
                except Exception as e:
                    logger.warning(f"L·ªói khi x·ª≠ l√Ω discontinued models switch: {str(e)}")
                
                # L·∫•y t·∫•t c·∫£ product links t·ª´ models page
                # Th·ª≠ c√°c selector kh√°c nhau ƒë·ªÉ t√¨m product links
                product_links = []
                
                selectors = [
                    "a[href*='/models/'][href$='/']",  # Links c√≥ /models/ v√† k·∫øt th√∫c b·∫±ng /
                    "a[href*='products/'][href*='models']",  # Links ch·ª©a products v√† models
                    ".prd-modelCard a",  # Model card links
                    ".prd-model-link",  # Model links
                    "a[data-ga-label*='model']"  # Links c√≥ GA label model
                ]
                
                for selector in selectors:
                    try:
                        links = driver.find_elements(By.CSS_SELECTOR, selector)
                        if links:
                            product_links = links
                            logger.info(f"‚úÖ T√¨m th·∫•y {len(links)} product links v·ªõi selector: {selector}")
                            break
                    except Exception as e:
                        logger.debug(f"L·ªói v·ªõi selector {selector}: {str(e)}")
                        continue
                
                # N·∫øu kh√¥ng t√¨m th·∫•y b·∫±ng selector, th·ª≠ t√¨m t·∫•t c·∫£ links ch·ª©a model
                if not product_links:
                    all_links = driver.find_elements(By.CSS_SELECTOR, "a[href]")
                    for link in all_links:
                        href = link.get_attribute('href')
                        if href and '/models/' in href and href != models_url:
                            product_links.append(link)
                    logger.info(f"üîç Fallback: T√¨m th·∫•y {len(product_links)} product links")
                
                for link_element in product_links:
                    try:
                        href = link_element.get_attribute('href')
                        
                        # L·∫•y product name t·ª´ text ho·∫∑c alt attribute
                        product_name = ""
                        try:
                            product_name = link_element.text.strip()
                            if not product_name:
                                # Th·ª≠ l·∫•y t·ª´ alt attribute c·ªßa img trong link
                                img = link_element.find_element(By.CSS_SELECTOR, "img")
                                product_name = img.get_attribute('alt') or ""
                        except:
                            pass
                        
                        if href and href != models_url:
                            # Filter ch·ªâ l·∫•y link s·∫£n ph·∫©m th·∫≠t s·ª±
                            if (('/models/' in href) 
                                and href.count('/') >= 6  # ƒê·∫£m b·∫£o c√≥ ƒë·ªß depth cho product URL
                                and not href.endswith('/models/')  # Kh√¥ng ph·∫£i models root
                                and 'category' not in href.lower()):
                                
                                # Convert relative URL th√†nh absolute URL
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                elif not href.startswith('http'):
                                    full_url = urljoin(models_url, href)
                                else:
                                    full_url = href
                                
                                # Extract product code t·ª´ URL (ph·∫ßn cu·ªëi)
                                product_code = full_url.rstrip('/').split('/')[-1]
                                if not product_name:
                                    product_name = product_code
                                
                                products_data.append({
                                    'name': product_name,
                                    'code': product_code,
                                    'url': full_url
                                })
                                logger.debug(f"‚úÖ T√¨m th·∫•y s·∫£n ph·∫©m: {product_name} ({product_code}) - {full_url}")
                            
                    except Exception as e:
                        logger.warning(f"L·ªói khi extract product link: {str(e)}")
                        continue
                
                # N·∫øu t√¨m th·∫•y products, break kh·ªèi retry loop
                if products_data:
                    break
                    
                # N·∫øu kh√¥ng t√¨m th·∫•y products, th·ª≠ l·∫°i
                if attempt < max_retries - 1:
                    logger.warning(f"üîÑ Kh√¥ng t√¨m th·∫•y products, th·ª≠ l·∫°i sau 3 gi√¢y...")
                    time.sleep(3)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói l·∫ßn th·ª≠ {attempt + 1} khi extract products t·ª´ {models_url}: {str(e)}")
                if attempt < max_retries - 1:
                    logger.info("üîÑ S·∫Ω th·ª≠ l·∫°i...")
                    time.sleep(5)
                    continue
                else:
                    logger.error("üí• ƒê√£ h·∫øt s·ªë l·∫ßn th·ª≠, fallback sang requests method")
                    # Cleanup driver tr∆∞·ªõc khi fallback
                    if driver:
                        self.close_driver(driver)
                        driver = None
                    products_data = self.extract_products_fallback(models_url)
                    break
            finally:
                # ƒê·∫£m b·∫£o driver ƒë∆∞·ª£c ƒë√≥ng sau m·ªói attempt
                if driver and attempt == max_retries - 1:  # Ch·ªâ ƒë√≥ng ·ªü attempt cu·ªëi
                    self.close_driver(driver)
                    driver = None
        
        # Cleanup final driver n·∫øu c√≤n
        if driver:
            self.close_driver(driver)
        
        self.stats["products_found"] += len(products_data)
        logger.info(f"üéØ T·ªïng c·ªông t√¨m th·∫•y {len(products_data)} s·∫£n ph·∫©m t·ª´ {models_url}")
        
        return products_data

    def extract_products_fallback(self, models_url):
        """
        Fallback method ƒë·ªÉ extract products s·ª≠ d·ª•ng requests + BeautifulSoup
        Khi Selenium g·∫∑p v·∫•n ƒë·ªÅ v·ªõi models page
        
        Args:
            models_url: URL c·ªßa models page
            
        Returns:
            list: Danh s√°ch product URLs v√† metadata
        """
        try:
            logger.info(f"üîÑ S·ª≠ d·ª•ng fallback method cho models: {models_url}")
            
            html = self.get_html_content(models_url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            products_data = []
            
            # T√¨m t·∫•t c·∫£ links c√≥ th·ªÉ l√† products
            potential_links = soup.find_all('a', href=True)
            
            for link in potential_links:
                try:
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    
                    if href and href != models_url:
                        # Filter links s·∫£n ph·∫©m v·ªõi ti√™u ch√≠ m·ªõi
                        if (('/models/' in href)
                            and href.count('/') >= 6
                            and not href.endswith('/models/')
                            and 'category' not in href.lower()):
                            
                            # Convert th√†nh absolute URL
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            elif href.startswith('http'):
                                full_url = href
                            else:
                                full_url = urljoin(models_url, href)
                            
                            # Extract product code t·ª´ URL
                            product_code = full_url.rstrip('/').split('/')[-1]
                            product_name = text if text else product_code
                            
                            # Ki·ªÉm tra kh√¥ng tr√πng l·∫∑p
                            if not any(p['url'] == full_url for p in products_data):
                                products_data.append({
                                    'name': product_name,
                                    'code': product_code,
                                    'url': full_url
                                })
                                logger.debug(f"Fallback t√¨m th·∫•y: {product_name} ({product_code})")
                        
                except Exception as e:
                    logger.debug(f"L·ªói khi x·ª≠ l√Ω link fallback: {str(e)}")
                    continue
            
            logger.info(f"Fallback method t√¨m th·∫•y {len(products_data)} s·∫£n ph·∫©m")
            return products_data
            
        except Exception as e:
            logger.error(f"L·ªói trong products fallback method: {str(e)}")
            return []

    def extract_product_details(self, product_url):
        """
        Extract chi ti·∫øt s·∫£n ph·∫©m t·ª´ product page Keyence
        L·∫•y t√™n s·∫£n ph·∫©m, m√£ s·∫£n ph·∫©m, th√¥ng s·ªë k·ªπ thu·∫≠t v√† ·∫£nh s·∫£n ph·∫©m
        
        Args:
            product_url: URL c·ªßa product page
            
        Returns:
            dict: Th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
        """
        try:
            self.emit_progress(
                50, 
                f"ƒêang c√†o th√¥ng tin s·∫£n ph·∫©m Keyence",
                f"URL: {product_url}"
            )
            
            html = self.get_html_content(product_url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            product_data = {
                'product_code': '',
                'product_name': '',
                'full_product_name': '',
                'category': '',
                'series': '',
                'image_url': '',
                'specifications': {},
                'specs_html_original': '',
                'original_url': product_url
            }
            
            # 1. L·∫•y m√£ s·∫£n ph·∫©m t·ª´ span.prd-utility-body-medium
            product_code_element = soup.find('span', class_='prd-utility-body-medium prd-utility-block')
            if product_code_element:
                product_data['product_code'] = product_code_element.get_text(strip=True)
            
            # 2. L·∫•y c√°c th√†nh ph·∫ßn ƒë·ªÉ t·∫°o t√™n s·∫£n ph·∫©m theo y√™u c·∫ßu ch√≠nh x√°c
            # Ph·∫ßn 1: Category t·ª´ breadcrumb navigation - ch√≠nh x√°c theo y√™u c·∫ßu
            category_name = ''
            
            # T√¨m element ch√≠nh x√°c nh∆∞ user ch·ªâ ƒë·ªãnh:
            # <a class="prd-inlineLink prd-utility-focusRing" href="/products/sensor/photoelectric/">
            # <span class="prd-inlineLink-label">C·∫£m bi·∫øn quang ƒëi·ªán</span></a>
            
            # ∆Øu ti√™n t√¨m link ch√≠nh x√°c v·ªõi class "prd-inlineLink prd-utility-focusRing"
            exact_link = soup.find('a', class_='prd-inlineLink prd-utility-focusRing')
            if exact_link:
                label_span = exact_link.find('span', class_='prd-inlineLink-label')
                if label_span:
                    category_name = label_span.get_text(strip=True)
                    # Lo·∫°i b·ªè "Trang ch·ªß" n·∫øu ƒë√≥ l√† k·∫øt qu·∫£
                    if category_name.lower() in ['trang ch·ªß', 'home']:
                        category_name = ''
            
            # Fallback: t√¨m trong t·∫•t c·∫£ breadcrumb links n·∫øu ch∆∞a c√≥
            if not category_name:
                prd_inline_links = soup.find_all('a', class_='prd-inlineLink')
                
                # First pass: t√¨m exact match "C·∫£m bi·∫øn quang ƒëi·ªán"
                for link in prd_inline_links:
                    label_span = link.find('span', class_='prd-inlineLink-label')
                    if label_span:
                        text = label_span.get_text(strip=True)
                        if text.lower() == 'c·∫£m bi·∫øn quang ƒëi·ªán':
                            category_name = text
                            break
                
                # Second pass: t√¨m specific sensor types n·∫øu ch∆∞a c√≥ exact match
                if not category_name:
                    specific_sensors = ['c·∫£m bi·∫øn s·ª£i quang', 'c·∫£m bi·∫øn laser', 'c·∫£m bi·∫øn ti·ªám c·∫≠n', 
                                      'c·∫£m bi·∫øn v·ªã tr√≠', 'c·∫£m bi·∫øn h√¨nh ·∫£nh', 'c·∫£m bi·∫øn √°p su·∫•t', 
                                      'c·∫£m bi·∫øn nhi·ªát ƒë·ªô', 'c·∫£m bi·∫øn ƒëo m·ª©c']
                    for link in prd_inline_links:
                        label_span = link.find('span', class_='prd-inlineLink-label')
                        if label_span:
                            text = label_span.get_text(strip=True)
                            if text.lower() in specific_sensors:
                                category_name = text
                                break
                
                # Third pass: fallback to general "c·∫£m bi·∫øn"
                if not category_name:
                    for link in prd_inline_links:
                        label_span = link.find('span', class_='prd-inlineLink-label')
                        if label_span:
                            text = label_span.get_text(strip=True)
                            if text.lower() == 'c·∫£m bi·∫øn':
                                category_name = text
                                break
                
                # Fallback cu·ªëi: l·∫•y link ƒë·∫ßu ti√™n kh√¥ng ph·∫£i "Trang ch·ªß"
                if not category_name:
                    for link in prd_inline_links:
                        label_span = link.find('span', class_='prd-inlineLink-label')
                        if label_span:
                            text = label_span.get_text(strip=True)
                            if text.lower() not in ['trang ch·ªß', 'home', 'products', 's·∫£n ph·∫©m']:
                                category_name = text
                                break
            
            product_data['category'] = category_name
            
            # Ph·∫ßn 3: M√¥ t·∫£ t·ª´ span.prd-utility-heading-1
            description = ''
            description_element = soup.find('span', class_='prd-utility-heading-1 prd-utility-marginBottom-2 prd-utility-block')
            if not description_element:
                description_element = soup.find('span', class_='prd-utility-heading-1')
            if not description_element:
                description_element = soup.find('h1', class_='prd-utility-heading-1')
            
            if description_element:
                description = description_element.get_text(strip=True)
            
            # 3. Gh√©p t√™n s·∫£n ph·∫©m theo th·ª© t·ª± ch√≠nh x√°c: Category + Product Code + Description + KEYENCE
            name_parts = []
            if category_name:
                name_parts.append(category_name)
            if product_data['product_code']:
                name_parts.append(product_data['product_code'])
            if description:
                name_parts.append(description)
            name_parts.append('KEYENCE')
            
            product_name = ' '.join(name_parts)
            product_data['product_name'] = product_name
            product_data['full_product_name'] = product_name
            
            # 5. L·∫•y ·∫£nh s·∫£n ph·∫©m t·ª´ img.prd-modelIntroduction-image
            img_element = soup.find('img', class_='prd-modelIntroduction-image')
            if img_element and img_element.get('src'):
                image_src = img_element['src']
                # Convert relative URL th√†nh absolute URL
                if image_src.startswith('/'):
                    product_data['image_url'] = urljoin(self.base_url, image_src)
                else:
                    product_data['image_url'] = image_src
            
            # 6. L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t theo chu·∫©n h√£ng v·ªõi BeautifulSoup ƒë∆°n gi·∫£n
            product_data['specifications'] = []
            product_data['footnotes'] = {}
            
            # L∆∞u nguy√™n kh·ªëi HTML <section> th√¥ng s·ªë k·ªπ thu·∫≠t c·ªßa trang g·ªëc (∆∞u ti√™n d√πng)
            original_specs_html = self._extract_original_specs_html(soup)
            # L√†m s·∫°ch theo y√™u c·∫ßu v√† ch√®n b·∫£n quy·ªÅn tr∆∞·ªõc footnotes n·∫øu c√≥
            cleaned_specs_html = self.clean_specs(original_specs_html) if original_specs_html else ''
            product_data['specs_html_original'] = cleaned_specs_html or original_specs_html or ''

            # Parse specs theo chu·∫©n h√£ng
            specs_items = self._parse_keyence_specs(soup)
            product_data['specifications'] = specs_items
            
            # Parse footnotes ri√™ng bi·ªát  
            footnotes = self._parse_keyence_footnotes(soup)
            product_data['footnotes'] = footnotes
            
            # 7. Extract series name t·ª´ URL n·∫øu ch∆∞a c√≥
            if not product_data.get('series'):
                # Series name th∆∞·ªùng l√† ph·∫ßn tr∆∞·ªõc /models/ trong URL
                if '/models/' in product_url:
                    series_part = product_url.split('/models/')[0]
                    series_name = series_part.split('/')[-1].upper()
                    product_data['series'] = series_name
            
            logger.info(f"‚úÖ ƒê√£ extract th√¥ng tin s·∫£n ph·∫©m: {product_data['product_code']}")
            return product_data
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ {product_url}: {str(e)}")
            return None

    def process_image_with_white_background(self, image_url, save_path, product_code):
        """
        T·∫£i v√† x·ª≠ l√Ω ·∫£nh s·∫£n ph·∫©m Keyence, th√™m n·ªÅn tr·∫Øng v√† chuy·ªÉn sang WebP
        Keyence images th∆∞·ªùng kh√¥ng c√≥ n·ªÅn, c·∫ßn th√™m white background
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            save_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
            product_code: M√£ s·∫£n ph·∫©m
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            if not image_url or not product_code:
                return False
            
            # T·∫£i ·∫£nh
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # M·ªü ·∫£nh t·ª´ bytes
            original_image = Image.open(BytesIO(response.content))
            
            # Ch·ªâ th√™m n·ªÅn tr·∫Øng n·∫øu c·∫ßn, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc g·ªëc
            processed_image = self.add_white_background_keyence(original_image)
            
            # T·∫°o t√™n file theo logic standardize_filename_keyence
            filename = f"{standardize_filename_keyence(product_code)}.webp"
            full_path = os.path.join(save_path, filename)
            
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(save_path, exist_ok=True)
            
            # L∆∞u tr·ª±c ti·∫øp sang WebP ƒë·ªÉ gi·ªØ nguy√™n ch·∫•t l∆∞·ª£ng
            processed_image.save(full_path, 'WebP', quality=95, method=6)
            result = True
            
            if result:
                self.stats["images_downloaded"] += 1
                logger.info(f"‚úÖ ƒê√£ t·∫£i v√† chuy·ªÉn ƒë·ªïi ·∫£nh Keyence: {filename}")
                return True
            else:
                self.stats["failed_images"] += 1
                logger.error(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi ·∫£nh WebP: {filename}")
                return False
                
        except Exception as e:
            self.stats["failed_images"] += 1
            logger.error(f"‚ùå L·ªói khi t·∫£i ·∫£nh Keyence t·ª´ {image_url}: {str(e)}")
            return False

    def add_white_background_keyence(self, image):
        """
        Ch·ªâ th√™m n·ªÅn tr·∫Øng v√†o ·∫£nh Keyence n·∫øu c·∫ßn, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc g·ªëc
        Keyence images th∆∞·ªùng transparent, c·∫ßn white background cho WordPress
        
        Args:
            image: PIL Image object
            
        Returns:
            PIL Image: ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v·ªõi n·ªÅn tr·∫Øng, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc
        """
        try:
            # L·∫•y k√≠ch th∆∞·ªõc g·ªëc
            original_size = image.size
            
            # N·∫øu ·∫£nh ƒë√£ c√≥ n·ªÅn RGB r·ªìi th√¨ return lu√¥n
            if image.mode == 'RGB':
                return image
            
            # Convert ·∫£nh sang RGBA ƒë·ªÉ preserve transparency
            if image.mode != 'RGBA':
                image = image.convert('RGBA')
            
            # T·∫°o ·∫£nh n·ªÅn tr·∫Øng v·ªõi k√≠ch th∆∞·ªõc g·ªëc
            background = Image.new('RGB', original_size, (255, 255, 255))
            
            # Paste ·∫£nh g·ªëc l√™n n·ªÅn tr·∫Øng (gi·ªØ nguy√™n k√≠ch th∆∞·ªõc)
            background.paste(image, (0, 0), image)
            
            return background
            
        except Exception as e:
            logger.error(f"L·ªói khi th√™m white background: {str(e)}")
            # Return ·∫£nh g·ªëc convert sang RGB n·∫øu c√≥ l·ªói
            return image.convert('RGB') if image.mode != 'RGB' else image

    def create_excel_with_keyence_specs(self, products_data, excel_path):
        """
        T·∫°o file Excel v·ªõi th√¥ng s·ªë k·ªπ thu·∫≠t theo ƒë·ªãnh d·∫°ng Keyence
        T·∫°o b·∫£ng HTML specifications theo format y√™u c·∫ßu v·ªõi Copyright Haiphongtech.vn
        
        Args:
            products_data: Danh s√°ch d·ªØ li·ªáu s·∫£n ph·∫©m
            excel_path: ƒê∆∞·ªùng d·∫´n file Excel
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            if not products_data:
                logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m Keyence ƒë·ªÉ xu·∫•t Excel")
                return False
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho DataFrame
            excel_data = []
            
            for product in products_data:
                # T·∫°o b·∫£ng HTML specifications theo format y√™u c·∫ßu
                specs_html = self.create_keyence_specifications_table_html(product)
                
                excel_row = {
                    'M√£ s·∫£n ph·∫©m': product.get('product_code', ''),
                    'T√™n s·∫£n ph·∫©m ti·∫øng Anh': product.get('product_name', ''),
                    'T√™n s·∫£n ph·∫©m ti·∫øng Vi·ªát': product.get('full_product_name', ''),
                    'Category': product.get('category', ''),
                    'Series': product.get('series', ''),
                    'Link s·∫£n ph·∫©m': product.get('original_url', ''),
                    'Link ·∫£nh': product.get('image_url', ''),
                    'Th√¥ng s·ªë k·ªπ thu·∫≠t HTML': specs_html,
                    'S·ªë l∆∞·ª£ng th√¥ng s·ªë': len(product.get('specifications', [])),
                    'S·ªë l∆∞·ª£ng footnotes': len(product.get('footnotes', {}))
                }
                excel_data.append(excel_row)
            
            # T·∫°o DataFrame
            df = pd.DataFrame(excel_data)
            
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(os.path.dirname(excel_path), exist_ok=True)
            
            # Xu·∫•t ra Excel v·ªõi formatting
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='Keyence_Products')
                
                # L·∫•y worksheet ƒë·ªÉ format
                worksheet = writer.sheets['Keyence_Products']
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)  # Gi·ªõi h·∫°n width t·ªëi ƒëa
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logger.info(f"‚úÖ ƒê√£ t·∫°o file Excel Keyence: {excel_path} v·ªõi {len(products_data)} s·∫£n ph·∫©m")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o file Excel Keyence {excel_path}: {str(e)}")
            return False

    def create_keyence_specifications_table_html(self, product):
        """
        Tr·∫£ v·ªÅ nguy√™n kh·ªëi HTML th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ website n·∫øu c√≥.
        N·∫øu kh√¥ng c√≥, fallback t·∫°o b·∫£ng 2-column chu·∫©n "Th√¥ng s·ªë / Gi√° tr·ªã".
        
        Args:
            product: D·ªØ li·ªáu s·∫£n ph·∫©m Keyence
            
        Returns:
            str: HTML table string 2-column
        """
        try:
            # 1) ∆Øu ti√™n d√πng HTML g·ªëc t·ª´ website n·∫øu ƒë√£ l∆∞u
            original_html = product.get('specs_html_original', '')
            if original_html:
                return original_html

            # 2) Fallback: t·∫°o b·∫£ng 2-column ƒë∆°n gi·∫£n
            product_code = product.get('product_code', '')
            specifications = product.get('specifications', [])
            footnotes = product.get('footnotes', {})
            
            # B·∫Øt ƒë·∫ßu v·ªõi b·∫£ng 2-column chu·∫©n
            rows = [
                '<table id="specifications" border="1" cellpadding="8" cellspacing="0" style="border-collapse:collapse;width:100%;font-family:Arial;">',
                '<thead><tr style="background:#f2f2f2;"><th>Th√¥ng s·ªë</th><th>Gi√° tr·ªã</th></tr></thead>',
                '<tbody>'
            ]
            
            # Th√™m m√£ s·∫£n ph·∫©m ƒë·∫ßu ti√™n
            rows.append(f'<tr><td><strong>M·∫´u</strong></td><td>{product_code}</td></tr>')
            
            # Th√™m t·∫•t c·∫£ specifications t·ª´ parsed data
            for item in specifications:
                key = item.get('key', '')
                value = item.get('value', '')
                
                if key and value:
                    # Escape HTML ƒë·ªÉ tr√°nh conflict
                    key_escaped = key.replace('<', '&lt;').replace('>', '&gt;')
                    value_escaped = value.replace('<', '&lt;').replace('>', '&gt;')
                    
                    rows.append(f'<tr><td><strong>{key_escaped}</strong></td><td>{value_escaped}</td></tr>')
            
            # Th√™m footnotes n·∫øu c√≥
            if footnotes:
                footnote_content = ""
                for attributeid, content in footnotes.items():
                    footnote_content += content + " "
                
                footnote_content = footnote_content.strip()
                if footnote_content:
                    footnote_escaped = footnote_content.replace('<', '&lt;').replace('>', '&gt;')
                    rows.append(f'<tr><td><strong>Ghi ch√∫</strong></td><td>{footnote_escaped}</td></tr>')
            
            # Th√™m Copyright
            rows.append('<tr><td><strong>Copyright</strong></td><td>Haiphongtech.vn</td></tr>')
            
            # ƒê√≥ng b·∫£ng
            rows.append('</tbody></table>')
            
            return '\n'.join(rows)
            
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o HTML table cho s·∫£n ph·∫©m Keyence: {str(e)}")
            return ""

    def crawl_products(self, category_urls):
        """
        Method ch√≠nh ƒë·ªÉ c√†o d·ªØ li·ªáu s·∫£n ph·∫©m Keyence t·ª´ danh s√°ch category URLs
        Full workflow: Category ‚Üí Series ‚Üí Products ‚Üí Details ‚Üí Images ‚Üí Excel
        
        Args:
            category_urls: Danh s√°ch URLs c·ªßa c√°c categories
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£
        """
        start_time = time.time()
        timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
        result_dir = os.path.join(self.output_root, f"KeyenceProducts_{timestamp}")
        os.makedirs(result_dir, exist_ok=True)
        
        self.emit_progress(0, "B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu Keyence", f"S·∫Ω x·ª≠ l√Ω {len(category_urls)} categories")
        
        # Process each category v·ªõi retry logic
        for i, category_url in enumerate(category_urls):
            category_success = False
            max_category_retries = 2
            
            for category_attempt in range(max_category_retries):
                logger.info(f"üîÑ Category attempt {category_attempt + 1}/{max_category_retries} for: {category_url}")
                category_success = self._process_single_keyence_category(
                    category_url, i, len(category_urls), result_dir
                )
                
                if category_success:
                    logger.info(f"‚úÖ Category th√†nh c√¥ng: {category_url}")
                    break
                elif category_attempt < max_category_retries - 1:
                    logger.warning(f"‚ö†Ô∏è Category attempt {category_attempt + 1} th·∫•t b·∫°i, th·ª≠ l·∫°i sau 10 gi√¢y...")
                    time.sleep(10)
                else:
                    logger.error(f"‚ùå Category th·∫•t b·∫°i ho√†n to√†n sau {max_category_retries} l·∫ßn th·ª≠: {category_url}")
        
        # Ho√†n th√†nh
        end_time = time.time()
        duration = end_time - start_time
        
        self.emit_progress(100, f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {self.stats['categories_processed']} categories")
        
        # Log th·ªëng k√™ cu·ªëi c√πng
        logger.info("=== TH·ªêNG K√ä CRAWLER KEYENCE ===")
        logger.info(f"Th·ªùi gian th·ª±c hi·ªán: {duration:.2f} gi√¢y")
        logger.info(f"Categories ƒë√£ x·ª≠ l√Ω: {self.stats['categories_processed']}")
        logger.info(f"Series t√¨m th·∫•y: {self.stats['series_found']}")
        logger.info(f"S·∫£n ph·∫©m t√¨m th·∫•y: {self.stats['products_found']}")
        logger.info(f"S·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω: {self.stats['products_processed']}")
        logger.info(f"·∫¢nh ƒë√£ t·∫£i: {self.stats['images_downloaded']}")

        logger.info(f"Request th·∫•t b·∫°i: {self.stats['failed_requests']}")
        logger.info(f"·∫¢nh th·∫•t b·∫°i: {self.stats['failed_images']}")
        
        return result_dir

    def _process_single_keyence_category(self, category_url, category_index, total_categories, result_dir):
        """
        Process m·ªôt category Keyence v·ªõi error handling v√† retry logic
        Full workflow ƒë·ªÉ t·∫°o folder c·∫•u tr√∫c theo y√™u c·∫ßu
        
        Args:
            category_url: URL c·ªßa category
            category_index: Index c·ªßa category trong danh s√°ch
            total_categories: T·ªïng s·ªë categories
            result_dir: Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω category {category_index+1}/{total_categories}: {category_url}")
            
            # Extract category name t·ª´ URL
            category_name = self.extract_category_name_from_url(category_url)
            category_dir = os.path.join(result_dir, sanitize_folder_name(category_name))
            os.makedirs(category_dir, exist_ok=True)
            
            # T·∫°o th∆∞ m·ª•c images theo y√™u c·∫ßu
            images_dir = os.path.join(category_dir, "images")
            os.makedirs(images_dir, exist_ok=True)
            
            # Progress calculation c·∫£i thi·ªán
            category_progress_base = int((category_index / total_categories) * 90)
            
            self.emit_progress(
                category_progress_base,
                f"ƒêang x·ª≠ l√Ω category [{category_index+1}/{total_categories}]: {category_name}",
                f"URL: {category_url}"
            )
            
            # L·∫•y danh s√°ch series (bao g·ªìm discontinued)
            logger.info(f"üìã ƒêang thu th·∫≠p series t·ª´ category: {category_name}")
            series_list = self.extract_series_from_category(category_url)
            
            if not series_list:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y series n√†o t·ª´ {category_url}")
                return False
            
            logger.info(f"‚úÖ T√¨m th·∫•y {len(series_list)} series trong category: {category_name}")
            
            # Collect t·∫•t c·∫£ products t·ª´ c√°c series v·ªõi ƒëa lu·ªìng
            all_products_data = []
            
            def process_keyence_series(series_info):
                """Process m·ªôt series Keyence v√† tr·∫£ v·ªÅ danh s√°ch products v·ªõi details"""
                try:
                    series_url = series_info['url']
                    series_name = series_info['name']
                    series_type = series_info.get('type', 'normal')
                    
                    self.emit_progress(
                        category_progress_base + 20,
                        f"ƒêang x·ª≠ l√Ω series: {series_name} ({series_type})",
                        series_url
                    )
                    
                    # L·∫•y danh s√°ch products t·ª´ series
                    products_list = self.extract_products_from_series(series_url)
                    
                    if not products_list:
                        logger.warning(f"Kh√¥ng t√¨m th·∫•y products n√†o t·ª´ series {series_name}")
                        return []
                    
                    # Extract chi ti·∫øt cho t·ª´ng product
                    series_products = []
                    for product_info in products_list:  # L·∫•y to√†n b·ªô s·∫£n ph·∫©m
                        try:
                            product_details = self.extract_product_details(product_info['url'])
                            if product_details:
                                series_products.append(product_details)
                                self.stats["products_processed"] += 1
                        except Exception as e:
                            logger.error(f"L·ªói khi x·ª≠ l√Ω product {product_info['url']}: {str(e)}")
                            continue
                    
                    return series_products
                    
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω series {series_info['url']}: {str(e)}")
                    return []
            
            # X·ª≠ l√Ω series v·ªõi ƒëa lu·ªìng
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.max_workers, len(series_list))) as executor:
                future_to_series = {executor.submit(process_keyence_series, series): series for series in series_list}
                
                for future in concurrent.futures.as_completed(future_to_series):
                    series = future_to_series[future]
                    try:
                        series_products = future.result()
                        all_products_data.extend(series_products)
                        logger.info(f"Ho√†n th√†nh series {series['name']}: {len(series_products)} s·∫£n ph·∫©m")
                    except Exception as e:
                        logger.error(f"L·ªói khi x·ª≠ l√Ω series {series['name']}: {str(e)}")
            
            if not all_products_data:
                logger.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m n√†o t·ª´ category {category_name}")
                return False
            
            self.emit_progress(
                category_progress_base + 50,
                f"ƒêang t·∫£i ·∫£nh cho {len(all_products_data)} s·∫£n ph·∫©m",
                f"Category: {category_name}"
            )
            
            # T·∫£i ·∫£nh v·ªõi ƒëa lu·ªìng v√† white background processing
            def download_keyence_image(product):
                """Download v√† x·ª≠ l√Ω ·∫£nh cho m·ªôt s·∫£n ph·∫©m Keyence"""
                try:
                    if product.get('image_url') and product.get('product_code'):
                        return self.process_image_with_white_background(
                            product['image_url'],
                            images_dir,
                            product['product_code']
                        )
                except Exception as e:
                    logger.error(f"L·ªói khi t·∫£i ·∫£nh cho {product.get('product_code', 'Unknown')}: {str(e)}")
                    return False
            
            # Download ·∫£nh v·ªõi ƒëa lu·ªìng
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                image_futures = [executor.submit(download_keyence_image, product) for product in all_products_data]
                concurrent.futures.wait(image_futures)
            
            # T·∫°o file Excel v·ªõi Keyence specs
            excel_path = os.path.join(category_dir, f"{category_name}.xlsx")
            excel_success = self.create_excel_with_keyence_specs(all_products_data, excel_path)
            
            if excel_success:
                logger.info(f"‚úÖ ƒê√£ ho√†n th√†nh category: {category_name}")
                logger.info(f"üìä T·ªïng k·∫øt category {category_name}: {len(all_products_data)} s·∫£n ph·∫©m, {self.stats['images_downloaded']} ·∫£nh")
            else:
                logger.warning(f"‚ö†Ô∏è Ho√†n th√†nh category {category_name} nh∆∞ng kh√¥ng t·∫°o ƒë∆∞·ª£c Excel")
            
            self.stats["categories_processed"] += 1
            return True
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω category {category_url}: {str(e)}")
            logger.error(f"Chi ti·∫øt l·ªói: {error_details}")
            
            # Emit error
            self.emit_progress(
                int((category_index / total_categories) * 90),
                f"L·ªói khi x·ª≠ l√Ω category {category_index+1}/{total_categories}",
                f"L·ªói: {str(e)}"
            )
            
            # ƒê·∫£m b·∫£o cleanup resources n·∫øu c√≥
            try:
                import gc
                gc.collect()
            except:
                pass
            
            return False

    def extract_category_name_from_url(self, url):
        """Extract t√™n category t·ª´ URL"""
        try:
            # Parse URL ƒë·ªÉ l·∫•y ph·∫ßn cu·ªëi
            path = urlparse(url).path
            # L·∫•y ph·∫ßn cu·ªëi sau d·∫•u '/' cu·ªëi c√πng
            category_name = path.strip('/').split('/')[-1]
            # Chuy·ªÉn th√†nh title case v√† thay th·∫ø d·∫•u g·∫°ch ngang
            return category_name.replace('-', ' ').title()
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ extract category name t·ª´ {url}: {str(e)}")
            return "Unknown_Category"

if __name__ == "__main__":
    # Test crawler v·ªõi photoelectric category
    crawler = KeyenceCrawler()
    test_urls = ["https://www.keyence.com.vn/products/sensor/photoelectric/"]
    result_dir = crawler.crawl_products(test_urls)
    print(f"K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {result_dir}")
