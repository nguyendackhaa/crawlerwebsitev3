"""
Omron Product Crawler - C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ website industrial.omron.co.uk
Phi√™n b·∫£n: 1.0
T√°c gi·∫£: Auto-generated based on existing crawler patterns
Ng√†y t·∫°o: $(date)
"""

import os
import time
import requests
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import concurrent.futures
from queue import Queue
import pandas as pd
from PIL import Image, ImageEnhance
from io import BytesIO
import json
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from app.webp_converter import WebPConverter
import threading

# Selenium imports for dynamic content
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains

# Gemini AI imports for translation
import google.generativeai as genai

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def sanitize_folder_name(name):
    """L√†m s·∫°ch t√™n folder ƒë·ªÉ ph√π h·ª£p v·ªõi h·ªá ƒëi·ªÅu h√†nh"""
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá
    clean_name = re.sub(r'[<>:"/\\|?*]', '_', name)
    clean_name = re.sub(r'[^\w\s-]', '', clean_name).strip()
    clean_name = re.sub(r'[-\s]+', '_', clean_name)
    return clean_name if clean_name else 'Unknown_Category'

def standardize_filename(code):
    """Chu·∫©n h√≥a m√£ s·∫£n ph·∫©m th√†nh t√™n file h·ª£p l·ªá theo logic Google Apps Script"""
    # Ki·ªÉm tra add-on kit
    had_add_on_kit = re.search(r'add[\s\-]*on[\s\-]*kit', code, re.IGNORECASE)
    
    # L√†m s·∫°ch chu·ªói lo·∫°i b·ªè ghi ch√∫ coating, addon...
    clean_code = re.sub(r'\(with special coating\)', '', code, flags=re.IGNORECASE)
    clean_code = re.sub(r'\[with special coating\]', '', clean_code, flags=re.IGNORECASE)
    clean_code = re.sub(r'add[\s\-]*on[\s\-]*kit', '', clean_code, flags=re.IGNORECASE).strip()
    
    # Chu·∫©n h√≥a nh∆∞ standardize_filename
    def standardize(s):
        result = re.sub(r'[\\/:*?"<>|,=\s]', '-', s)  # Replace invalid chars with dash
        result = re.sub(r'-+', '-', result)  # Replace multiple dashes with single dash
        result = re.sub(r'[^a-zA-Z0-9\-_]', '', result)  # Keep only valid chars
        return result.strip('-')
    
    result = standardize(clean_code.upper())
    if had_add_on_kit:
        result += "-ADK"
    
    return result

class OmronCrawler:
    """
    Crawler chuy√™n d·ª•ng cho website industrial.omron.co.uk
    C√†o d·ªØ li·ªáu c·∫£m bi·∫øn v√† thi·∫øt b·ªã t·ª± ƒë·ªông h√≥a v·ªõi x·ª≠ l√Ω ƒëa lu·ªìng
    """
    
    # URL mapping ƒë·ªÉ t·ª± ƒë·ªông s·ª≠a c√°c category URLs ph·ªï bi·∫øn
    URL_MAPPINGS = {
        'proximity-sensors': 'inductive-sensors',
        'encoders': 'rotary-encoders', 
        'limit-switches': 'mechanical-sensors-limit-switches',
        'fiber-optic': 'fiber-optic-sensors-and-amplifiers',
        'plc': 'programmable-logic-controllers',
    }
    
    def __init__(self, output_root=None, max_workers=8, max_retries=3, socketio=None, gemini_api_key=None):
        """
        Kh·ªüi t·∫°o OmronCrawler
        
        Args:
            output_root: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u k·∫øt qu·∫£
            max_workers: S·ªë lu·ªìng t·ªëi ƒëa
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i khi request th·∫•t b·∫°i
            socketio: Socket.IO instance ƒë·ªÉ emit ti·∫øn tr√¨nh
            gemini_api_key: API key cho Gemini AI translation
        """
        self.output_root = output_root or os.path.join(os.getcwd(), "output_omron")
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.socketio = socketio
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(self.output_root, exist_ok=True)
        
        # Base URLs
        self.base_url = "https://industrial.omron.co.uk"
        
        # Kh·ªüi t·∫°o Gemini AI
        self.gemini_model = None
        api_key = gemini_api_key or os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY')
        
        if api_key:
            try:
                genai.configure(api_key=api_key)
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o Gemini AI th√†nh c√¥ng")
            except Exception as e:
                logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o Gemini AI: {str(e)}")
                self.gemini_model = None
        else:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ Gemini API key, s·∫Ω b·ªè qua vi·ªác d·ªãch t·ª± ƒë·ªông")
            logger.info("üí° ƒê·ªÉ s·ª≠ d·ª•ng d·ªãch t·ª± ƒë·ªông, h√£y thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY")
        
        # C·∫•u h√¨nh session v·ªõi retry strategy
        self.session = requests.Session()
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers m√¥ ph·ªèng tr√¨nh duy·ªát th·∫≠t
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-GB,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # C·∫•u h√¨nh Selenium WebDriver
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # Disable c√°c features c√≥ th·ªÉ g√¢y overlay/popup
        self.chrome_options.add_argument('--disable-extensions')
        self.chrome_options.add_argument('--disable-plugins')
        self.chrome_options.add_argument('--disable-javascript-dialogs')
        self.chrome_options.add_argument('--disable-notifications')
        self.chrome_options.add_argument('--disable-infobars')
        self.chrome_options.add_argument('--disable-popup-blocking')
        self.chrome_options.add_argument('--disable-default-apps')
        self.chrome_options.add_argument('--no-first-run')
        self.chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        # Disable logging ƒë·ªÉ gi·∫£m noise
        self.chrome_options.add_argument('--log-level=3')
        self.chrome_options.add_argument('--disable-logging')
        
        # Preferences ƒë·ªÉ disable tutorial v√† overlay
        prefs = {
            "profile.default_content_setting_values": {
                "notifications": 2,
                "popups": 2
            },
            "profile.managed_default_content_settings": {
                "images": 2
            }
        }
        self.chrome_options.add_experimental_option("prefs", prefs)
        
        # Th·ªëng k√™
        self.stats = {
            "categories_processed": 0,
            "series_found": 0,
            "products_found": 0,
            "products_processed": 0,
            "images_downloaded": 0,
            "failed_requests": 0,
            "failed_images": 0,
            "translations_completed": 0
        }
        
        logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o OmronCrawler v·ªõi Selenium support v√† Gemini AI")
    
    def get_driver(self):
        """T·∫°o m·ªôt WebDriver Chrome m·ªõi"""
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(10)
            return driver
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver: {e}")
            raise
    
    def close_driver(self, driver):
        """ƒê√≥ng WebDriver"""
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"L·ªói khi ƒë√≥ng WebDriver: {e}")
    
    def emit_progress(self, percent, message, detail=""):
        """Emit ti·∫øn tr√¨nh qua Socket.IO"""
        if self.socketio:
            self.socketio.emit('progress_update', {
                'percent': percent,
                'message': message,
                'detail': detail
            })
        else:
            print(f"[{percent}%] {message} - {detail}")
    
    def setup_gemini_ai(self, api_key):
        """
        Thi·∫øt l·∫≠p Gemini AI v·ªõi API key
        
        Args:
            api_key: Gemini API key
            
        Returns:
            bool: True n·∫øu thi·∫øt l·∫≠p th√†nh c√¥ng
        """
        if not api_key:
            logger.error("‚ùå API key kh√¥ng ƒë∆∞·ª£c cung c·∫•p")
            return False
            
        try:
            genai.configure(api_key=api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
            logger.info("‚úÖ ƒê√£ thi·∫øt l·∫≠p Gemini AI th√†nh c√¥ng")
            return True
        except Exception as e:
            logger.error(f"‚ùå L·ªói thi·∫øt l·∫≠p Gemini AI: {str(e)}")
            self.gemini_model = None
            return False
    
    def translate_with_gemini(self, text, target_language="Vietnamese"):
        """
        D·ªãch text sang target_language s·ª≠ d·ª•ng Gemini AI
        
        Args:
            text: Text c·∫ßn d·ªãch
            target_language: Ng√¥n ng·ªØ ƒë√≠ch
            
        Returns:
            str: Text ƒë√£ ƒë∆∞·ª£c d·ªãch
        """
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán c∆° b·∫£n
        if not text or not text.strip():
            return text
            
        # N·∫øu kh√¥ng c√≥ Gemini model, log v√† tr·∫£ v·ªÅ text g·ªëc
        if not self.gemini_model:
            logger.warning("Gemini AI ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p - s·∫Ω s·ª≠ d·ª•ng text g·ªëc")
            return text
        
        try:
            # T·ªëi ∆∞u prompt ƒë·ªÉ d·ªãch t·ªët h∆°n
            prompt = f"""D·ªãch ƒëo·∫°n text k·ªπ thu·∫≠t sau sang ti·∫øng Vi·ªát. 
Gi·ªØ nguy√™n:
- M√£ s·∫£n ph·∫©m, model number
- ƒê∆°n v·ªã ƒëo l∆∞·ªùng (mm, V, A, etc.)
- S·ªë li·ªáu k·ªπ thu·∫≠t
- T√™n th∆∞∆°ng hi·ªáu

Ch·ªâ d·ªãch:
- M√¥ t·∫£ s·∫£n ph·∫©m
- Thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
- T√≠nh nƒÉng v√† ƒë·∫∑c ƒëi·ªÉm

Text c·∫ßn d·ªãch: "{text}"

Tr·∫£ v·ªÅ ch·ªâ b·∫£n d·ªãch ti·∫øng Vi·ªát, kh√¥ng th√™m gi·∫£i th√≠ch:"""

            logger.debug(f"ƒêang d·ªãch text: {text[:100]}...")
            response = self.gemini_model.generate_content(prompt)
            
            if response and response.text:
                translated_text = response.text.strip()
                self.stats["translations_completed"] += 1
                logger.debug(f"D·ªãch th√†nh c√¥ng: {text[:50]} -> {translated_text[:50]}")
                return translated_text
            else:
                logger.warning(f"Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ Gemini cho text: {text[:50]}...")
                return text
                
        except Exception as e:
            logger.error(f"L·ªói khi d·ªãch v·ªõi Gemini cho text '{text[:50]}...': {str(e)}")
            return text

    def fix_category_url(self, url):
        """
        T·ª± ƒë·ªông s·ª≠a URL category n·∫øu kh√¥ng h·ª£p l·ªá
        
        Args:
            url: URL g·ªëc
            
        Returns:
            str: URL ƒë√£ ƒë∆∞·ª£c s·ª≠a (n·∫øu c·∫ßn)
        """
        if '/en/products/' not in url:
            return url
            
        # Extract category name t·ª´ URL
        category_slug = url.split('/en/products/')[-1].strip('/')
        
        # Ki·ªÉm tra trong mapping table
        if category_slug in self.URL_MAPPINGS:
            new_category = self.URL_MAPPINGS[category_slug]
            new_url = url.replace(f'/en/products/{category_slug}', f'/en/products/{new_category}')
            logger.info(f"üîÑ Auto-fixed URL: {category_slug} -> {new_category}")
            self.emit_progress(0, f"URL ƒë√£ ƒë∆∞·ª£c s·ª≠a", f"{category_slug} -> {new_category}")
            return new_url
            
        return url
    
    def get_html_content(self, url, timeout=30):
        """L·∫•y n·ªôi dung HTML t·ª´ URL"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"L·ªói khi l·∫•y HTML t·ª´ {url}: {str(e)}")
            return None
    
    def extract_series_from_category(self, category_url):
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ series t·ª´ trang category v·ªõi Selenium
        Ph√¢n t√≠ch ph·∫ßn t·ª≠ HTML ƒë·ªÉ l·∫•y c√°c link series s·∫£n ph·∫©m
        
        Args:
            category_url: URL c·ªßa trang category
            
        Returns:
            list: Danh s√°ch c√°c series URLs v√† metadata
        """
        driver = None
        series_data = []
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"üîÑ Th·ª≠ l·∫ßn {attempt + 1}/{max_retries} extract series t·ª´ {category_url}")
                
                # T·∫°o driver m·ªõi cho m·ªói attempt
                if driver:
                    self.close_driver(driver)
                driver = self.get_driver()
                
                self.emit_progress(
                    10, 
                    f"ƒêang thu th·∫≠p series t·ª´ category (l·∫ßn th·ª≠ {attempt + 1})",
                    f"URL: {category_url}"
                )
                
                # Load trang category v·ªõi timeout
                driver.set_page_load_timeout(30)
                driver.get(category_url)
                
                # ƒê·ª£i trang load xong v·ªõi timeout ng·∫Øn h∆°n
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".inputgroup.shortened"))
                    )
                except TimeoutException:
                    logger.warning(f"‚è∞ Timeout khi ƒë·ª£i page load, th·ª≠ fallback method")
                    # Th·ª≠ l·∫•y series b·∫±ng requests n·∫øu Selenium timeout
                    return self.extract_series_fallback(category_url)
                
                # T√¨m c√°c series links trong fieldset.products
                series_links = driver.find_elements(By.CSS_SELECTOR, "fieldset.products a[href*='/en/products/']")
                
                if not series_links:
                    # Th·ª≠ selector fallback trong inputgroup
                    series_links = driver.find_elements(By.CSS_SELECTOR, ".inputgroup a[href*='/en/products/']")
                    
                if not series_links:
                    # Th·ª≠ selector legacy
                    series_links = driver.find_elements(By.CSS_SELECTOR, ".inputgroup.shortened a[href*='/en/products/']")
                
                for link_element in series_links:
                    try:
                        href = link_element.get_attribute('href')
                        series_name = link_element.text.strip()
                        
                        if href and series_name:
                            # Convert relative URL th√†nh absolute URL
                            full_url = urljoin(self.base_url, href)
                            
                            # Tr√°nh tr√πng l·∫∑p
                            if not any(s['url'] == full_url for s in series_data):
                                series_data.append({
                                    'name': series_name,
                                    'url': full_url
                                })
                                logger.debug(f"‚úÖ T√¨m th·∫•y series: {series_name} - {full_url}")
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è L·ªói khi extract series link: {str(e)}")
                        continue
                
                # N·∫øu t√¨m th·∫•y series, break kh·ªèi retry loop
                if series_data:
                    break
                    
                # N·∫øu kh√¥ng t√¨m th·∫•y series, th·ª≠ l·∫°i
                if attempt < max_retries - 1:
                    logger.warning(f"üîÑ Kh√¥ng t√¨m th·∫•y series, th·ª≠ l·∫°i sau 2 gi√¢y...")
                    time.sleep(2)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói l·∫ßn th·ª≠ {attempt + 1} khi extract series t·ª´ {category_url}: {str(e)}")
                if attempt < max_retries - 1:
                    logger.info("üîÑ S·∫Ω th·ª≠ l·∫°i...")
                    time.sleep(3)
                    continue
                else:
                    logger.error("üí• ƒê√£ h·∫øt s·ªë l·∫ßn th·ª≠, fallback sang requests method")
                    return self.extract_series_fallback(category_url)
            finally:
                # ƒê·∫£m b·∫£o driver ƒë∆∞·ª£c ƒë√≥ng sau m·ªói attempt
                if driver and attempt == max_retries - 1:  # Ch·ªâ ƒë√≥ng ·ªü attempt cu·ªëi
                    self.close_driver(driver)
                    driver = None
        
        # Cleanup final driver n·∫øu c√≤n
        if driver:
            self.close_driver(driver)
        
        self.stats["series_found"] += len(series_data)
        logger.info(f"üéØ T·ªïng c·ªông t√¨m th·∫•y {len(series_data)} series t·ª´ {category_url}")
        
        return series_data
    
    def extract_series_fallback(self, category_url):
        """
        Fallback method ƒë·ªÉ extract series s·ª≠ d·ª•ng requests + BeautifulSoup
        Khi Selenium g·∫∑p v·∫•n ƒë·ªÅ v·ªõi overlay ho·∫∑c dynamic content
        
        Args:
            category_url: URL c·ªßa category page
            
        Returns:
            list: Danh s√°ch series URLs v√† metadata
        """
        try:
            logger.info(f"üîÑ S·ª≠ d·ª•ng fallback method cho category: {category_url}")
            
            html = self.get_html_content(category_url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            series_data = []
            
            # T√¨m fieldset.products tr∆∞·ªõc ti√™n
            products_fieldset = soup.find('fieldset', class_='products')
            if products_fieldset:
                # T√¨m t·∫•t c·∫£ series links trong fieldset.products
                potential_links = products_fieldset.find_all('a', href=True)
                logger.info(f"üéØ T√¨m th·∫•y fieldset.products v·ªõi {len(potential_links)} links")
            else:
                # Fallback: t√¨m t·∫•t c·∫£ links trong page
                potential_links = soup.find_all('a', href=True)
                logger.info(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y fieldset.products, fallback v·ªõi {len(potential_links)} links")
            
            for link in potential_links:
                try:
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    
                    if href and text:
                        # Filter links series - ∆∞u ti√™n links trong fieldset.products
                        if (('/en/products/' in href or 'products/' in href)
                            and len(text) <= 50  # Series names th∆∞·ªùng ng·∫Øn
                            and len(text) > 1
                            and not text.lower() in ['products', 'home', 'back', 'next', 'more', 'specifications', 'ordering info']
                            and href.count('/') <= 5):  # Series URLs th∆∞·ªùng ng·∫Øn h∆°n product URLs
                            
                            # Convert th√†nh absolute URL
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            else:
                                full_url = href
                            
                            # Ki·ªÉm tra kh√¥ng tr√πng l·∫∑p
                            if not any(s['url'] == full_url for s in series_data):
                                series_data.append({
                                    'name': text,
                                    'url': full_url
                                })
                                logger.debug(f"üîç Fallback t√¨m th·∫•y: {text} - {full_url}")
                        
                except Exception as e:
                    logger.debug(f"L·ªói khi x·ª≠ l√Ω link fallback: {str(e)}")
                    continue
            
            # L·ªçc v√† sort ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t
            series_data = series_data[:20]  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° nhi·ªÅu false positive
            
            logger.info(f"üéØ Fallback method t√¨m th·∫•y {len(series_data)} series")
            return series_data
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong fallback method: {str(e)}")
            return []
    
    # Placeholder methods - s·∫Ω ƒë∆∞·ª£c implement trong c√°c b∆∞·ªõc ti·∫øp theo
    def extract_products_from_series(self, series_url):
        """
        Extract t·∫•t c·∫£ products t·ª´ series v·ªõi x·ª≠ l√Ω 'Show more products'
        S·ª≠ d·ª•ng Selenium ƒë·ªÉ click n√∫t "Show more products" v√† l·∫•y t·∫•t c·∫£ s·∫£n ph·∫©m
        
        Args:
            series_url: URL c·ªßa series page
            
        Returns:
            list: Danh s√°ch product URLs v√† metadata
        """
        driver = None
        products_data = []
        max_retries = 2
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"üîÑ Th·ª≠ l·∫ßn {attempt + 1}/{max_retries} extract products t·ª´ {series_url}")
                
                # T·∫°o driver m·ªõi cho m·ªói attempt
                if driver:
                    self.close_driver(driver)
                driver = self.get_driver()
                
                self.emit_progress(
                    20, 
                    f"ƒêang thu th·∫≠p s·∫£n ph·∫©m t·ª´ series (l·∫ßn th·ª≠ {attempt + 1})",
                    f"URL: {series_url}"
                )
                
                # Load trang series v·ªõi timeout
                driver.set_page_load_timeout(45)  # Series pages c√≥ th·ªÉ n·∫∑ng h∆°n
                driver.get(series_url)
                
                # ƒê·ª£i trang load ho√†n to√†n
                time.sleep(3)
                
                # ƒê√≥ng tutorial overlay n·∫øu c√≥
                try:
                    # T√¨m v√† click overlay ƒë·ªÉ ƒë√≥ng tutorial
                    overlay = driver.find_element(By.CSS_SELECTOR, ".introjs-overlay")
                    if overlay.is_displayed():
                        overlay.click()
                        logger.info("ƒê√£ ƒë√≥ng tutorial overlay")
                        time.sleep(2)
                except NoSuchElementException:
                    logger.debug("Kh√¥ng c√≥ tutorial overlay")
                except Exception as e:
                    logger.debug(f"L·ªói khi ƒë√≥ng overlay: {str(e)}")
                
                # Ch·ªù cho Features view load v√† click
                try:
                    # ƒê·ª£i element c√≥ th·ªÉ click
                    features_view = WebDriverWait(driver, 15).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "#feature-view"))
                    )
                    
                    # Scroll ƒë·∫øn element ƒë·ªÉ ƒë·∫£m b·∫£o n√≥ visible
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", features_view)
                    time.sleep(1)
                    
                    # Th·ª≠ click b·∫±ng JavaScript ƒë·ªÉ bypass overlay
                    driver.execute_script("arguments[0].click();", features_view)
                    time.sleep(3)
                    
                    logger.info("ƒê√£ chuy·ªÉn sang Features view")
                except TimeoutException:
                    logger.info("Kh√¥ng t√¨m th·∫•y Features view, ti·∫øp t·ª•c v·ªõi view hi·ªán t·∫°i")
                except Exception as e:
                    logger.warning(f"L·ªói khi click Features view: {str(e)}, th·ª≠ ti·∫øp t·ª•c")
                
                # X·ª≠ l√Ω n√∫t "Show more products" ƒë·ªÉ load t·∫•t c·∫£ s·∫£n ph·∫©m
                max_attempts = 5  # Gi·ªõi h·∫°n s·ªë l·∫ßn click
                click_attempt = 0
                
                while click_attempt < max_attempts:
                    try:
                        # T√¨m n√∫t "Show all products" ho·∫∑c "Show more products"
                        show_more_buttons = driver.find_elements(By.CSS_SELECTOR, "a#show-all-products, a[id*='show'], a[class*='show-more']")
                        
                        if show_more_buttons:
                            button = show_more_buttons[0]
                            if button.is_displayed() and button.is_enabled():
                                # Scroll ƒë·∫øn button
                                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                                time.sleep(1)
                                
                                # Th·ª≠ click b·∫±ng JavaScript ƒë·ªÉ bypass overlay
                                driver.execute_script("arguments[0].click();", button)
                                logger.info(f"ƒê√£ click 'Show more products' l·∫ßn {click_attempt + 1}")
                                
                                # ƒê·ª£i content load
                                time.sleep(4)
                                click_attempt += 1
                            else:
                                break
                        else:
                            logger.info("Kh√¥ng t√¨m th·∫•y n√∫t 'Show more products'")
                            break
                            
                    except Exception as e:
                        logger.warning(f"L·ªói khi click 'Show more products' l·∫ßn {click_attempt + 1}: {str(e)}")
                        break
                
                # T√¨m products trong table.details theo c·∫•u tr√∫c m·ªõi
                product_links = []
                
                # Th·ª≠ t√¨m table v·ªõi class details tr∆∞·ªõc
                try:
                    table_selector = "table.details, table[class*='col-0'][class*='col-4'][class*='col-7'][class*='col-9']"
                    product_table = driver.find_element(By.CSS_SELECTOR, table_selector)
                    if product_table:
                        # T√¨m t·∫•t c·∫£ product links trong table
                        product_links = product_table.find_elements(By.CSS_SELECTOR, "td.product-name a, .product-name a")
                        logger.info(f"üéØ T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m trong table.details")
                except NoSuchElementException:
                    logger.info("Kh√¥ng t√¨m th·∫•y table.details, th·ª≠ c√°c selector kh√°c")
                    
                # N·∫øu kh√¥ng t√¨m th·∫•y trong table, th·ª≠ c√°c selector backup
                if not product_links:
                    selectors = [
                        "a[data-ga-action='Product clicked']",
                        "a[href*='/en/products/'][href*='-']",
                        ".product-item a",
                        ".product-link"
                    ]
                    
                    for selector in selectors:
                        try:
                            links = driver.find_elements(By.CSS_SELECTOR, selector)
                            if links:
                                product_links = links
                                logger.info(f"Fallback: T√¨m th·∫•y {len(links)} product links v·ªõi selector: {selector}")
                                break
                        except Exception as e:
                            logger.debug(f"L·ªói v·ªõi selector {selector}: {str(e)}")
                            continue
                
                for link_element in product_links:
                    try:
                        href = link_element.get_attribute('href')
                        product_name = link_element.text.strip()
                        
                        if href and product_name:
                            # Convert relative/absolute URL th√†nh absolute URL
                            if href.startswith('/'):
                                # Absolute path nh∆∞ /en/products/h3dt-a1-24-240vac-dc
                                full_url = urljoin(self.base_url, href)
                            elif href.startswith('http'):
                                # Full URL
                                full_url = href
                            else:
                                # Relative path nh∆∞ H3DT-A1-24-240VAC-DC -> /en/products/h3dt-a1-24-240vac-dc
                                full_url = urljoin(series_url + '/', href.lower())
                            
                            # Ki·ªÉm tra URL c√≥ h·ª£p l·ªá
                            if (len(product_name) > 2 and 
                                ('products' in full_url or 'omron' in full_url) and
                                len(href) > 3):  # Tr√°nh links r·ªóng ho·∫∑c qu√° ng·∫Øn
                                
                                products_data.append({
                                    'name': product_name,
                                    'url': full_url
                                })
                                logger.debug(f"‚úÖ T√¨m th·∫•y s·∫£n ph·∫©m: {product_name} - {full_url}")
                            
                    except Exception as e:
                        logger.warning(f"L·ªói khi extract product link: {str(e)}")
                        continue
                
                # N·∫øu t√¨m th·∫•y products, break kh·ªèi retry loop
                if products_data:
                    break
                    
                # N·∫øu kh√¥ng t√¨m th·∫•y products, th·ª≠ l·∫°i
                if attempt < max_retries - 1:
                    logger.warning(f"üîÑ Kh√¥ng t√¨m th·∫•y products, th·ª≠ l·∫°i sau 3 gi√¢y...")
                    time.sleep(3)
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói l·∫ßn th·ª≠ {attempt + 1} khi extract products t·ª´ {series_url}: {str(e)}")
                if attempt < max_retries - 1:
                    logger.info("üîÑ S·∫Ω th·ª≠ l·∫°i...")
                    time.sleep(5)
                    continue
                else:
                    logger.error("üí• ƒê√£ h·∫øt s·ªë l·∫ßn th·ª≠, fallback sang requests method")
                    # Cleanup driver tr∆∞·ªõc khi fallback
                    if driver:
                        self.close_driver(driver)
                        driver = None
                    products_data = self.extract_products_fallback(series_url)
                    break
            finally:
                # ƒê·∫£m b·∫£o driver ƒë∆∞·ª£c ƒë√≥ng sau m·ªói attempt
                if driver and attempt == max_retries - 1:  # Ch·ªâ ƒë√≥ng ·ªü attempt cu·ªëi
                    self.close_driver(driver)
                    driver = None
        
        # Cleanup final driver n·∫øu c√≤n
        if driver:
            self.close_driver(driver)
        
        self.stats["products_found"] += len(products_data)
        logger.info(f"üéØ T·ªïng c·ªông t√¨m th·∫•y {len(products_data)} s·∫£n ph·∫©m t·ª´ {series_url}")
        
        return products_data
    
    def extract_products_fallback(self, series_url):
        """
        Fallback method ƒë·ªÉ extract products s·ª≠ d·ª•ng requests + BeautifulSoup
        Khi Selenium g·∫∑p v·∫•n ƒë·ªÅ v·ªõi overlay ho·∫∑c dynamic content
        
        Args:
            series_url: URL c·ªßa series page
            
        Returns:
            list: Danh s√°ch product URLs v√† metadata
        """
        try:
            logger.info(f"ƒêang th·ª≠ fallback method cho {series_url}")
            
            html = self.get_html_content(series_url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            products_data = []
            
            # T√¨m table.details tr∆∞·ªõc ti√™n
            product_table = soup.find('table', class_='details')
            if not product_table:
                # Th·ª≠ t√¨m table c√≥ c√°c class col
                product_table = soup.find('table', class_=lambda x: x and 'col-0' in x and 'col-4' in x)
            
            if product_table:
                # T√¨m products trong table
                product_rows = product_table.find_all('tr', class_='filtered')
                logger.info(f"üéØ T√¨m th·∫•y table.details v·ªõi {len(product_rows)} s·∫£n ph·∫©m")
                
                for row in product_rows:
                    try:
                        product_cell = row.find('td', class_='product-name')
                        if product_cell:
                            link = product_cell.find('a')
                            if link and link.get('href') and link.text.strip():
                                href = link.get('href')
                                text = link.text.strip()
                                
                                # Convert th√†nh absolute URL
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                elif href.startswith('http'):
                                    full_url = href
                                else:
                                    full_url = urljoin(series_url + '/', href.lower())
                                
                                products_data.append({
                                    'name': text,
                                    'url': full_url
                                })
                                logger.debug(f"Table fallback t√¨m th·∫•y: {text} - {full_url}")
                    except Exception as e:
                        logger.debug(f"L·ªói khi x·ª≠ l√Ω row trong table: {str(e)}")
                        continue
            else:
                # Fallback: t√¨m t·∫•t c·∫£ links trong page
                logger.info("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y table.details, fallback t√¨m t·∫•t c·∫£ links")
                potential_links = soup.find_all('a', href=True)
                
                for link in potential_links:
                    try:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        
                        if href and text:
                            # Filter links s·∫£n ph·∫©m v·ªõi ti√™u ch√≠ m·ªõi
                            if (len(text) > 2
                                and len(href) > 3
                                and not text.lower() in ['products', 'home', 'back', 'next', 'specifications', 'ordering info']):
                                
                                # Convert th√†nh absolute URL
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                elif href.startswith('http'):
                                    full_url = href
                                else:
                                    full_url = urljoin(series_url + '/', href.lower())
                                
                                # Ki·ªÉm tra kh√¥ng tr√πng l·∫∑p
                                if not any(p['url'] == full_url for p in products_data):
                                    products_data.append({
                                        'name': text,
                                        'url': full_url
                                    })
                                    logger.debug(f"General fallback t√¨m th·∫•y: {text} - {full_url}")
                        
                    except Exception as e:
                        logger.debug(f"L·ªói khi x·ª≠ l√Ω link fallback: {str(e)}")
                        continue
            
            logger.info(f"Fallback method t√¨m th·∫•y {len(products_data)} s·∫£n ph·∫©m")
            return products_data
            
        except Exception as e:
            logger.error(f"L·ªói trong fallback method: {str(e)}")
            return []
    
    def extract_product_details(self, product_url):
        """
        Extract chi ti·∫øt s·∫£n ph·∫©m t·ª´ product page
        L·∫•y t√™n s·∫£n ph·∫©m, m√£ s·∫£n ph·∫©m, th√¥ng s·ªë k·ªπ thu·∫≠t v√† ·∫£nh s·∫£n ph·∫©m
        
        Args:
            product_url: URL c·ªßa product page
            
        Returns:
            dict: Th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
        """
        try:
            self.emit_progress(
                50, 
                f"ƒêang c√†o th√¥ng tin s·∫£n ph·∫©m",
                f"URL: {product_url}"
            )
            
            html = self.get_html_content(product_url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            product_data = {
                'product_code': '',
                'product_name': '',
                'full_product_name': '',
                'category': '',
                'series': '',
                'image_url': '',
                'specifications': {},
                'original_url': product_url
            }
            
            # 1. L·∫•y m√£ s·∫£n ph·∫©m t·ª´ <h1>
            h1_element = soup.find('h1')
            if h1_element:
                product_data['product_code'] = h1_element.get_text(strip=True)
            
            # 2. L·∫•y c√°c ph·∫ßn t·ª≠ ƒë·ªÉ t·∫°o t√™n s·∫£n ph·∫©m ƒë·∫ßy ƒë·ªß
            # T√¨m breadcrumb navigation ƒë·ªÉ l·∫•y category v√† series
            breadcrumb_items = soup.find_all('li', class_='without-dropdown')
            
            category_name = ''
            series_name = ''
            
            for li in breadcrumb_items:
                a_tag = li.find('a')
                if a_tag and a_tag.find('span'):
                    span_text = a_tag.find('span').get_text(strip=True)
                    href = a_tag.get('href', '')
                    
                    # X√°c ƒë·ªãnh category (th∆∞·ªùng l√† link ƒë·∫ßu ti√™n sau products)
                    if '/en/products/' in href and not any(char.isdigit() for char in span_text.lower()):
                        if not category_name and span_text.lower() not in ['products', 'home']:
                            category_name = span_text
                            product_data['category'] = category_name
                    
                    # X√°c ƒë·ªãnh series (th∆∞·ªùng l√† link c√≥ t√™n ng·∫Øn v√† c√≥ ch·ªØ c√°i + s·ªë)
                    elif len(span_text) <= 10 and any(char.isalnum() for char in span_text):
                        series_name = span_text
                        product_data['series'] = series_name
            
            # 3. Gh√©p t√™n s·∫£n ph·∫©m theo th·ª© t·ª±: Category + Product Code + Series + OMRON
            name_parts = []
            if category_name:
                name_parts.append(category_name)
            if product_data['product_code']:
                name_parts.append(product_data['product_code'])
            if series_name:
                name_parts.append(f"series {series_name}")
            name_parts.append('OMRON')
            
            english_name = ' '.join(name_parts)
            product_data['product_name'] = english_name
            
            # D·ªãch t√™n s·∫£n ph·∫©m sang ti·∫øng Vi·ªát s·ª≠ d·ª•ng Gemini
            vietnamese_name = self.translate_with_gemini(english_name)
            product_data['full_product_name'] = vietnamese_name
            
            # 4. L·∫•y ·∫£nh s·∫£n ph·∫©m t·ª´ figure > a.image-link > img
            figure_element = soup.find('figure')
            if figure_element:
                img_link = figure_element.find('a', class_='image-link')
                if img_link:
                    img_tag = img_link.find('img')
                    if img_tag and img_tag.get('src'):
                        image_src = img_tag['src']
                        # S·ª≠ d·ª•ng URL ch·∫•t l∆∞·ª£ng cao t·ª´ href n·∫øu c√≥
                        high_quality_url = img_link.get('href')
                        if high_quality_url:
                            product_data['image_url'] = high_quality_url
                        else:
                            product_data['image_url'] = image_src
            
            # 5. L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t t·ª´ b·∫£ng Specifications
            spec_table = soup.find('table', class_='one')
            if spec_table:
                rows = spec_table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        if key and value:
                            # D·ªãch key v√† value sang ti·∫øng Vi·ªát
                            vietnamese_key = self.translate_with_gemini(key)
                            vietnamese_value = self.translate_with_gemini(value)
                            product_data['specifications'][vietnamese_key] = vietnamese_value
            
            return product_data
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ {product_url}: {str(e)}")
            return None
    
    def download_and_process_image(self, image_url, save_path, product_code):
        """
        T·∫£i v√† x·ª≠ l√Ω ·∫£nh s·∫£n ph·∫©m, chuy·ªÉn sang WebP
        S·ª≠ d·ª•ng logic t∆∞∆°ng t·ª± Google Apps Script ƒë·ªÉ t·∫°o t√™n file
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            save_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
            product_code: M√£ s·∫£n ph·∫©m
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            if not image_url or not product_code:
                return False
            
            # T·∫£i ·∫£nh
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # M·ªü ·∫£nh t·ª´ bytes
            image = Image.open(BytesIO(response.content))
            
            # Th√™m n·ªÅn tr·∫Øng v√† resize
            processed_image = self.add_white_background_to_image(image)
            
            # T·∫°o t√™n file theo logic standardize_filename
            filename = f"{standardize_filename(product_code)}.webp"
            full_path = os.path.join(save_path, filename)
            
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(save_path, exist_ok=True)
            
            # L∆∞u ·∫£nh t·∫°m d∆∞·ªõi d·∫°ng PNG
            temp_path = full_path.replace('.webp', '.png')
            processed_image.save(temp_path, 'PNG', quality=95)
            
            # Chuy·ªÉn ƒë·ªïi sang WebP
            result = WebPConverter.convert_to_webp(
                input_path=temp_path,
                output_path=full_path,
                quality=90,
                lossless=False,
                method=6
            )
            
            # X√≥a file t·∫°m
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            if result:
                self.stats["images_downloaded"] += 1
                logger.info(f"‚úÖ ƒê√£ t·∫£i v√† chuy·ªÉn ƒë·ªïi ·∫£nh: {filename}")
                return True
            else:
                self.stats["failed_images"] += 1
                logger.error(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi ·∫£nh WebP: {filename}")
                return False
                
        except Exception as e:
            self.stats["failed_images"] += 1
            logger.error(f"‚ùå L·ªói khi t·∫£i ·∫£nh t·ª´ {image_url}: {str(e)}")
            return False
    
    def add_white_background_to_image(self, image, target_size=(800, 800)):
        """
        Th√™m n·ªÅn tr·∫Øng v√†o ·∫£nh v√† resize v·ªÅ k√≠ch th∆∞·ªõc target
        
        Args:
            image: PIL Image object
            target_size: K√≠ch th∆∞·ªõc m·ª•c ti√™u (width, height)
            
        Returns:
            PIL Image: ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        # T·∫°o ·∫£nh n·ªÅn tr·∫Øng v·ªõi k√≠ch th∆∞·ªõc target
        background = Image.new('RGB', target_size, (255, 255, 255))
        
        # Convert ·∫£nh g·ªëc sang RGBA n·∫øu c·∫ßn
        if image.mode != 'RGBA':
            image = image.convert('RGBA')
        
        # T√≠nh to√°n t·ª∑ l·ªá resize ƒë·ªÉ fit v√†o target size
        img_ratio = min(target_size[0] / image.width, target_size[1] / image.height)
        new_size = (int(image.width * img_ratio), int(image.height * img_ratio))
        
        # Resize ·∫£nh
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        # T√≠nh to√°n v·ªã tr√≠ ƒë·ªÉ paste ·∫£nh v√†o gi·ªØa n·ªÅn tr·∫Øng
        x = (target_size[0] - new_size[0]) // 2
        y = (target_size[1] - new_size[1]) // 2
        
        # Paste ·∫£nh v√†o n·ªÅn tr·∫Øng
        background.paste(image, (x, y), image if image.mode == 'RGBA' else None)
        
        return background
    
    def create_excel_with_specifications(self, products_data, excel_path):
        """
        T·∫°o file Excel v·ªõi th√¥ng s·ªë k·ªπ thu·∫≠t theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu
        T·∫°o b·∫£ng HTML specifications theo format c·ª• th·ªÉ c·ªßa ng∆∞·ªùi d√πng
        
        Args:
            products_data: Danh s√°ch d·ªØ li·ªáu s·∫£n ph·∫©m
            excel_path: ƒê∆∞·ªùng d·∫´n file Excel
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            if not products_data:
                logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m ƒë·ªÉ xu·∫•t Excel")
                return False
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho DataFrame
            excel_data = []
            
            for product in products_data:
                # T·∫°o b·∫£ng HTML specifications theo format y√™u c·∫ßu
                specs_html = self.create_specifications_table_html(product)
                
                excel_row = {
                    'M√£ s·∫£n ph·∫©m': product.get('product_code', ''),
                    'T√™n s·∫£n ph·∫©m ti·∫øng Anh': product.get('product_name', ''),
                    'T√™n s·∫£n ph·∫©m ti·∫øng Vi·ªát': product.get('full_product_name', ''),
                    'Category': product.get('category', ''),
                    'Series': product.get('series', ''),
                    'Link s·∫£n ph·∫©m': product.get('original_url', ''),
                    'Link ·∫£nh': product.get('image_url', ''),
                    'Th√¥ng s·ªë k·ªπ thu·∫≠t HTML': specs_html,
                    'S·ªë l∆∞·ª£ng th√¥ng s·ªë': len(product.get('specifications', {}))
                }
                excel_data.append(excel_row)
            
            # T·∫°o DataFrame
            df = pd.DataFrame(excel_data)
            
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(os.path.dirname(excel_path), exist_ok=True)
            
            # Xu·∫•t ra Excel v·ªõi formatting
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='Products')
                
                # L·∫•y worksheet ƒë·ªÉ format
                worksheet = writer.sheets['Products']
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)  # Gi·ªõi h·∫°n width t·ªëi ƒëa
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logger.info(f"‚úÖ ƒê√£ t·∫°o file Excel: {excel_path} v·ªõi {len(products_data)} s·∫£n ph·∫©m")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o file Excel {excel_path}: {str(e)}")
            return False
    
    def create_specifications_table_html(self, product):
        """
        T·∫°o b·∫£ng HTML th√¥ng s·ªë k·ªπ thu·∫≠t theo format c·ª• th·ªÉ
        
        Args:
            product: D·ªØ li·ªáu s·∫£n ph·∫©m
            
        Returns:
            str: HTML table string
        """
        try:
            product_code = product.get('product_code', '')
            product_name = product.get('full_product_name', '')
            specifications = product.get('specifications', {})
            
            # B·∫Øt ƒë·∫ßu t·∫°o HTML table theo format y√™u c·∫ßu
            html = '''<table id="specifications" border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; font-family: Arial; width: 100%;">
<thead>
<tr style="background-color: #f2f2f2;">
<th>Th√¥ng s·ªë</th>
<th>Gi√° tr·ªã</th>
</tr>
</thead>
<tbody>
<tr>
<td style="font-weight: bold;">M√£ s·∫£n ph·∫©m</td>
<td>{}</td>
</tr>
<tr>
<td style="font-weight: bold;">T√™n s·∫£n ph·∫©m</td>
<td>{}</td>
</tr>'''.format(product_code, product_name)
            
            # Th√™m c√°c th√¥ng s·ªë k·ªπ thu·∫≠t
            for key, value in specifications.items():
                html += f'''
<tr>
<td style="font-weight: bold;">{key}</td>
<td>{value}</td>
</tr>'''
            
            # Th√™m Copyright ·ªü cu·ªëi b·∫£ng
            html += '''
<tr>
<td style="font-weight: bold;">Copyright</td>
<td>Haiphongtech.vn</td>
</tr>
</tbody>
</table>'''
            
            return html
            
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o HTML table cho s·∫£n ph·∫©m: {str(e)}")
            return ""
    
    def crawl_products(self, category_urls):
        """
        Method ch√≠nh ƒë·ªÉ c√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ danh s√°ch category URLs
        
        Args:
            category_urls: Danh s√°ch URLs c·ªßa c√°c categories
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£
        """
        start_time = time.time()
        timestamp = datetime.now().strftime("%d%m%Y_%H%M%S")
        result_dir = os.path.join(self.output_root, f"OmronProduct_{timestamp}")
        os.makedirs(result_dir, exist_ok=True)
        
        self.emit_progress(0, "B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu Omron", f"S·∫Ω x·ª≠ l√Ω {len(category_urls)} categories")
        
        # Process each category with retry logic
        for i, original_url in enumerate(category_urls):
            # T·ª± ƒë·ªông s·ª≠a URL n·∫øu c·∫ßn
            category_url = self.fix_category_url(original_url)
            if category_url != original_url:
                self.emit_progress(5, f"URL ƒë√£ ƒë∆∞·ª£c s·ª≠a", f"'{original_url.split('/')[-1]}' -> '{category_url.split('/')[-1]}'")
                logger.info(f"üîß Fixed URL: {original_url} -> {category_url}")
            category_success = False
            max_category_retries = 2
            
            for category_attempt in range(max_category_retries):
                logger.info(f"üîÑ Category attempt {category_attempt + 1}/{max_category_retries} for: {category_url}")
                category_success = self._process_single_category(
                    category_url, i, len(category_urls), result_dir
                )
                
                if category_success:
                    logger.info(f"‚úÖ Category th√†nh c√¥ng: {category_url}")
                    break
                elif category_attempt < max_category_retries - 1:
                    logger.warning(f"‚ö†Ô∏è Category attempt {category_attempt + 1} th·∫•t b·∫°i, th·ª≠ l·∫°i sau 10 gi√¢y...")
                    time.sleep(10)
                else:
                    logger.error(f"‚ùå Category th·∫•t b·∫°i ho√†n to√†n sau {max_category_retries} l·∫ßn th·ª≠: {category_url}")
        
        # Ho√†n th√†nh
        end_time = time.time()
        duration = end_time - start_time
        
        self.emit_progress(100, f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {self.stats['categories_processed']} categories")
        
        # Log th·ªëng k√™ cu·ªëi c√πng
        logger.info("=== TH·ªêNG K√ä CRAWLER OMRON ===")
        logger.info(f"Th·ªùi gian th·ª±c hi·ªán: {duration:.2f} gi√¢y")
        logger.info(f"Categories ƒë√£ x·ª≠ l√Ω: {self.stats['categories_processed']}")
        logger.info(f"Series t√¨m th·∫•y: {self.stats['series_found']}")
        logger.info(f"S·∫£n ph·∫©m t√¨m th·∫•y: {self.stats['products_found']}")
        logger.info(f"S·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω: {self.stats['products_processed']}")
        logger.info(f"·∫¢nh ƒë√£ t·∫£i: {self.stats['images_downloaded']}")
        logger.info(f"B·∫£n d·ªãch ho√†n th√†nh: {self.stats['translations_completed']}")
        logger.info(f"Request th·∫•t b·∫°i: {self.stats['failed_requests']}")
        logger.info(f"·∫¢nh th·∫•t b·∫°i: {self.stats['failed_images']}")
        
        return result_dir
    
    def _process_single_category(self, category_url, category_index, total_categories, result_dir):
        """
        Process m·ªôt category v·ªõi error handling v√† retry logic
        
        Args:
            category_url: URL c·ªßa category
            category_index: Index c·ªßa category trong danh s√°ch
            total_categories: T·ªïng s·ªë categories
            result_dir: Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω category {category_index+1}/{total_categories}: {category_url}")
            
            # Extract category name t·ª´ URL
            category_name = self.extract_category_name_from_url(category_url)
            category_dir = os.path.join(result_dir, sanitize_folder_name(category_name))
            os.makedirs(category_dir, exist_ok=True)
            
            # T·∫°o th∆∞ m·ª•c images
            images_dir = os.path.join(category_dir, "images")
            os.makedirs(images_dir, exist_ok=True)
            
            # Progress calculation c·∫£i thi·ªán
            category_progress_base = int((category_index / total_categories) * 90)  # Reserve 10% for final steps
            
            self.emit_progress(
                category_progress_base,
                f"ƒêang x·ª≠ l√Ω category [{category_index+1}/{total_categories}]: {category_name}",
                f"URL: {category_url}"
            )
            
            # L·∫•y danh s√°ch series
            logger.info(f"üìã ƒêang thu th·∫≠p series t·ª´ category: {category_name}")
            series_list = self.extract_series_from_category(category_url)
            
            if not series_list:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y series n√†o t·ª´ {category_url}")
                return False
            
            logger.info(f"‚úÖ T√¨m th·∫•y {len(series_list)} series trong category: {category_name}")
            
            # Collect t·∫•t c·∫£ products t·ª´ c√°c series v·ªõi ƒëa lu·ªìng
            all_products_data = []
            
            def process_series(series_info):
                """Process m·ªôt series v√† tr·∫£ v·ªÅ danh s√°ch products"""
                try:
                    series_url = series_info['url']
                    series_name = series_info['name']
                    
                    self.emit_progress(
                        category_progress_base + 20,
                        f"ƒêang x·ª≠ l√Ω series: {series_name}",
                        series_url
                    )
                    
                    # L·∫•y danh s√°ch products t·ª´ series
                    products_list = self.extract_products_from_series(series_url)
                    
                    if not products_list:
                        logger.warning(f"Kh√¥ng t√¨m th·∫•y products n√†o t·ª´ series {series_name}")
                        return []
                    
                    # Extract chi ti·∫øt cho t·ª´ng product
                    series_products = []
                    for product_info in products_list:
                        try:
                            product_details = self.extract_product_details(product_info['url'])
                            if product_details:
                                series_products.append(product_details)
                                self.stats["products_processed"] += 1
                        except Exception as e:
                            logger.error(f"L·ªói khi x·ª≠ l√Ω product {product_info['url']}: {str(e)}")
                            continue
                    
                    return series_products
                    
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω series {series_info['url']}: {str(e)}")
                    return []
            
            # X·ª≠ l√Ω series v·ªõi ƒëa lu·ªìng
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(self.max_workers, len(series_list))) as executor:
                future_to_series = {executor.submit(process_series, series): series for series in series_list}
                
                for future in concurrent.futures.as_completed(future_to_series):
                    series = future_to_series[future]
                    try:
                        series_products = future.result()
                        all_products_data.extend(series_products)
                        logger.info(f"Ho√†n th√†nh series {series['name']}: {len(series_products)} s·∫£n ph·∫©m")
                    except Exception as e:
                        logger.error(f"L·ªói khi x·ª≠ l√Ω series {series['name']}: {str(e)}")
            
            if not all_products_data:
                logger.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m n√†o t·ª´ category {category_name}")
                return False
            
            self.emit_progress(
                category_progress_base + 50,
                f"ƒêang t·∫£i ·∫£nh cho {len(all_products_data)} s·∫£n ph·∫©m",
                f"Category: {category_name}"
            )
            
            # T·∫£i ·∫£nh v·ªõi ƒëa lu·ªìng
            def download_image(product):
                """Download ·∫£nh cho m·ªôt s·∫£n ph·∫©m"""
                try:
                    if product.get('image_url') and product.get('product_code'):
                        return self.download_and_process_image(
                            product['image_url'],
                            images_dir,
                            product['product_code']
                        )
                except Exception as e:
                    logger.error(f"L·ªói khi t·∫£i ·∫£nh cho {product.get('product_code', 'Unknown')}: {str(e)}")
                    return False
            
            # Download ·∫£nh v·ªõi ƒëa lu·ªìng
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                image_futures = [executor.submit(download_image, product) for product in all_products_data]
                concurrent.futures.wait(image_futures)
            
            # T·∫°o file Excel
            excel_path = os.path.join(category_dir, f"{category_name}.xlsx")
            excel_success = self.create_excel_with_specifications(all_products_data, excel_path)
            
            if excel_success:
                logger.info(f"‚úÖ ƒê√£ ho√†n th√†nh category: {category_name}")
                logger.info(f"üìä T·ªïng k·∫øt category {category_name}: {len(all_products_data)} s·∫£n ph·∫©m, {self.stats['images_downloaded']} ·∫£nh")
            else:
                logger.warning(f"‚ö†Ô∏è Ho√†n th√†nh category {category_name} nh∆∞ng kh√¥ng t·∫°o ƒë∆∞·ª£c Excel")
            
            self.stats["categories_processed"] += 1
            return True
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω category {category_url}: {str(e)}")
            logger.error(f"Chi ti·∫øt l·ªói: {error_details}")
            
            # Emit error
            self.emit_progress(
                int((category_index / total_categories) * 90),
                f"L·ªói khi x·ª≠ l√Ω category {category_index+1}/{total_categories}",
                f"L·ªói: {str(e)}"
            )
            
            # ƒê·∫£m b·∫£o cleanup resources n·∫øu c√≥
            try:
                # Force garbage collection ƒë·ªÉ clean up memory
                import gc
                gc.collect()
            except:
                pass
            
            return False
    
    def extract_category_name_from_url(self, url):
        """Extract t√™n category t·ª´ URL"""
        try:
            # Parse URL ƒë·ªÉ l·∫•y ph·∫ßn cu·ªëi
            path = urlparse(url).path
            # L·∫•y ph·∫ßn cu·ªëi sau d·∫•u '/' cu·ªëi c√πng
            category_name = path.strip('/').split('/')[-1]
            # Chuy·ªÉn th√†nh title case v√† thay th·∫ø d·∫•u g·∫°ch ngang
            return category_name.replace('-', ' ').title()
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ extract category name t·ª´ {url}: {str(e)}")
            return "Unknown_Category"

if __name__ == "__main__":
    # Test crawler
    crawler = OmronCrawler()
    test_urls = ["https://industrial.omron.co.uk/en/products/photoelectric-sensors"]
    result_dir = crawler.crawl_products(test_urls)
    print(f"K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {result_dir}")