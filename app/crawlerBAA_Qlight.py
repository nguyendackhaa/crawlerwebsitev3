"""
Crawler BAA Qlight - C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ https://baa.vn/vn/qlight/
H·ªó tr·ª£ ƒëa lu·ªìng, chuy·ªÉn ƒë·ªïi ·∫£nh WebP v√† x·ª≠ l√Ω series
"""

import os
import shutil
import time
import zipfile
from datetime import datetime
import pandas as pd
import re
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import requests
import traceback
from queue import Queue
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from PIL import Image
import io
import hashlib
from openpyxl import Workbook
from openpyxl.drawing.image import Image as XLImage
import openpyxl.styles
import tempfile

# Import c√°c h√†m t·ª´ module crawler hi·ªán c√≥
from app.crawler import (
    is_category_url, is_product_url, extract_product_urls, extract_product_info,
    download_baa_product_images_fixed, get_html_content, HEADERS
)
from app.webp_converter import WebPConverter
from app import socketio

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BAAQlightCrawler:
    """Crawler chuy√™n d·ª•ng cho BAA Qlight v·ªõi h·ªó tr·ª£ ƒëa lu·ªìng v√† x·ª≠ l√Ω series"""
    
    def __init__(self, base_url="https://baa.vn/vn/qlight/", max_workers=10):
        self.base_url = base_url
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        
    def extract_series_info(self, url):
        """
        Tr√≠ch xu·∫•t th√¥ng tin series t·ª´ trang s·∫£n ph·∫©m
        
        Args:
            url (str): URL trang s·∫£n ph·∫©m
            
        Returns:
            dict: Th√¥ng tin series bao g·ªìm t√™n v√† URL
        """
        try:
            html = get_html_content(url)
            if not html:
                return None
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # Ph∆∞∆°ng ph√°p 1: T√¨m theo c·∫•u tr√∫c HTML ƒë√£ cung c·∫•p
            series_header = soup.select_one('div.product__symbol-header.px-2.py-1.m-0.row')
            if not series_header:
                # Th·ª≠ t√¨m v·ªõi selector kh√°c
                series_header = soup.select_one('div.product__symbol-header')
            
            if series_header:
                # T√¨m label "Series:"
                series_label = series_header.select_one('span.product__symbol-label')
                if series_label and 'Series:' in series_label.text:
                    # T√¨m link series trong span.product__symbol__value
                    series_value = series_header.select_one('span.product__symbol__value')
                    if series_value:
                        series_link = series_value.select_one('a.text-decoration-none.text-link')
                        if series_link:
                            series_name = series_link.get_text(strip=True)
                            series_url = urljoin(self.base_url, series_link.get('href', ''))
                            
                            return {
                                'name': series_name,
                                'url': series_url,
                                'original_url': url
                            }
            
            # Ph∆∞∆°ng ph√°p 2: T√¨m series t·ª´ breadcrumb
            breadcrumb = soup.select_one('.breadcrumb, .breadcrumb-item')
            if breadcrumb:
                breadcrumb_links = breadcrumb.select('a[href*="series"]')
                for link in breadcrumb_links:
                    href = link.get('href', '')
                    if 'series' in href.lower():
                        series_name = link.get_text(strip=True)
                        series_url = urljoin(self.base_url, href)
                        
                        return {
                            'name': series_name,
                            'url': series_url,
                            'original_url': url
                        }
            
            # Ph∆∞∆°ng ph√°p 3: T√¨m series t·ª´ URL pattern
            if 'series' in url.lower():
                # Tr√≠ch xu·∫•t t√™n series t·ª´ URL
                url_parts = url.split('/')
                for i, part in enumerate(url_parts):
                    if 'series' in part.lower() and i + 1 < len(url_parts):
                        series_name = url_parts[i + 1].replace('_', ' ').title()
                        series_url = url
                        
                        return {
                            'name': series_name,
                            'url': series_url,
                            'original_url': url
                        }
            
            # Ph∆∞∆°ng ph√°p 4: T√¨m series t·ª´ meta tags
            meta_series = soup.select_one('meta[name="series"], meta[property="series"]')
            if meta_series:
                series_name = meta_series.get('content', '')
                if series_name:
                    # T·∫°o URL series t·ª´ t√™n
                    series_url = urljoin(self.base_url, f"/vn/series/{series_name.lower().replace(' ', '-')}/")
                    
                    return {
                        'name': series_name,
                        'url': series_url,
                        'original_url': url
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin series t·ª´ {url}: {str(e)}")
            return None
    
    def get_products_from_series(self, series_url):
        """
        L·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ m·ªôt series
        
        Args:
            series_url (str): URL c·ªßa series
            
        Returns:
            list: Danh s√°ch URL s·∫£n ph·∫©m trong series
        """
        try:
            html = get_html_content(series_url)
            if not html:
                return []
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ link s·∫£n ph·∫©m trong series
            product_urls = []
            
            # Ph∆∞∆°ng ph√°p 1: T√¨m trong c√°c container s·∫£n ph·∫©m
            product_containers = soup.select('.product-item, .product-card, .product, [class*="product"], .item, .card')
            
            for container in product_containers:
                links = container.select('a[href*="/vn/"]')
                for link in links:
                    href = link.get('href')
                    if href and is_product_url(href):
                        full_url = urljoin(self.base_url, href)
                        if full_url not in product_urls:
                            product_urls.append(full_url)
            
            # Ph∆∞∆°ng ph√°p 2: T√¨m t·∫•t c·∫£ link c√≥ pattern s·∫£n ph·∫©m
            if not product_urls:
                all_links = soup.select('a[href*="/vn/"]')
                for link in all_links:
                    href = link.get('href')
                    if href and is_product_url(href):
                        full_url = urljoin(self.base_url, href)
                        if full_url not in product_urls:
                            product_urls.append(full_url)
            
            # Ph∆∞∆°ng ph√°p 3: T√¨m theo pattern URL c·ª• th·ªÉ c·ªßa BAA
            if not product_urls:
                # T√¨m c√°c link c√≥ pattern _s·ªë ·ªü cu·ªëi (ID s·∫£n ph·∫©m)
                pattern_links = soup.select('a[href*="_"]')
                for link in pattern_links:
                    href = link.get('href')
                    if href and re.search(r'_\d+/?$', href):
                        full_url = urljoin(self.base_url, href)
                        if full_url not in product_urls:
                            product_urls.append(full_url)
            
            # Lo·∫°i b·ªè c√°c URL kh√¥ng ph·∫£i s·∫£n ph·∫©m
            filtered_urls = []
            for url in product_urls:
                if is_product_url(url):
                    filtered_urls.append(url)
            
            logger.info(f"T√¨m th·∫•y {len(filtered_urls)} s·∫£n ph·∫©m trong series: {series_url}")
            return filtered_urls
            
        except Exception as e:
            logger.error(f"L·ªói khi l·∫•y s·∫£n ph·∫©m t·ª´ series {series_url}: {str(e)}")
            return []
    
    def crawl_product_info(self, url, index=1):
        """
        C√†o th√¥ng tin m·ªôt s·∫£n ph·∫©m
        
        Args:
            url (str): URL s·∫£n ph·∫©m
            index (int): S·ªë th·ª© t·ª±
            
        Returns:
            dict: Th√¥ng tin s·∫£n ph·∫©m
        """
        try:
            # S·ª≠ d·ª•ng h√†m extract_product_info c√≥ s·∫µn
            product_info = extract_product_info(url, index=index)
            
            # Th√™m th√¥ng tin series
            series_info = self.extract_series_info(url)
            if series_info:
                product_info['Series'] = series_info['name']
                product_info['Series_URL'] = series_info['url']
            else:
                product_info['Series'] = ''
                product_info['Series_URL'] = ''
            
            return product_info
            
        except Exception as e:
            logger.error(f"L·ªói khi c√†o th√¥ng tin s·∫£n ph·∫©m {url}: {str(e)}")
            return None
    
    def crawl_products_multithread(self, product_urls):
        """
        C√†o th√¥ng tin s·∫£n ph·∫©m s·ª≠ d·ª•ng ƒëa lu·ªìng
        
        Args:
            product_urls (list): Danh s√°ch URL s·∫£n ph·∫©m
            
        Returns:
            list: Danh s√°ch th√¥ng tin s·∫£n ph·∫©m
        """
        results = []
        lock = threading.Lock()
        completed_count = 0
        total_count = len(product_urls)
        
        def process_product(args):
            nonlocal completed_count
            url, index = args
            try:
                product_info = self.crawl_product_info(url, index)
                if product_info:
                    with lock:
                        results.append(product_info)
                        completed_count += 1
                        progress = (completed_count / total_count) * 100
                        logger.info(f"‚úì [{completed_count}/{total_count}] ({progress:.1f}%) - {product_info.get('T√™n s·∫£n ph·∫©m', 'Unknown')[:50]}...")
                else:
                    with lock:
                        completed_count += 1
                        logger.warning(f"‚ö†Ô∏è [{completed_count}/{total_count}] Kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin: {url}")
                return product_info
            except Exception as e:
                with lock:
                    completed_count += 1
                    logger.error(f"‚ùå [{completed_count}/{total_count}] L·ªói x·ª≠ l√Ω s·∫£n ph·∫©m {url}: {str(e)}")
                return None
        
        logger.info(f"üîÑ B·∫Øt ƒë·∫ßu c√†o th√¥ng tin {total_count} s·∫£n ph·∫©m v·ªõi {self.max_workers} lu·ªìng...")
        
        # T·∫°o danh s√°ch arguments cho ThreadPoolExecutor
        args_list = [(url, i+1) for i, url in enumerate(product_urls)]
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω ƒëa lu·ªìng
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_product, args) for args in args_list]
            
            # Ch·ªù t·∫•t c·∫£ futures ho√†n th√†nh
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"‚ùå L·ªói trong thread: {str(e)}")
        
        logger.info(f"‚úÖ Ho√†n th√†nh c√†o th√¥ng tin {len(results)}/{total_count} s·∫£n ph·∫©m")
        return results
    
    def convert_images_to_webp(self, product_info_list, output_folder):
        """
        Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ ·∫£nh s·∫£n ph·∫©m sang ƒë·ªãnh d·∫°ng WebP
        
        Args:
            product_info_list (list): Danh s√°ch th√¥ng tin s·∫£n ph·∫©m
            output_folder (str): Th∆∞ m·ª•c l∆∞u ·∫£nh
            
        Returns:
            list: Danh s√°ch th√¥ng tin s·∫£n ph·∫©m v·ªõi ƒë∆∞·ªùng d·∫´n ·∫£nh WebP
        """
        webp_folder = os.path.join(output_folder, "webp_images")
        os.makedirs(webp_folder, exist_ok=True)
        
        converted_products = []
        total_products = len(product_info_list)
        
        logger.info(f"üñºÔ∏è B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi {total_products} ·∫£nh sang WebP...")
        
        for i, product_info in enumerate(product_info_list, 1):
            try:
                img_url = product_info.get('·∫¢nh s·∫£n ph·∫©m', '')
                if not img_url:
                    product_info['·∫¢nh_WebP'] = ''
                    converted_products.append(product_info)
                    logger.info(f"‚è≠Ô∏è B·ªè qua s·∫£n ph·∫©m {i}/{total_products}: Kh√¥ng c√≥ ·∫£nh")
                    continue
                
                # T·∫£i ·∫£nh t·ª´ URL
                response = self.session.get(img_url, timeout=30)
                response.raise_for_status()
                
                # T·∫°o t√™n file WebP an to√†n
                product_code = product_info.get('M√£ s·∫£n ph·∫©m', 'unknown')
                # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh·ªèi t√™n file
                safe_product_code = re.sub(r'[<>:"/\\|?*]', '_', product_code)
                webp_filename = f"{safe_product_code}.webp"
                webp_path = os.path.join(webp_folder, webp_filename)
                
                # Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
                if os.path.exists(webp_path):
                    product_info['·∫¢nh_WebP'] = webp_path
                    converted_products.append(product_info)
                    logger.info(f"‚úì S·ª≠ d·ª•ng ·∫£nh c√≥ s·∫µn {i}/{total_products}: {safe_product_code}")
                    continue
                
                # Chuy·ªÉn ƒë·ªïi sang WebP s·ª≠ d·ª•ng WebPConverter
                img = Image.open(io.BytesIO(response.content))
                
                # Chuy·ªÉn ƒë·ªïi sang RGB n·∫øu c·∫ßn
                if img.mode in ('RGBA', 'LA', 'P'):
                    img = img.convert('RGB')
                
                # L∆∞u ·∫£nh t·∫°m th·ªùi ƒë·ªÉ s·ª≠ d·ª•ng WebPConverter
                temp_img_path = os.path.join(webp_folder, f"temp_{safe_product_code}.jpg")
                img.save(temp_img_path, 'JPEG', quality=95)
                
                # S·ª≠ d·ª•ng WebPConverter ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
                result = WebPConverter.convert_to_webp(
                    input_path=temp_img_path,
                    output_path=webp_path,
                    quality=90,
                    lossless=False,
                    method=6
                )
                
                # X√≥a file t·∫°m
                if os.path.exists(temp_img_path):
                    os.remove(temp_img_path)
                
                if result['success']:
                    # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ·∫£nh
                    product_info['·∫¢nh_WebP'] = webp_path
                    converted_products.append(product_info)
                    logger.info(f"‚úì ƒê√£ chuy·ªÉn ƒë·ªïi ·∫£nh {i}/{total_products}: {safe_product_code} ({result['compression_ratio']:.1f}%)")
                else:
                    logger.error(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi WebP cho {safe_product_code}: {result.get('error', 'Unknown error')}")
                    product_info['·∫¢nh_WebP'] = ''
                    converted_products.append(product_info)
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi ·∫£nh cho s·∫£n ph·∫©m {product_info.get('M√£ s·∫£n ph·∫©m', 'unknown')}: {str(e)}")
                product_info['·∫¢nh_WebP'] = ''
                converted_products.append(product_info)
        
        logger.info(f"‚úÖ Ho√†n th√†nh chuy·ªÉn ƒë·ªïi {len(converted_products)} ·∫£nh sang WebP")
        return converted_products
    
    def create_excel_by_series(self, product_info_list, output_folder):
        """
        T·∫°o file Excel ri√™ng cho t·ª´ng series
        
        Args:
            product_info_list (list): Danh s√°ch th√¥ng tin s·∫£n ph·∫©m
            output_folder (str): Th∆∞ m·ª•c l∆∞u file Excel
            
        Returns:
            list: Danh s√°ch ƒë∆∞·ªùng d·∫´n file Excel ƒë√£ t·∫°o
        """
        # Nh√≥m s·∫£n ph·∫©m theo series
        series_groups = {}
        
        for product in product_info_list:
            series_name = product.get('Series', 'Unknown_Series')
            if series_name not in series_groups:
                series_groups[series_name] = []
            series_groups[series_name].append(product)
        
        excel_files = []
        total_series = len(series_groups)
        
        logger.info(f"üìä B·∫Øt ƒë·∫ßu t·∫°o {total_series} file Excel theo series...")
        
        for i, (series_name, products) in enumerate(series_groups.items(), 1):
            try:
                # T·∫°o t√™n file Excel an to√†n
                safe_series_name = re.sub(r'[<>:"/\\|?*]', '_', series_name)
                excel_filename = f"BAA_Qlight_{safe_series_name}.xlsx"
                excel_path = os.path.join(output_folder, excel_filename)
                
                # T·∫°o DataFrame v·ªõi th·ª© t·ª± c·ªôt h·ª£p l√Ω
                df = pd.DataFrame(products)
                
                # S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt
                column_order = [
                    'STT', 'M√£ s·∫£n ph·∫©m', 'T√™n s·∫£n ph·∫©m', 'Series', 'Gi√°', 
                    'T·ªïng quan', '·∫¢nh s·∫£n ph·∫©m', '·∫¢nh_WebP', 'Series_URL', 'URL'
                ]
                
                # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c√≥ trong d·ªØ li·ªáu
                available_columns = [col for col in column_order if col in df.columns]
                other_columns = [col for col in df.columns if col not in column_order]
                final_columns = available_columns + other_columns
                
                df = df[final_columns]
                
                # L∆∞u file Excel
                with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                    df.to_excel(writer, sheet_name='Products', index=False)
                    
                    # Th√™m ·∫£nh v√†o sheet ri√™ng n·∫øu c√≥
                    workbook = writer.book
                    if '·∫¢nh_WebP' in df.columns:
                        self._add_images_to_excel(workbook, products, output_folder)
                    
                    # Th√™m th√¥ng tin series v√†o sheet m·ªõi
                    self._add_series_info_sheet(workbook, series_name, len(products))
                
                excel_files.append(excel_path)
                logger.info(f"‚úì [{i}/{total_series}] ƒê√£ t·∫°o file Excel: {excel_filename} ({len(products)} s·∫£n ph·∫©m)")
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói t·∫°o Excel cho series {series_name}: {str(e)}")
        
        logger.info(f"‚úÖ Ho√†n th√†nh t·∫°o {len(excel_files)} file Excel")
        return excel_files
    
    def _add_images_to_excel(self, workbook, products, output_folder):
        """
        Th√™m ·∫£nh v√†o file Excel
        
        Args:
            workbook: Workbook object
            products (list): Danh s√°ch s·∫£n ph·∫©m
            output_folder (str): Th∆∞ m·ª•c ch·ª©a ·∫£nh
        """
        try:
            # T·∫°o sheet m·ªõi cho ·∫£nh
            if 'Images' in workbook.sheetnames:
                workbook.remove(workbook['Images'])
            
            image_sheet = workbook.create_sheet('Images')
            
            # Th√™m ti√™u ƒë·ªÅ
            image_sheet['A1'] = 'M√£ s·∫£n ph·∫©m'
            image_sheet['B1'] = 'T√™n s·∫£n ph·∫©m'
            image_sheet['C1'] = '·∫¢nh s·∫£n ph·∫©m'
            
            # Th√™m ·∫£nh cho t·ª´ng s·∫£n ph·∫©m
            for i, product in enumerate(products, start=2):
                image_sheet[f'A{i}'] = product.get('M√£ s·∫£n ph·∫©m', '')
                image_sheet[f'B{i}'] = product.get('T√™n s·∫£n ph·∫©m', '')
                
                webp_path = product.get('·∫¢nh_WebP', '')
                if webp_path and os.path.exists(webp_path):
                    try:
                        # Th√™m ·∫£nh v√†o Excel
                        img = XLImage(webp_path)
                        img.width = 100
                        img.height = 100
                        image_sheet.add_image(img, f'C{i}')
                    except Exception as e:
                        logger.error(f"L·ªói th√™m ·∫£nh v√†o Excel: {str(e)}")
        
        except Exception as e:
            logger.error(f"L·ªói t·∫°o sheet ·∫£nh: {str(e)}")
    
    def _add_series_info_sheet(self, workbook, series_name, product_count):
        """
        Th√™m sheet th√¥ng tin series v√†o workbook
        
        Args:
            workbook: Workbook object
            series_name (str): T√™n series
            product_count (int): S·ªë l∆∞·ª£ng s·∫£n ph·∫©m trong series
        """
        try:
            # T·∫°o sheet m·ªõi cho th√¥ng tin series
            if 'Series_Info' in workbook.sheetnames:
                workbook.remove(workbook['Series_Info'])
            
            info_sheet = workbook.create_sheet('Series_Info')
            
            # Th√™m th√¥ng tin series
            info_sheet['A1'] = 'Th√¥ng tin Series'
            info_sheet['A1'].font = openpyxl.styles.Font(bold=True, size=14)
            
            info_sheet['A3'] = 'T√™n Series:'
            info_sheet['B3'] = series_name
            info_sheet['A3'].font = openpyxl.styles.Font(bold=True)
            
            info_sheet['A4'] = 'S·ªë s·∫£n ph·∫©m:'
            info_sheet['B4'] = product_count
            info_sheet['A4'].font = openpyxl.styles.Font(bold=True)
            
            info_sheet['A5'] = 'Ng√†y t·∫°o:'
            info_sheet['B5'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            info_sheet['A5'].font = openpyxl.styles.Font(bold=True)
            
            # ƒêi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
            info_sheet.column_dimensions['A'].width = 15
            info_sheet.column_dimensions['B'].width = 30
            
        except Exception as e:
            logger.error(f"L·ªói t·∫°o sheet th√¥ng tin series: {str(e)}")
    
    def get_all_series_list(self):
        """
        L·∫•y danh s√°ch t·∫•t c·∫£ series t·ª´ widget tr√™n trang web
        
        Returns:
            list: Danh s√°ch series v·ªõi th√¥ng tin t√™n, URL v√† s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
        """
        try:
            logger.info("üîç ƒêang l·∫•y danh s√°ch t·∫•t c·∫£ series...")
            
            # L·∫•y HTML t·ª´ trang ch√≠nh
            html = get_html_content(self.base_url)
            if not html:
                logger.error("‚ùå Kh√¥ng th·ªÉ t·∫£i trang ch√≠nh")
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # T√¨m widget series theo c·∫•u tr√∫c HTML ƒë√£ cung c·∫•p
            series_widget = soup.select_one('div.widget ul.widget_Items[data-id="Seri"]')
            if not series_widget:
                logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y widget series")
                return []
            
            series_list = []
            
            # T√¨m t·∫•t c·∫£ c√°c item series
            series_items = series_widget.select('li.widget_Item[data-input="Seri"]')
            
            for item in series_items:
                try:
                    # L·∫•y link series
                    link = item.select_one('a')
                    if not link:
                        continue
                    
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # L·∫•y t√™n series
                    label = item.select_one('span.itemLabel')
                    series_name = label.get_text(strip=True) if label else ''
                    
                    # L·∫•y s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
                    count_span = item.select_one('span.count')
                    count_text = count_span.get_text(strip=True) if count_span else '[0]'
                    # Tr√≠ch xu·∫•t s·ªë t·ª´ text [123]
                    count_match = re.search(r'\[(\d+)\]', count_text)
                    product_count = int(count_match.group(1)) if count_match else 0
                    
                    # T·∫°o URL ƒë·∫ßy ƒë·ªß
                    full_url = urljoin(self.base_url, href)
                    
                    series_info = {
                        'name': series_name,
                        'url': full_url,
                        'product_count': product_count,
                        'original_href': href
                    }
                    
                    series_list.append(series_info)
                    
                except Exception as e:
                    logger.error(f"‚ùå L·ªói x·ª≠ l√Ω series item: {str(e)}")
                    continue
            
            # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng s·∫£n ph·∫©m gi·∫£m d·∫ßn
            series_list.sort(key=lambda x: x['product_count'], reverse=True)
            
            logger.info(f"‚úÖ T√¨m th·∫•y {len(series_list)} series")
            return series_list
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l·∫•y danh s√°ch series: {str(e)}")
            return []
    
    def crawl_specific_series(self, series_url, series_name, output_folder=None):
        """
        C√†o d·ªØ li·ªáu cho m·ªôt series c·ª• th·ªÉ
        
        Args:
            series_url (str): URL c·ªßa series
            series_name (str): T√™n series
            output_folder (str): Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
            
        Returns:
            dict: K·∫øt qu·∫£ c√†o d·ªØ li·ªáu cho series
        """
        if not output_folder:
            output_folder = os.path.join(os.getcwd(), f"output_baa_qlight_{series_name}")
        
        os.makedirs(output_folder, exist_ok=True)
        
        start_time = time.time()
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu series: {series_name}")
        
        try:
            # B∆∞·ªõc 1: L·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ series
            logger.info(f"üìã ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ series {series_name}...")
            product_urls = self.get_products_from_series(series_url)
            logger.info(f"‚úì T√¨m th·∫•y {len(product_urls)} s·∫£n ph·∫©m t·ª´ series {series_name}")
            
            if not product_urls:
                return {
                    'success': False,
                    'error': 'Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o trong series',
                    'message': f"Series {series_name} kh√¥ng c√≥ s·∫£n ph·∫©m"
                }
            
            # B∆∞·ªõc 2: C√†o th√¥ng tin s·∫£n ph·∫©m ƒëa lu·ªìng
            logger.info(f"üîÑ ƒêang c√†o th√¥ng tin s·∫£n ph·∫©m t·ª´ series {series_name} (ƒëa lu·ªìng)...")
            product_info_list = self.crawl_products_multithread(product_urls)
            logger.info(f"‚úì ƒê√£ c√†o th√¥ng tin {len(product_info_list)} s·∫£n ph·∫©m")
            
            # B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi ·∫£nh sang WebP
            logger.info(f"üñºÔ∏è ƒêang chuy·ªÉn ƒë·ªïi ·∫£nh sang WebP cho series {series_name}...")
            product_info_list = self.convert_images_to_webp(product_info_list, output_folder)
            logger.info("‚úì Ho√†n th√†nh chuy·ªÉn ƒë·ªïi ·∫£nh WebP")
            
            # B∆∞·ªõc 4: T·∫°o file Excel cho series
            logger.info(f"üìä ƒêang t·∫°o file Excel cho series {series_name}...")
            excel_files = self.create_excel_by_series(product_info_list, output_folder)
            logger.info(f"‚úì ƒê√£ t·∫°o {len(excel_files)} file Excel")
            
            # B∆∞·ªõc 5: T·∫°o file t·ªïng h·ª£p cho series
            logger.info(f"üìã ƒêang t·∫°o file t·ªïng h·ª£p cho series {series_name}...")
            summary_df = pd.DataFrame(product_info_list)
            summary_path = os.path.join(output_folder, f"BAA_Qlight_{series_name}_Summary.xlsx")
            summary_df.to_excel(summary_path, index=False)
            
            end_time = time.time()
            duration = end_time - start_time
            
            result = {
                'success': True,
                'series_name': series_name,
                'series_url': series_url,
                'total_products': len(product_info_list),
                'excel_files': excel_files,
                'summary_file': summary_path,
                'output_folder': output_folder,
                'duration': duration,
                'message': f"Ho√†n th√†nh c√†o d·ªØ li·ªáu {len(product_info_list)} s·∫£n ph·∫©m t·ª´ series {series_name} trong {duration:.2f} gi√¢y"
            }
            
            logger.info(f"‚úÖ {result['message']}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh c√†o d·ªØ li·ªáu series {series_name}: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'message': f"L·ªói c√†o series {series_name}: {str(e)}"
            }
    
    def crawl_baa_qlight(self, output_folder=None):
        """
        H√†m ch√≠nh ƒë·ªÉ c√†o d·ªØ li·ªáu BAA Qlight (t·∫•t c·∫£ series)
        
        Args:
            output_folder (str): Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
            
        Returns:
            dict: K·∫øt qu·∫£ c√†o d·ªØ li·ªáu
        """
        if not output_folder:
            output_folder = os.path.join(os.getcwd(), "output_baa_qlight")
        
        os.makedirs(output_folder, exist_ok=True)
        
        start_time = time.time()
        logger.info("üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu BAA Qlight...")
        
        try:
            # B∆∞·ªõc 1: L·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ trang ch√≠nh
            logger.info("üìã ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ trang ch√≠nh...")
            product_urls = extract_product_urls(self.base_url)
            logger.info(f"‚úì T√¨m th·∫•y {len(product_urls)} s·∫£n ph·∫©m t·ª´ trang ch√≠nh")
            
            # B∆∞·ªõc 2: Thu th·∫≠p th√¥ng tin series t·ª´ c√°c s·∫£n ph·∫©m
            logger.info("üîç ƒêang thu th·∫≠p th√¥ng tin series...")
            series_info = {}
            series_products = {}
            
            # L·∫•y th√¥ng tin series t·ª´ m·ªôt s·ªë s·∫£n ph·∫©m ƒë·∫ßu ti√™n ƒë·ªÉ t√¨m c√°c series
            sample_products = product_urls[:min(10, len(product_urls))]
            for url in sample_products:
                series_data = self.extract_series_info(url)
                if series_data and series_data['name'] not in series_info:
                    series_info[series_data['name']] = series_data
                    logger.info(f"‚úì T√¨m th·∫•y series: {series_data['name']}")
            
            # B∆∞·ªõc 3: L·∫•y s·∫£n ph·∫©m t·ª´ t·ª´ng series
            logger.info("üì¶ ƒêang l·∫•y s·∫£n ph·∫©m t·ª´ c√°c series...")
            all_product_urls = set(product_urls)  # B·∫Øt ƒë·∫ßu v·ªõi s·∫£n ph·∫©m t·ª´ trang ch√≠nh
            
            for series_name, series_data in series_info.items():
                series_products_list = self.get_products_from_series(series_data['url'])
                series_products[series_name] = series_products_list
                all_product_urls.update(series_products_list)
                logger.info(f"‚úì Series '{series_name}': {len(series_products_list)} s·∫£n ph·∫©m")
            
            # Chuy·ªÉn v·ªÅ list v√† lo·∫°i b·ªè tr√πng l·∫∑p
            final_product_urls = list(all_product_urls)
            logger.info(f"‚úì T·ªïng c·ªông {len(final_product_urls)} s·∫£n ph·∫©m unique")
            
            # B∆∞·ªõc 4: C√†o th√¥ng tin s·∫£n ph·∫©m ƒëa lu·ªìng
            logger.info("üîÑ ƒêang c√†o th√¥ng tin s·∫£n ph·∫©m (ƒëa lu·ªìng)...")
            product_info_list = self.crawl_products_multithread(final_product_urls)
            logger.info(f"‚úì ƒê√£ c√†o th√¥ng tin {len(product_info_list)} s·∫£n ph·∫©m")
            
            # B∆∞·ªõc 5: Chuy·ªÉn ƒë·ªïi ·∫£nh sang WebP
            logger.info("üñºÔ∏è ƒêang chuy·ªÉn ƒë·ªïi ·∫£nh sang WebP...")
            product_info_list = self.convert_images_to_webp(product_info_list, output_folder)
            logger.info("‚úì Ho√†n th√†nh chuy·ªÉn ƒë·ªïi ·∫£nh WebP")
            
            # B∆∞·ªõc 6: T·∫°o file Excel theo series
            logger.info("üìä ƒêang t·∫°o file Excel theo series...")
            excel_files = self.create_excel_by_series(product_info_list, output_folder)
            logger.info(f"‚úì ƒê√£ t·∫°o {len(excel_files)} file Excel")
            
            # B∆∞·ªõc 7: T·∫°o file t·ªïng h·ª£p
            logger.info("üìã ƒêang t·∫°o file t·ªïng h·ª£p...")
            summary_df = pd.DataFrame(product_info_list)
            summary_path = os.path.join(output_folder, "BAA_Qlight_Summary.xlsx")
            summary_df.to_excel(summary_path, index=False)
            
            # B∆∞·ªõc 8: T·∫°o b√°o c√°o series
            logger.info("üìà ƒêang t·∫°o b√°o c√°o series...")
            series_report = []
            for series_name, products in series_products.items():
                series_report.append({
                    'Series': series_name,
                    'S·ªë s·∫£n ph·∫©m': len(products),
                    'URL Series': series_info.get(series_name, {}).get('url', '')
                })
            
            series_df = pd.DataFrame(series_report)
            series_report_path = os.path.join(output_folder, "BAA_Qlight_Series_Report.xlsx")
            series_df.to_excel(series_report_path, index=False)
            
            end_time = time.time()
            duration = end_time - start_time
            
            result = {
                'success': True,
                'total_products': len(product_info_list),
                'total_series': len(series_info),
                'excel_files': excel_files,
                'summary_file': summary_path,
                'series_report': series_report_path,
                'output_folder': output_folder,
                'duration': duration,
                'series_info': series_info,
                'message': f"Ho√†n th√†nh c√†o d·ªØ li·ªáu {len(product_info_list)} s·∫£n ph·∫©m t·ª´ {len(series_info)} series trong {duration:.2f} gi√¢y"
            }
            
            logger.info(f"‚úÖ {result['message']}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh c√†o d·ªØ li·ªáu: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'message': f"L·ªói: {str(e)}"
            }

# H√†m ti·ªán √≠ch ƒë·ªÉ s·ª≠ d·ª•ng t·ª´ b√™n ngo√†i
def crawl_baa_qlight(output_folder=None, max_workers=10):
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ c√†o d·ªØ li·ªáu BAA Qlight (t·∫•t c·∫£ series)
    
    Args:
        output_folder (str): Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        max_workers (int): S·ªë lu·ªìng t·ªëi ƒëa
        
    Returns:
        dict: K·∫øt qu·∫£ c√†o d·ªØ li·ªáu
    """
    crawler = BAAQlightCrawler(max_workers=max_workers)
    return crawler.crawl_baa_qlight(output_folder)

def get_all_series_list(max_workers=5):
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ series
    
    Args:
        max_workers (int): S·ªë lu·ªìng t·ªëi ƒëa
        
    Returns:
        list: Danh s√°ch series v·ªõi th√¥ng tin t√™n, URL v√† s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
    """
    crawler = BAAQlightCrawler(max_workers=max_workers)
    return crawler.get_all_series_list()

def crawl_specific_series(series_url, series_name, output_folder=None, max_workers=10):
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ c√†o d·ªØ li·ªáu cho m·ªôt series c·ª• th·ªÉ
    
    Args:
        series_url (str): URL c·ªßa series
        series_name (str): T√™n series
        output_folder (str): Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        max_workers (int): S·ªë lu·ªìng t·ªëi ƒëa
        
    Returns:
        dict: K·∫øt qu·∫£ c√†o d·ªØ li·ªáu cho series
    """
    crawler = BAAQlightCrawler(max_workers=max_workers)
    return crawler.crawl_specific_series(series_url, series_name, output_folder)

def test_series_extraction():
    """
    H√†m test ƒë·ªÉ ki·ªÉm tra vi·ªác tr√≠ch xu·∫•t series
    """
    crawler = BAAQlightCrawler()
    test_url = "https://baa.vn/vn/den-thap-led-sang-tinh-chop-nhay-d45mm-qlight-st45l-and-st45ml-series_4779/"
    
    print("üîç Testing series extraction...")
    series_info = crawler.extract_series_info(test_url)
    if series_info:
        print(f"‚úì T√¨m th·∫•y series: {series_info['name']}")
        print(f"  URL: {series_info['url']}")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin series")
    
    return series_info

def test_product_extraction():
    """
    H√†m test ƒë·ªÉ ki·ªÉm tra vi·ªác tr√≠ch xu·∫•t s·∫£n ph·∫©m
    """
    crawler = BAAQlightCrawler()
    test_url = "https://baa.vn/vn/den-thap-led-sang-tinh-chop-nhay-d45mm-qlight-st45l-and-st45ml-series_4779/"
    
    print("üîç Testing product extraction...")
    product_info = crawler.crawl_product_info(test_url, 1)
    if product_info:
        print(f"‚úì T√¨m th·∫•y s·∫£n ph·∫©m: {product_info.get('T√™n s·∫£n ph·∫©m', 'Unknown')}")
        print(f"  M√£: {product_info.get('M√£ s·∫£n ph·∫©m', 'Unknown')}")
        print(f"  Series: {product_info.get('Series', 'Unknown')}")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin s·∫£n ph·∫©m")
    
    return product_info

if __name__ == "__main__":
    print("üöÄ BAA Qlight Crawler - Test Mode")
    print("=" * 50)
    
    # Test 1: Tr√≠ch xu·∫•t series
    print("\n1. Testing Series Extraction:")
    series_result = test_series_extraction()
    
    # Test 2: Tr√≠ch xu·∫•t s·∫£n ph·∫©m
    print("\n2. Testing Product Extraction:")
    product_result = test_product_extraction()
    
    # Test 3: Ch·∫°y crawler ƒë·∫ßy ƒë·ªß (t√πy ch·ªçn)
    print("\n3. Full Crawler Test (t√πy ch·ªçn):")
    response = input("B·∫°n c√≥ mu·ªën ch·∫°y crawler ƒë·∫ßy ƒë·ªß kh√¥ng? (y/n): ")
    if response.lower() == 'y':
        print("üîÑ B·∫Øt ƒë·∫ßu crawler ƒë·∫ßy ƒë·ªß...")
        result = crawl_baa_qlight(max_workers=5)  # Gi·∫£m s·ªë lu·ªìng ƒë·ªÉ test
        print(f"‚úÖ K·∫øt qu·∫£: {result}")
    else:
        print("‚è≠Ô∏è B·ªè qua test crawler ƒë·∫ßy ƒë·ªß")
    
    print("\n‚úÖ Test ho√†n th√†nh!")
