#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Crawler cho website HoplongTech.com
Chuy√™n c√†o d·ªØ li·ªáu c·∫£m bi·∫øn v√† thi·∫øt b·ªã t·ª± ƒë·ªông h√≥a
Copyright ¬© 2025 Haiphongtech.vn
"""

import os
import re
import time
import json
import requests
import traceback
from datetime import datetime
from urllib.parse import urljoin, urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import threading

# Import c√°c h√†m utility - s·ª≠ d·ª•ng try-except ƒë·ªÉ tr√°nh l·ªói import
try:
    from . import utils
except ImportError:
    # Fallback cho standalone execution
    try:
        import utils
    except ImportError:
        utils = None

# Import c√°c h√†m utility t·ª´ module ch√≠nh
try:
    from .category_crawler import log_and_emit, update_progress, safe_print
except ImportError:
    # Fallback n·∫øu kh√¥ng import ƒë∆∞·ª£c
    def log_and_emit(message):
        print(f"[LOG] {message}")
    
    def update_progress(percent, message):
        print(f"[{percent}%] {message}")
    
    def safe_print(message):
        print(message)

class HoplongCrawler:
    """
    Crawler chuy√™n d·ª•ng cho website HoplongTech.com
    C√†o d·ªØ li·ªáu c·∫£m bi·∫øn v√† thi·∫øt b·ªã t·ª± ƒë·ªông h√≥a v·ªõi ph√¢n chia theo th∆∞∆°ng hi·ªáu
    """
    
    def __init__(self, socketio=None):
        """
        Kh·ªüi t·∫°o HoplongCrawler
        
        Args:
            socketio: Socket.IO instance ƒë·ªÉ emit ti·∫øn tr√¨nh (optional)
        """
        self.socketio = socketio
        self.base_url = "https://hoplongtech.com"
        self.category_base_url = "https://hoplongtech.com/category/cam-bien"
        
        # C·∫•u h√¨nh session v·ªõi retry strategy
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers m√¥ ph·ªèng tr√¨nh duy·ªát th·∫≠t
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # C·∫•u h√¨nh crawling t·ªëi ∆∞u cho t·ªëc ƒë·ªô
        self.max_workers = 8  # TƒÉng s·ªë thread ƒë·ªÉ crawl nhanh h∆°n
        self.delay_between_requests = 0.3  # Gi·∫£m delay ƒë·ªÉ tƒÉng t·ªëc
        self.batch_size = 50  # X·ª≠ l√Ω theo batch ƒë·ªÉ hi·ªáu qu·∫£ h∆°n
        
        log_and_emit("‚úÖ ƒê√£ kh·ªüi t·∫°o HoplongCrawler v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u t·ªëc ƒë·ªô")
        log_and_emit(f"üöÄ C·∫•u h√¨nh: {self.max_workers} workers, delay {self.delay_between_requests}s")
    
    def get_sensor_categories(self):
        """
        L·∫•y danh s√°ch c√°c lo·∫°i c·∫£m bi·∫øn t·ª´ trang danh m·ª•c
        
        Returns:
            list: Danh s√°ch c√°c danh m·ª•c c·∫£m bi·∫øn v·ªõi th√¥ng tin chi ti·∫øt
        """
        log_and_emit(f"üîç ƒêang l·∫•y danh s√°ch danh m·ª•c c·∫£m bi·∫øn t·ª´ {self.category_base_url}")
        
        try:
            response = self.session.get(self.category_base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            categories = []
            
            # T√¨m t·∫•t c·∫£ div.cate-name
            cate_divs = soup.find_all('div', class_='cate-name')
            
            for div in cate_divs:
                try:
                    # L·∫•y link v√† t√™n danh m·ª•c
                    link_tag = div.find('a')
                    if not link_tag:
                        continue
                    
                    category_url = link_tag.get('href')
                    if not category_url:
                        continue
                    
                    # ƒê·∫£m b·∫£o URL ƒë·∫ßy ƒë·ªß
                    if category_url.startswith('/'):
                        category_url = urljoin(self.base_url, category_url)
                    
                    category_name = link_tag.get_text(strip=True)
                    
                    # L·∫•y s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
                    span_tag = div.find('span')
                    product_count_text = span_tag.get_text(strip=True) if span_tag else "0 S·∫£n ph·∫©m"
                    
                    # Tr√≠ch xu·∫•t s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
                    count_match = re.search(r'(\d+)', product_count_text)
                    product_count = int(count_match.group(1)) if count_match else 0
                    
                    category_info = {
                        'name': category_name,
                        'url': category_url,
                        'product_count': product_count,
                        'display_text': f"{category_name} ({product_count} s·∫£n ph·∫©m)"
                    }
                    
                    categories.append(category_info)
                    log_and_emit(f"üìÇ T√¨m th·∫•y danh m·ª•c: {category_name} - {product_count} s·∫£n ph·∫©m")
                    
                except Exception as e:
                    log_and_emit(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω danh m·ª•c: {str(e)}")
                    continue
            
            log_and_emit(f"‚úÖ ƒê√£ l·∫•y {len(categories)} danh m·ª•c c·∫£m bi·∫øn")
            return categories
            
        except Exception as e:
            log_and_emit(f"‚ùå L·ªói l·∫•y danh s√°ch danh m·ª•c: {str(e)}")
            return []
    
    def get_products_from_category(self, category_url, max_pages=None):
        """
        L·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c
        
        Args:
            category_url (str): URL c·ªßa danh m·ª•c
            max_pages (int, optional): S·ªë trang t·ªëi ƒëa ƒë·ªÉ c√†o
            
        Returns:
            list: Danh s√°ch URL s·∫£n ph·∫©m
        """
        log_and_emit(f"üîç ƒêang l·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´: {category_url}")
        
        product_urls = []
        current_page = 1
        
        while True:
            if max_pages and current_page > max_pages:
                break
            
            # T·∫°o URL cho trang hi·ªán t·∫°i
            if current_page == 1:
                page_url = category_url
            else:
                # Ki·ªÉm tra format pagination c·ªßa website
                if '?' in category_url:
                    page_url = f"{category_url}&page={current_page}"
                else:
                    page_url = f"{category_url}?page={current_page}"
            
            try:
                log_and_emit(f"üìÑ ƒêang c√†o trang {current_page}: {page_url}")
                
                response = self.session.get(page_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m c√°c link s·∫£n ph·∫©m
                page_products = self._extract_product_links(soup)
                
                if not page_products:
                    log_and_emit(f"üìÑ Trang {current_page} kh√¥ng c√≥ s·∫£n ph·∫©m, k·∫øt th√∫c")
                    break
                
                product_urls.extend(page_products)
                log_and_emit(f"üì¶ Trang {current_page}: T√¨m th·∫•y {len(page_products)} s·∫£n ph·∫©m")
                
                # Ki·ªÉm tra xem c√≥ trang ti·∫øp theo kh√¥ng
                if not self._has_next_page(soup):
                    log_and_emit(f"üìÑ ƒê√£ ƒë·∫øn trang cu·ªëi, k·∫øt th√∫c")
                    break
                
                current_page += 1
                time.sleep(self.delay_between_requests)
                
            except Exception as e:
                log_and_emit(f"‚ùå L·ªói c√†o trang {current_page}: {str(e)}")
                break
        
        log_and_emit(f"‚úÖ ƒê√£ l·∫•y {len(product_urls)} s·∫£n ph·∫©m t·ª´ {current_page-1} trang")
        return product_urls
    
    def _extract_product_links(self, soup):
        """
        Tr√≠ch xu·∫•t link s·∫£n ph·∫©m t·ª´ soup c·ªßa trang danh m·ª•c
        
        Args:
            soup: BeautifulSoup object c·ªßa trang
            
        Returns:
            list: Danh s√°ch URL s·∫£n ph·∫©m
        """
        product_links = []
        
        # T√¨m c√°c link s·∫£n ph·∫©m - c√≥ th·ªÉ c√≥ nhi·ªÅu pattern kh√°c nhau
        selectors = [
            'a[href*="/products/"]',  # Link ch·ª©a /products/
            'a[wire\\:navigate][title]',  # Link c√≥ wire:navigate v√† title
            '.product-item a',  # Link trong item s·∫£n ph·∫©m
            '.product-card a',  # Link trong card s·∫£n ph·∫©m
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if href and '/products/' in href:
                    # ƒê·∫£m b·∫£o URL ƒë·∫ßy ƒë·ªß
                    if href.startswith('/'):
                        full_url = urljoin(self.base_url, href)
                    else:
                        full_url = href
                    
                    if full_url not in product_links:
                        product_links.append(full_url)
        
        return product_links
    
    def _has_next_page(self, soup):
        """
        Ki·ªÉm tra xem c√≥ trang ti·∫øp theo kh√¥ng
        
        Args:
            soup: BeautifulSoup object c·ªßa trang hi·ªán t·∫°i
            
        Returns:
            bool: True n·∫øu c√≥ trang ti·∫øp theo
        """
        # C√°c selector c√≥ th·ªÉ ƒë·ªÉ t√¨m n√∫t "Next" ho·∫∑c pagination
        next_selectors = [
            '.pagination .next:not(.disabled)',
            '.pagination a[rel="next"]',
            '.page-nav .next',
        ]
        
        for selector in next_selectors:
            if soup.select(selector):
                return True
        
        # T√¨m link ch·ª©a text "Trang sau", "Next", ">"
        links = soup.find_all('a')
        for link in links:
            link_text = link.get_text(strip=True)
            if link_text in ['Trang sau', 'Next', '>', '‚Ä∫', '¬ª']:
                return True
        
        return False
    
    def crawl_product_details(self, product_url):
        """
        C√†o th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m
        
        Args:
            product_url (str): URL c·ªßa s·∫£n ph·∫©m
            
        Returns:
            dict: Th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
        """
        try:
            log_and_emit(f"üì¶ ƒêang c√†o s·∫£n ph·∫©m: {product_url}")
            
            response = self.session.get(product_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # L·∫•y th√¥ng tin c∆° b·∫£n
            product_info = {
                'url': product_url,
                'name': self._extract_product_name(soup),
                'code': self._extract_product_code(soup),
                'price': self._extract_product_price(soup),
                'brand': self._extract_product_brand(soup),
                'specifications': self._extract_specifications(soup),
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            log_and_emit(f"‚úÖ ƒê√£ c√†o th√†nh c√¥ng: {product_info['name']}")
            return product_info
            
        except Exception as e:
            log_and_emit(f"‚ùå L·ªói c√†o s·∫£n ph·∫©m {product_url}: {str(e)}")
            return None
    
    def _extract_product_name(self, soup):
        """Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m"""
        selectors = [
            'h1.content-title',
            'h1',
            '.product-title',
            '.content-title'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return "Kh√¥ng c√≥ t√™n s·∫£n ph·∫©m"
    
    def _extract_product_code(self, soup):
        """Tr√≠ch xu·∫•t m√£ s·∫£n ph·∫©m"""
        selectors = [
            'p.content-meta__sku',
            '.product-sku',
            '.sku'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text(strip=True)
                # Tr√≠ch xu·∫•t m√£ t·ª´ text "M√£ s·∫£n ph·∫©m: ABC123"
                match = re.search(r'M√£ s·∫£n ph·∫©m:\s*(.+)', text)
                if match:
                    return match.group(1).strip()
                return text
        
        return "Kh√¥ng c√≥ m√£ s·∫£n ph·∫©m"
    
    def _extract_product_price(self, soup):
        """Tr√≠ch xu·∫•t gi√° s·∫£n ph·∫©m"""
        price_info = {
            'price': None,
            'price_text': None,
            'vat_note': None
        }
        
        # T√¨m div ch·ª©a gi√°
        price_container = soup.select_one('.left')
        if price_container:
            # L·∫•y gi√° ch√≠nh
            price_elements = price_container.select('p')
            for p in price_elements:
                text = p.get_text(strip=True)
                if 'ƒë' in text and 'VAT' not in text.upper():
                    price_info['price_text'] = text
                    # Tr√≠ch xu·∫•t s·ªë ti·ªÅn
                    price_match = re.search(r'([\d,.]+)ƒë', text)
                    if price_match:
                        price_str = price_match.group(1).replace(',', '').replace('.', '')
                        try:
                            price_info['price'] = int(price_str)
                        except:
                            pass
                elif 'VAT' in text.upper():
                    price_info['vat_note'] = text
        
        return price_info
    
    def _extract_product_brand(self, soup):
        """Tr√≠ch xu·∫•t th∆∞∆°ng hi·ªáu s·∫£n ph·∫©m"""
        selectors = [
            'a.content-meta__brand',
            '.brand-name',
            '.product-brand a',
            '.brand a'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return "Kh√¥ng c√≥ th∆∞∆°ng hi·ªáu"
    
    def _extract_specifications(self, soup):
        """Tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t"""
        specifications = {}
        
        # T√¨m div ch·ª©a th√¥ng s·ªë k·ªπ thu·∫≠t
        tech_div = soup.select_one('#technical')
        if not tech_div:
            tech_div = soup.select_one('[id="technical"]')
        if not tech_div:
            tech_div = soup.select_one('.content-tab__detail')
        
        if tech_div:
            # T√¨m t·∫•t c·∫£ c√°c ul trong div th√¥ng s·ªë
            ul_elements = tech_div.find_all('ul')
            
            for ul in ul_elements:
                li_elements = ul.find_all('li')
                
                for li in li_elements:
                    title_span = li.find('span', class_='title')
                    content_span = li.find('span', class_='content')
                    
                    if title_span and content_span:
                        title = title_span.get_text(strip=True)
                        content = content_span.get_text(strip=True)
                        specifications[title] = content
        
        return specifications
    
    def crawl_category_products(self, selected_categories, output_dir=None, max_products_per_category=None):
        """
        C√†o s·∫£n ph·∫©m t·ª´ c√°c danh m·ª•c ƒë√£ ch·ªçn
        
        Args:
            selected_categories (list): Danh s√°ch c√°c danh m·ª•c ƒë√£ ch·ªçn
            output_dir (str, optional): Th∆∞ m·ª•c output
            max_products_per_category (int, optional): S·ªë s·∫£n ph·∫©m t·ªëi ƒëa m·ªói danh m·ª•c
            
        Returns:
            dict: K·∫øt qu·∫£ crawling
        """
        if not output_dir:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"hoplongtech_products_{timestamp}"
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(output_dir, exist_ok=True)
        
        results = {
            'total_categories': len(selected_categories),
            'total_products': 0,
            'successful_products': 0,
            'failed_products': 0,
            'categories_data': {},
            'brands_data': {},
            'output_dir': output_dir,
            'errors': [],
            'start_time': time.time()  # Track th·ªùi gian b·∫Øt ƒë·∫ßu
        }
        
        log_and_emit(f"üöÄ B·∫Øt ƒë·∫ßu c√†o {len(selected_categories)} danh m·ª•c")
        
        try:
            for idx, category in enumerate(selected_categories, 1):
                category_name = category['name']
                category_url = category['url']
                
                log_and_emit(f"üìÇ [{idx}/{len(selected_categories)}] ƒêang x·ª≠ l√Ω danh m·ª•c: {category_name}")
                update_progress((idx-1) * 100 // len(selected_categories), 
                              f"ƒêang c√†o danh m·ª•c: {category_name}")
                
                try:
                    # L·∫•y danh s√°ch s·∫£n ph·∫©m
                    product_urls = self.get_products_from_category(category_url)
                    
                    if max_products_per_category:
                        product_urls = product_urls[:max_products_per_category]
                    
                    results['total_products'] += len(product_urls)
                    log_and_emit(f"üì¶ T√¨m th·∫•y {len(product_urls)} s·∫£n ph·∫©m trong danh m·ª•c {category_name}")
                    
                    # C√†o th√¥ng tin t·ª´ng s·∫£n ph·∫©m
                    category_products = []
                    
                    if product_urls:
                        log_and_emit(f"üî• B·∫Øt ƒë·∫ßu crawl {len(product_urls)} s·∫£n ph·∫©m v·ªõi {self.max_workers} workers")
                        
                        # X·ª≠ l√Ω theo batch ƒë·ªÉ t·ªëi ∆∞u
                        total_products = len(product_urls)
                        completed_products = 0
                        
                        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                            # Submit t·∫•t c·∫£ tasks c√πng l√∫c
                            future_to_url = {
                                executor.submit(self.crawl_product_details, url): url 
                                for url in product_urls
                            }
                            
                            # Process results as they complete
                            for future in as_completed(future_to_url):
                                url = future_to_url[future]
                                try:
                                    product_info = future.result()
                                    completed_products += 1
                                    
                                    # Update progress chi ti·∫øt
                                    progress_percent = (completed_products * 100) // total_products
                                    update_progress(
                                        (idx-1) * 100 // len(selected_categories) + progress_percent // len(selected_categories), 
                                        f"[{category_name}] ƒê√£ c√†o {completed_products}/{total_products} s·∫£n ph·∫©m"
                                    )
                                    
                                    if product_info:
                                        category_products.append(product_info)
                                        results['successful_products'] += 1
                                        
                                        # Ph√¢n lo·∫°i theo th∆∞∆°ng hi·ªáu
                                        brand = product_info['brand']
                                        if brand not in results['brands_data']:
                                            results['brands_data'][brand] = []
                                        results['brands_data'][brand].append(product_info)
                                        
                                        # Log ti·∫øn tr√¨nh chi ti·∫øt m·ªói 10 s·∫£n ph·∫©m
                                        if completed_products % 10 == 0:
                                            log_and_emit(f"‚ö° ƒê√£ ho√†n th√†nh {completed_products}/{total_products} s·∫£n ph·∫©m ({progress_percent}%)")
                                        
                                    else:
                                        results['failed_products'] += 1
                                        results['errors'].append(f"Kh√¥ng c√†o ƒë∆∞·ª£c s·∫£n ph·∫©m: {url}")
                                        
                                except Exception as e:
                                    completed_products += 1
                                    log_and_emit(f"‚ùå L·ªói x·ª≠ l√Ω s·∫£n ph·∫©m {url}: {str(e)}")
                                    results['failed_products'] += 1
                                    results['errors'].append(f"L·ªói c√†o s·∫£n ph·∫©m {url}: {str(e)}")
                                
                                # Gi·∫£m delay v√¨ ƒë√£ c√≥ rate limiting trong session
                                time.sleep(self.delay_between_requests / self.max_workers)
                    
                    results['categories_data'][category_name] = category_products
                    log_and_emit(f"‚úÖ Ho√†n th√†nh danh m·ª•c {category_name}: {len(category_products)} s·∫£n ph·∫©m th√†nh c√¥ng")
                    
                except Exception as e:
                    error_msg = f"L·ªói x·ª≠ l√Ω danh m·ª•c {category_name}: {str(e)}"
                    log_and_emit(f"‚ùå {error_msg}")
                    results['errors'].append(error_msg)
                    continue
            
            # L∆∞u k·∫øt qu·∫£
            self._save_results(results, output_dir)
            
            update_progress(100, "Ho√†n th√†nh crawling!")
            log_and_emit(f"üéâ Crawling ho√†n th√†nh! K·∫øt qu·∫£: {results['successful_products']}/{results['total_products']} s·∫£n ph·∫©m")
            
            if results['errors']:
                log_and_emit(f"‚ö†Ô∏è C√≥ {len(results['errors'])} l·ªói trong qu√° tr√¨nh crawling")
            
            return results
            
        except Exception as e:
            error_msg = f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh crawling: {str(e)}"
            log_and_emit(f"‚ùå {error_msg}")
            results['errors'].append(error_msg)
            import traceback
            traceback.print_exc()
            return results
    
    def _save_results(self, results, output_dir):
        """
        L∆∞u k·∫øt qu·∫£ crawling
        
        Args:
            results (dict): D·ªØ li·ªáu k·∫øt qu·∫£
            output_dir (str): Th∆∞ m·ª•c output
        """
        log_and_emit("üíæ ƒêang l∆∞u k·∫øt qu·∫£...")
        
        # L∆∞u theo th∆∞∆°ng hi·ªáu (nh∆∞ y√™u c·∫ßu)
        brands_dir = os.path.join(output_dir, "brands")
        os.makedirs(brands_dir, exist_ok=True)
        
        for brand, products in results['brands_data'].items():
            if products:
                # T·∫°o t√™n file an to√†n
                safe_brand_name = re.sub(r'[^\w\s-]', '', brand).strip()
                safe_brand_name = re.sub(r'[-\s]+', '_', safe_brand_name)
                
                brand_dir = os.path.join(brands_dir, safe_brand_name)
                os.makedirs(brand_dir, exist_ok=True)
                
                # L∆∞u JSON
                json_file = os.path.join(brand_dir, f"{safe_brand_name}_products.json")
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(products, f, ensure_ascii=False, indent=2)
                
                # T·∫°o b·∫£ng Excel
                self._create_excel_report(products, brand_dir, safe_brand_name)
                
                log_and_emit(f"üìÅ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m th∆∞∆°ng hi·ªáu {brand}")
        
        # L∆∞u t·ªïng h·ª£p v·ªõi th·ªëng k√™ chi ti·∫øt
        summary_data = {
            'summary': {
                'total_categories': results['total_categories'],
                'total_products': results['total_products'],
                'successful_products': results['successful_products'],
                'failed_products': results['failed_products'],
                'success_rate': round(results['successful_products'] / results['total_products'] * 100, 2) if results['total_products'] > 0 else 0,
                'brands_count': len(results['brands_data']),
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'crawl_duration': f"{time.time() - results['start_time']:.1f}s"
            },
            'brands_summary': {
                brand: len(products) 
                for brand, products in results['brands_data'].items()
            },
            'categories_summary': {
                cat_name: len(products)
                for cat_name, products in results['categories_data'].items()
            },
            'errors': results.get('errors', [])
        }
        
        summary_file = os.path.join(output_dir, "crawling_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # T·∫°o file ZIP ƒë·ªÉ download
        zip_file = self._create_download_zip(output_dir)
        results['download_file'] = zip_file
        
        log_and_emit(f"‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ k·∫øt qu·∫£ v√†o: {output_dir}")
        log_and_emit(f"üì¶ File t·∫£i xu·ªëng: {zip_file}")
    
    def _create_excel_report(self, products, output_dir, brand_name):
        """
        T·∫°o b√°o c√°o Excel cho s·∫£n ph·∫©m
        
        Args:
            products (list): Danh s√°ch s·∫£n ph·∫©m
            output_dir (str): Th∆∞ m·ª•c output
            brand_name (str): T√™n th∆∞∆°ng hi·ªáu
        """
        try:
            import pandas as pd
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho Excel
            excel_data = []
            
            for product in products:
                row = {
                    'T√™n s·∫£n ph·∫©m': product.get('name', ''),
                    'M√£ s·∫£n ph·∫©m': product.get('code', ''),
                    'Th∆∞∆°ng hi·ªáu': product.get('brand', ''),
                    'Gi√° (VNƒê)': product.get('price', {}).get('price', ''),
                    'Gi√° (Text)': product.get('price', {}).get('price_text', ''),
                    'Ghi ch√∫ VAT': product.get('price', {}).get('vat_note', ''),
                    'URL': product.get('url', ''),
                    'Th·ªùi gian c√†o': product.get('crawled_at', '')
                }
                
                # Th√™m th√¥ng s·ªë k·ªπ thu·∫≠t
                specs = product.get('specifications', {})
                for spec_name, spec_value in specs.items():
                    row[f"Th√¥ng s·ªë: {spec_name}"] = spec_value
                
                excel_data.append(row)
            
            # T·∫°o DataFrame v√† l∆∞u Excel
            df = pd.DataFrame(excel_data)
            excel_file = os.path.join(output_dir, f"{brand_name}_products.xlsx")
            
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='S·∫£n ph·∫©m', index=False)
                
                # ƒê·ªãnh d·∫°ng worksheet
                worksheet = writer.sheets['S·∫£n ph·∫©m']
                
                # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            log_and_emit(f"üìä ƒê√£ t·∫°o b√°o c√°o Excel: {excel_file}")
            
        except ImportError:
            log_and_emit("‚ö†Ô∏è Kh√¥ng c√≥ pandas, b·ªè qua t·∫°o file Excel")
        except Exception as e:
            log_and_emit(f"‚ùå L·ªói t·∫°o Excel: {str(e)}")
    
    def _create_download_zip(self, output_dir):
        """
        T·∫°o file ZIP ƒë·ªÉ download
        
        Args:
            output_dir (str): Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ZIP
        """
        import zipfile
        
        zip_filename = f"{output_dir}.zip"
        
        try:
            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Th√™m t·∫•t c·∫£ files trong output_dir
                for root, dirs, files in os.walk(output_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arc_name = os.path.relpath(file_path, os.path.dirname(output_dir))
                        zipf.write(file_path, arc_name)
            
            log_and_emit(f"üì¶ ƒê√£ t·∫°o file ZIP: {zip_filename}")
            return zip_filename
            
        except Exception as e:
            log_and_emit(f"‚ùå L·ªói t·∫°o ZIP: {str(e)}")
            return None
    
    def get_category_selection_interface(self):
        """
        T·∫°o giao di·ªán cho ng∆∞·ªùi d√πng ch·ªçn danh m·ª•c
        
        Returns:
            dict: Th√¥ng tin cho giao di·ªán web
        """
        categories = self.get_sensor_categories()
        
        return {
            'success': True,
            'categories': categories,
            'total_categories': len(categories),
            'total_products': sum(cat['product_count'] for cat in categories),
            'base_url': self.base_url,
            'category_base_url': self.category_base_url
        }

# C√°c h√†m utility b·ªï sung
def create_hoplong_crawler_session():
    """T·∫°o session m·ªõi cho HoplongCrawler"""
    return HoplongCrawler()

def test_hoplong_crawler():
    """H√†m test c∆° b·∫£n cho HoplongCrawler"""
    crawler = HoplongCrawler()
    
    print("üß™ Testing HoplongCrawler...")
    
    # Test l·∫•y danh m·ª•c
    categories = crawler.get_sensor_categories()
    print(f"‚úÖ T√¨m th·∫•y {len(categories)} danh m·ª•c")
    
    if categories:
        # Test c√†o 1 s·∫£n ph·∫©m t·ª´ danh m·ª•c ƒë·∫ßu ti√™n
        first_category = categories[0]
        print(f"üîç Test danh m·ª•c: {first_category['name']}")
        
        products = crawler.get_products_from_category(first_category['url'], max_pages=1)
        print(f"üì¶ T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
        
        if products:
            # Test crawl 1 s·∫£n ph·∫©m
            product_info = crawler.crawl_product_details(products[0])
            if product_info:
                print(f"‚úÖ Test th√†nh c√¥ng: {product_info['name']}")
                return True
    
    print("‚ùå Test th·∫•t b·∫°i")
    return False

if __name__ == "__main__":
    test_hoplong_crawler() 