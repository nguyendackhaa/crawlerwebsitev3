"""
Autonics Product Crawler - C√†o d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ website Autonics.com
Phi√™n b·∫£n: 1.0
T√°c gi·∫£: Auto-generated based on existing crawler patterns
Ng√†y t·∫°o: $(date)
"""

import os
import time
import requests
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import concurrent.futures
from queue import Queue
import pandas as pd
from PIL import Image, ImageEnhance
from io import BytesIO
import json
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
try:
    from app.webp_converter import WebPConverter
except ImportError:
    from webp_converter import WebPConverter
import threading

# Selenium imports for dynamic content
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def sanitize_folder_name(name):
    """L√†m s·∫°ch t√™n folder ƒë·ªÉ ph√π h·ª£p v·ªõi h·ªá ƒëi·ªÅu h√†nh"""
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá
    clean_name = re.sub(r'[<>:"/\\|?*]', '_', name)
    clean_name = re.sub(r'[^\w\s-]', '', clean_name).strip()
    clean_name = re.sub(r'[-\s]+', '_', clean_name)
    return clean_name if clean_name else 'Unknown_Category'

def standardize_filename(code):
    """Chu·∫©n h√≥a m√£ s·∫£n ph·∫©m th√†nh t√™n file h·ª£p l·ªá"""
    return re.sub(r'[\\/:*?"<>|,=\s]', '-', code).replace('-+', '-').strip('-').upper()

class AutonicsCrawler:
    """
    Crawler chuy√™n d·ª•ng cho website Autonics.com
    C√†o d·ªØ li·ªáu c·∫£m bi·∫øn v√† thi·∫øt b·ªã t·ª± ƒë·ªông h√≥a v·ªõi x·ª≠ l√Ω ƒëa lu·ªìng
    """
    
    def __init__(self, output_root=None, max_workers=8, max_retries=3, socketio=None):
        """
        Kh·ªüi t·∫°o AutonicsCrawler
        
        Args:
            output_root: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u k·∫øt qu·∫£
            max_workers: S·ªë lu·ªìng t·ªëi ƒëa
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i khi request th·∫•t b·∫°i
            socketio: Socket.IO instance ƒë·ªÉ emit ti·∫øn tr√¨nh
        """
        self.output_root = output_root or os.path.join(os.getcwd(), "output_autonics")
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.socketio = socketio
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(self.output_root, exist_ok=True)
        
        # Base URLs
        self.base_url = "https://www.autonics.com"
        self.vietnam_base_url = "https://www.autonics.com/vn"
        
        # C·∫•u h√¨nh session v·ªõi retry strategy
        self.session = requests.Session()
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Headers m√¥ ph·ªèng tr√¨nh duy·ªát th·∫≠t
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # C·∫•u h√¨nh Selenium WebDriver (t∆∞∆°ng t·ª± FotekScraper)
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        # Disable logging ƒë·ªÉ gi·∫£m noise
        self.chrome_options.add_argument('--log-level=3')
        self.chrome_options.add_argument('--disable-logging')
        self.chrome_options.add_argument('--disable-extensions')
        
        # Th·ªëng k√™
        self.stats = {
            "categories_processed": 0,
            "series_found": 0,
            "products_found": 0,
            "products_processed": 0,
            "images_downloaded": 0,
            "failed_requests": 0,
            "failed_images": 0
        }
        
        logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o AutonicsCrawler v·ªõi Selenium support")
    
    def get_driver(self):
        """T·∫°o m·ªôt WebDriver Chrome m·ªõi (t∆∞∆°ng t·ª± FotekScraper)"""
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.implicitly_wait(10)
            return driver
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver: {e}")
            raise
    
    def close_driver(self, driver):
        """ƒê√≥ng WebDriver"""
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"L·ªói khi ƒë√≥ng WebDriver: {e}")
    
    def emit_progress(self, percent, message, detail=""):
        """Emit ti·∫øn tr√¨nh qua Socket.IO"""
        if self.socketio:
            self.socketio.emit('progress_update', {
                'percent': percent,
                'message': message,
                'detail': detail
            })
        else:
            print(f"[{percent}%] {message} - {detail}")
    
    def get_html_content(self, url, timeout=30):
        """L·∫•y n·ªôi dung HTML t·ª´ URL"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"L·ªói khi l·∫•y HTML t·ª´ {url}: {str(e)}")
            return None
    
    def extract_series_from_category(self, category_url):
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ series t·ª´ trang category v·ªõi x·ª≠ l√Ω ph√¢n trang
        Website Autonics s·ª≠ d·ª•ng server-side rendering v·ªõi Vue.js, data ƒë∆∞·ª£c embed trong window.__INIT_DATA__
        
        Args:
            category_url: URL c·ªßa trang category
            
        Returns:
            list: Danh s√°ch c√°c series URLs v√† metadata
        """
        series_data = []
        page = 1
        
        while True:
            # T·∫°o URL v·ªõi tham s·ªë page
            if '?' in category_url:
                paginated_url = f"{category_url}&page={page}"
            else:
                paginated_url = f"{category_url}?page={page}"
            
            self.emit_progress(
                0, 
                f"ƒêang thu th·∫≠p series t·ª´ trang {page}",
                f"URL: {paginated_url}"
            )
            
            html = self.get_html_content(paginated_url)
            if not html:
                break
            
            # Extract data t·ª´ window.__INIT_DATA__
            current_page_series = self.extract_init_data_from_html(html)
            
            if not current_page_series:
                break
            
            # Th√™m v√†o danh s√°ch t·ªïng
            for series in current_page_series:
                if series not in series_data:
                    series_data.append(series)
            
            # Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng b·∫±ng c√°ch parse pagination info
            has_next = self.check_has_next_page(html, page)
            
            if not has_next:
                break
                
            page += 1
        
        # Convert th√†nh series URLs
        series_urls = []
        for series in series_data:
            if series.get('urlNm'):
                series_url = f"{self.base_url}/vn/series/{series['urlNm']}"
                series_urls.append(series_url)
        
        self.stats["series_found"] += len(series_urls)
        logger.info(f"T√¨m th·∫•y {len(series_urls)} series t·ª´ {category_url}")
        logger.info(f"Series URLs: {series_urls[:5]}{'...' if len(series_urls) > 5 else ''}")
        return series_urls
    
    def extract_init_data_from_html(self, html):
        """
        Extract d·ªØ li·ªáu t·ª´ window.__INIT_DATA__ object trong HTML
        
        Args:
            html: HTML content
            
        Returns:
            list: Danh s√°ch series data t·ª´ resultList
        """
        try:
            # T√¨m script tag ch·ª©a window.__INIT_DATA__
            start_marker = 'window.__INIT_DATA__ = '
            start_idx = html.find(start_marker)
            
            if start_idx == -1:
                logger.warning("Kh√¥ng t√¨m th·∫•y window.__INIT_DATA__ trong HTML")
                return []
            
            # T√¨m ƒëi·ªÉm b·∫Øt ƒë·∫ßu JSON
            json_start = start_idx + len(start_marker)
            
            # T√¨m ƒëi·ªÉm k·∫øt th√∫c JSON (tr∆∞·ªõc d·∫•u ;)
            json_end = html.find(';\n', json_start)
            if json_end == -1:
                json_end = html.find(';', json_start)
            
            if json_end == -1:
                logger.warning("Kh√¥ng t√¨m th·∫•y ƒëi·ªÉm k·∫øt th√∫c JSON")
                return []
            
            # Extract JSON string
            json_str = html[json_start:json_end].strip()
            
            # Parse JSON
            import json
            data = json.loads(json_str)
            
            # L·∫•y resultList
            result_list = data.get('resultList', [])
            
            logger.info(f"Extracted {len(result_list)} series t·ª´ __INIT_DATA__")
            
            return result_list
            
        except Exception as e:
            logger.error(f"L·ªói khi extract __INIT_DATA__: {str(e)}")
            return []
    
    def check_has_next_page(self, html, current_page):
        """
        Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng b·∫±ng c√°ch parse pagination info t·ª´ __INIT_DATA__
        
        Args:
            html: HTML content
            current_page: Trang hi·ªán t·∫°i
            
        Returns:
            bool: True n·∫øu c√≥ trang ti·∫øp theo
        """
        try:
            start_marker = 'window.__INIT_DATA__ = '
            start_idx = html.find(start_marker)
            
            if start_idx == -1:
                return False
            
            json_start = start_idx + len(start_marker)
            json_end = html.find(';\n', json_start)
            if json_end == -1:
                json_end = html.find(';', json_start)
            
            if json_end == -1:
                return False
            
            json_str = html[json_start:json_end].strip()
            
            import json
            data = json.loads(json_str)
            
            # L·∫•y pagination info
            pagination_info = data.get('paginationInfo', {})
            total_page_count = pagination_info.get('totalPageCount', 1)
            
            logger.info(f"Pagination: trang {current_page}/{total_page_count}")
            
            return current_page < total_page_count
            
        except Exception as e:
            logger.error(f"L·ªói khi check pagination: {str(e)}")
            return False
    
    def detect_url_type(self, url):
        """
        Ph√°t hi·ªán lo·∫°i URL: category, series, ho·∫∑c model
        
        Args:
            url: URL c·∫ßn ph√¢n t√≠ch
            
        Returns:
            str: 'category', 'series', 'model', ho·∫∑c 'unknown'
        """
        try:
            if '/vn/product/category/' in url:
                return 'category'
            elif '/vn/series/' in url:
                return 'series'
            elif '/vn/model/' in url:
                return 'model'
            else:
                return 'unknown'
        except Exception as e:
            logger.error(f"L·ªói khi detect URL type cho {url}: {str(e)}")
            return 'unknown'
    
    def extract_model_code_from_url(self, url):
        """
        Tr√≠ch xu·∫•t model code t·ª´ model URL
        
        Args:
            url: Model URL (v√≠ d·ª•: https://www.autonics.com/vn/model/BYS500-TDT1,2)
            
        Returns:
            str: Model code (v√≠ d·ª•: BYS500-TDT1,2)
        """
        try:
            # Extract t·ª´ URL pattern: /vn/model/{model_code}
            parts = url.split('/vn/model/')
            if len(parts) > 1:
                model_code = parts[1].strip()
                # Remove any query parameters
                if '?' in model_code:
                    model_code = model_code.split('?')[0]
                return model_code
            return ''
        except Exception as e:
            logger.error(f"L·ªói khi extract model code t·ª´ URL {url}: {str(e)}")
            return ''
    
    def crawl_single_model(self, model_url):
        """
        C√†o d·ªØ li·ªáu t·ª´ m·ªôt model URL c·ª• th·ªÉ
        
        Args:
            model_url: URL c·ªßa model c·ª• th·ªÉ
            
        Returns:
            tuple: (products_data, category_name)
        """
        logger.info(f"B·∫Øt ƒë·∫ßu c√†o single model: {model_url}")
        
        # Extract model code t·ª´ URL
        model_code = self.extract_model_code_from_url(model_url)
        if not model_code:
            logger.error(f"Kh√¥ng th·ªÉ extract model code t·ª´ URL: {model_url}")
            return [], "Unknown_Model"
        
        self.emit_progress(10, f"ƒêang c√†o model: {model_code}")
        
        try:
            # L·∫•y th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
            self.emit_progress(50, f"ƒêang l·∫•y th√¥ng tin chi ti·∫øt model {model_code}...")
            product_details = self.extract_product_details(model_url)
            
            if not product_details:
                logger.warning(f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin chi ti·∫øt cho model: {model_url}")
                return [], model_code
            
            # Update stats
            self.stats["products_found"] += 1
            self.stats["products_processed"] += 1
            
            self.emit_progress(90, f"ƒê√£ ho√†n th√†nh c√†o model {model_code}")
            
            # T·∫°o category name t·ª´ model code ho·∫∑c category info
            category_name = product_details.get('category', model_code)
            if not category_name:
                category_name = f"Model_{sanitize_folder_name(model_code)}"
            else:
                category_name = sanitize_folder_name(category_name)
            
            logger.info(f"ƒê√£ c√†o th√†nh c√¥ng model {model_code} thu·ªôc category {category_name}")
            
            return [product_details], category_name
            
        except Exception as e:
            logger.error(f"L·ªói khi c√†o single model {model_url}: {str(e)}")
            return [], model_code
    
    def crawl_multiple_models(self, model_urls):
        """
        C√†o d·ªØ li·ªáu t·ª´ nhi·ªÅu model URLs v√† g·ªôp v√†o 1 folder/Excel chung
        
        Args:
            model_urls: List c√°c model URLs
            
        Returns:
            tuple: (all_products_data, folder_name)
        """
        logger.info(f"B·∫Øt ƒë·∫ßu c√†o {len(model_urls)} models v√† g·ªôp v√†o 1 folder chung")
        
        all_products_data = []
        successful_models = []
        failed_models = []
        
        for i, model_url in enumerate(model_urls):
            try:
                progress = (i / len(model_urls)) * 80  # Reserve 20% for final processing
                self.emit_progress(progress, f"ƒêang c√†o model {i+1}/{len(model_urls)}")
                
                # Extract model code
                model_code = self.extract_model_code_from_url(model_url)
                if not model_code:
                    logger.warning(f"Kh√¥ng th·ªÉ extract model code t·ª´ URL: {model_url}")
                    failed_models.append(model_url)
                    continue
                
                # L·∫•y th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
                product_details = self.extract_product_details(model_url)
                
                if product_details:
                    all_products_data.append(product_details)
                    successful_models.append(model_code)
                    
                    # Update stats
                    self.stats["products_found"] += 1
                    self.stats["products_processed"] += 1
                    
                    logger.info(f"‚úÖ ƒê√£ c√†o th√†nh c√¥ng model: {model_code}")
                else:
                    logger.warning(f"‚ùå Kh√¥ng th·ªÉ l·∫•y th√¥ng tin chi ti·∫øt cho model: {model_url}")
                    failed_models.append(model_url)
                
            except Exception as e:
                logger.error(f"L·ªói khi c√†o model {model_url}: {str(e)}")
                failed_models.append(model_url)
        
        # T·∫°o folder name d·ª±a tr√™n s·ªë l∆∞·ª£ng models th√†nh c√¥ng
        if successful_models:
            if len(successful_models) == 1:
                folder_name = f"Single_Model_{successful_models[0]}"
            else:
                folder_name = f"Multiple_Models_{len(successful_models)}_Products"
            
            folder_name = sanitize_folder_name(folder_name)
        else:
            folder_name = "Failed_Models"
        
        # Log k·∫øt qu·∫£
        logger.info(f"=== K·∫æT QU·∫¢ CRAWL MULTIPLE MODELS ===")
        logger.info(f"T·ªïng models: {len(model_urls)}")
        logger.info(f"Th√†nh c√¥ng: {len(successful_models)}")
        logger.info(f"Th·∫•t b·∫°i: {len(failed_models)}")
        logger.info(f"Folder name: {folder_name}")
        
        if successful_models:
            logger.info(f"Models th√†nh c√¥ng: {', '.join(successful_models[:10])}{'...' if len(successful_models) > 10 else ''}")
        
        if failed_models:
            logger.warning(f"Models th·∫•t b·∫°i: {failed_models}")
        
        return all_products_data, folder_name
    
    def extract_products_from_series(self, series_url):
        """
        Tr√≠ch xu·∫•t t·∫•t c·∫£ s·∫£n ph·∫©m t·ª´ trang series v·ªõi Selenium WebDriver v√† pagination handling
        Series pages s·ª≠ d·ª•ng client-side rendering, c·∫ßn Selenium ƒë·ªÉ load ƒë·∫ßy ƒë·ªß
        
        Args:
            series_url: URL c·ªßa trang series
            
        Returns:
            list: Danh s√°ch c√°c product URLs v√† metadata
        """
        logger.info(f"B·∫Øt ƒë·∫ßu extract products t·ª´ series: {series_url}")
        
        # S·ª≠ d·ª•ng Selenium thay v√¨ requests (t∆∞∆°ng t·ª± FotekScraper)
        driver = self.get_driver()
        all_products_data = []
        
        try:
            # Single page load ƒë·ªÉ extract both expected count v√† max pages
            logger.info(f"üîç Loading {series_url} ƒë·ªÉ extract metadata...")
            driver.get(series_url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(3)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract expected count t·ª´ title
            expected_count = self.extract_count_from_soup(soup)
            logger.info(f"üéØ Expected product count t·ª´ title: {expected_count}")
            
            # Extract max pages t·ª´ same soup
            max_pages = self.extract_max_pages_from_soup(soup)
            logger.info(f"üìÑ Detected max pages: {max_pages}")
            
            # X·ª≠ l√Ω ph√¢n trang - l·∫∑p qua t·∫•t c·∫£ c√°c trang
            page = 1
            
            while page <= max_pages:
                # T·∫°o URL v·ªõi page parameter
                if page == 1:
                    page_url = series_url
                else:
                    separator = '&' if '?' in series_url else '?'
                    page_url = f"{series_url}{separator}page={page}"
                
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω trang {page}: {page_url}")
                
                driver.get(page_url)
                
                # ƒê·ª£i page load xong
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # Increased wait time for pages 3+ (dynamic loading issue)
                if page >= 3:
                    logger.info(f"‚è≥ Increased wait time for page {page} (potential dynamic loading)")
                    time.sleep(8)  # Longer wait for higher pages
                else:
                    time.sleep(3)
                    
                # Wait for series-model section to be fully loaded
                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "section#series-model"))
                    )
                    logger.info(f"‚úÖ series-model section loaded for page {page}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  series-model section not found within timeout for page {page}: {e}")
                    # Continue anyway, might still have data
                
                # L·∫•y HTML sau khi JavaScript render
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # T√¨m section ch·ª©a models
                series_model = soup.find('section', id='series-model')
                if not series_model:
                    logger.warning(f"Kh√¥ng t√¨m th·∫•y section#series-model trong {page_url}")
                    break
                
                # T√¨m t·∫•t c·∫£ model items
                model_items = series_model.find_all('li')
                
                # Filter ra ch·ªâ nh·ªØng li c√≥ product links
                valid_items = []
                for item in model_items:
                    a_tag = item.find('a')
                    if a_tag and a_tag.get('href') and '/vn/model/' in a_tag.get('href'):
                        valid_items.append(item)
                
                logger.info(f"üì¶ Trang {page}: T√¨m th·∫•y {len(valid_items)} s·∫£n ph·∫©m trong {len(model_items)} items")
                
                # N·∫øu kh√¥ng c√≥ s·∫£n ph·∫©m n√†o, retry m·ªôt l·∫ßn tr∆∞·ªõc khi stop
                if not valid_items:
                    if page <= 3:  # Only retry for early pages that should have data
                        logger.warning(f"‚ö†Ô∏è  No products found on page {page}, retrying v·ªõi longer wait...")
                        time.sleep(10)  # Extended wait for retry
                        
                        # Retry: reload page
                        driver.get(page_url)
                        WebDriverWait(driver, 15).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        time.sleep(8)
                        
                        # Re-parse after retry
                        html = driver.page_source
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        series_model = soup.find('section', id='series-model')
                        if series_model:
                            model_items = series_model.find_all('li')
                            valid_items = []
                            for item in model_items:
                                a_tag = item.find('a')
                                if a_tag and a_tag.get('href') and '/vn/model/' in a_tag.get('href'):
                                    valid_items.append(item)
                        
                        logger.info(f"üîÑ RETRY: Trang {page} found {len(valid_items)} products after retry")
                    
                    if not valid_items:
                        logger.info(f"‚úÖ Confirmed: Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ·ªü trang {page}, k·∫øt th√∫c pagination")
                        break
                
                # Extract products t·ª´ trang n√†y
                page_products = []
                for item in valid_items:
                    try:
                        a_tag = item.find('a')
                        href = a_tag.get('href')
                        
                        # T·∫°o full URL
                        product_url = urljoin(self.base_url, href)
                        
                        # L·∫•y t√™n model
                        title_element = item.find('p', class_='title') or item.find('em', class_='title')
                        model_name = ''
                        if title_element:
                            model_name = title_element.get_text(strip=True)
                        
                        # L·∫•y model code t·ª´ URL ho·∫∑c title
                        model_code = model_name
                        if not model_code:
                            # Extract t·ª´ URL: /vn/model/BYS500-TDT1,2 -> BYS500-TDT1,2
                            url_parts = href.split('/')
                            if len(url_parts) > 0:
                                model_code = url_parts[-1]
                        
                        if model_code:
                            product_data = {
                                'url': product_url,
                                'name': model_name or model_code,
                                'series_url': series_url,
                                'model_code': model_code,
                                'page': page
                            }
                            
                            page_products.append(product_data)
                            
                    except Exception as e:
                        logger.warning(f"L·ªói khi extract model item: {str(e)}")
                        continue
                
                all_products_data.extend(page_products)
                logger.info(f"‚úÖ Trang {page}: Extract ƒë∆∞·ª£c {len(page_products)} s·∫£n ph·∫©m")
                
                # Ki·ªÉm tra xem c√≥ trang ti·∫øp theo kh√¥ng
                paging_wrap = soup.find('div', class_='paging-wrap')
                if not paging_wrap:
                    logger.info("üìÑ Kh√¥ng t√¨m th·∫•y pagination, k·∫øt th√∫c")
                    break
                
                # T√¨m trang hi·ªán t·∫°i v√† check next page
                current_page_link = paging_wrap.find('a', class_='current')
                if current_page_link:
                    current_page_num = current_page_link.get_text(strip=True)
                    logger.info(f"üìç ƒêang ·ªü trang: {current_page_num}")
                
                # Check next page link
                next_links = paging_wrap.find_all('a')
                has_next = False
                for link in next_links:
                    link_text = link.get_text(strip=True)
                    # T√¨m page number l·ªõn h∆°n page hi·ªán t·∫°i
                    if link_text.isdigit() and int(link_text) == page + 1:
                        has_next = True
                        break
                
                if not has_next:
                    logger.info(f"‚úÖ ƒê√£ ƒë·∫øn trang cu·ªëi c√πng (trang {page})")
                    break
                
                page += 1
            
            self.stats["products_found"] += len(all_products_data)
            
            # Validation: So s√°nh actual vs expected count
            actual_count = len(all_products_data)
            if expected_count > 0:
                if actual_count == expected_count:
                    logger.info(f"‚úÖ PERFECT MATCH: {actual_count}/{expected_count} s·∫£n ph·∫©m")
                elif actual_count > expected_count:
                    logger.warning(f"‚ö†Ô∏è  OVER-CRAWLED: {actual_count}/{expected_count} s·∫£n ph·∫©m (+{actual_count - expected_count})")
                else:
                    logger.warning(f"‚ö†Ô∏è  UNDER-CRAWLED: {actual_count}/{expected_count} s·∫£n ph·∫©m (-{expected_count - actual_count})")
            
            logger.info(f"üéâ T·ªîNG C·ªòNG: T√¨m th·∫•y {actual_count} s·∫£n ph·∫©m t·ª´ {page-1} trang c·ªßa {series_url}")
            
            return all_products_data
            
        except Exception as e:
            logger.error(f"L·ªói khi extract products t·ª´ {series_url}: {str(e)}")
            return []
            
        finally:
            self.close_driver(driver)
    
    def extract_count_from_soup(self, soup):
        """
        Extract expected product count t·ª´ BeautifulSoup object
        <h3 class="sub-title col-4 col-m-12">PRFD Series Model <span class="fc0">(68)</span></h3>
        """
        try:
            title_element = soup.find('h3', class_='sub-title')
            if not title_element:
                return 0
            
            count_span = title_element.find('span', class_='fc0')
            if not count_span:
                return 0
            
            count_text = count_span.get_text(strip=True)
            count_match = re.search(r'\((\d+)\)', count_text)
            
            if count_match:
                expected_count = int(count_match.group(1))
                return expected_count
            else:
                return 0
        except Exception as e:
            logger.error(f"L·ªói extract count t·ª´ soup: {str(e)}")
            return 0
    
    def extract_max_pages_from_soup(self, soup):
        """
        Extract max pages t·ª´ BeautifulSoup object
        """
        try:
            paging_wrap = soup.find('div', class_='paging-wrap')
            if not paging_wrap:
                return 1
            
            page_numbers = []
            for link in paging_wrap.find_all('a'):
                link_text = link.get_text(strip=True)
                if link_text.isdigit():
                    page_numbers.append(int(link_text))
            
            if page_numbers:
                max_page = max(page_numbers)
                logger.info(f"‚úÖ Found pagination numbers: {sorted(page_numbers)}, max = {max_page}")
                return max_page
            else:
                return 1
        except Exception as e:
            logger.error(f"L·ªói extract max pages t·ª´ soup: {str(e)}")
            return 50  # Fallback
    
    def get_expected_product_count(self, series_url, driver):
        """
        Extract expected product count t·ª´ title element
        <h3 class="sub-title col-4 col-m-12">PRFD Series Model <span class="fc0">(68)</span></h3>
        
        Args:
            series_url: URL c·ªßa series page
            driver: Selenium WebDriver instance
            
        Returns:
            int: Expected product count, 0 n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            logger.info(f"üîç Extracting expected count t·ª´: {series_url}")
            driver.get(series_url)
            
            # Wait for page load
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(3)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # Find title element v·ªõi pattern: "PRFD Series Model (68)"
            title_element = soup.find('h3', class_='sub-title')
            if not title_element:
                logger.warning("Kh√¥ng t√¨m th·∫•y h3.sub-title element")
                return 0
            
            # Extract count t·ª´ span v·ªõi class fc0
            count_span = title_element.find('span', class_='fc0')
            if not count_span:
                logger.warning("Kh√¥ng t√¨m th·∫•y span.fc0 trong title")
                return 0
            
            # Parse count t·ª´ "(68)" format
            count_text = count_span.get_text(strip=True)
            count_match = re.search(r'\((\d+)\)', count_text)
            
            if count_match:
                expected_count = int(count_match.group(1))
                logger.info(f"‚úÖ Found expected count: {expected_count}")
                return expected_count
            else:
                logger.warning(f"Cannot parse count t·ª´: '{count_text}'")
                return 0
                
        except Exception as e:
            logger.error(f"L·ªói khi extract expected count: {str(e)}")
            return 0
    
    def detect_total_pages(self, series_url, driver):
        """
        Detect total number of pages t·ª´ pagination links
        
        Args:
            series_url: URL c·ªßa series page  
            driver: Selenium WebDriver instance
            
        Returns:
            int: Total pages, default 50 n·∫øu kh√¥ng detect ƒë∆∞·ª£c
        """
        try:
            logger.info(f"üîç Detecting total pages t·ª´: {series_url}")
            driver.get(series_url)
            
            # Wait for page load
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(3)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # Find pagination wrapper
            paging_wrap = soup.find('div', class_='paging-wrap')
            if not paging_wrap:
                logger.info("Kh√¥ng t√¨m th·∫•y pagination, assume 1 page")
                return 1
            
            # Extract all page numbers
            page_numbers = []
            for link in paging_wrap.find_all('a'):
                link_text = link.get_text(strip=True)
                if link_text.isdigit():
                    page_numbers.append(int(link_text))
            
            if page_numbers:
                max_page = max(page_numbers)
                logger.info(f"‚úÖ Detected max page: {max_page} t·ª´ pagination")
                return max_page
            else:
                logger.info("Kh√¥ng detect ƒë∆∞·ª£c page numbers, assume 1 page")
                return 1
                
        except Exception as e:
            logger.error(f"L·ªói khi detect total pages: {str(e)}")
            # Fallback: unlimited scanning v·ªõi safety limit
            logger.info("‚ö†Ô∏è  Fallback: Using safety limit 50 pages")
            return 50
    
# Removed extract_products_from_init_data method - now using Selenium directly
    
    def extract_product_details(self, product_url):
        """
        Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt c·ªßa s·∫£n ph·∫©m
        Product detail page c≈©ng s·ª≠ d·ª•ng window.__INIT_DATA__ ƒë·ªÉ embed data
        
        Args:
            product_url: URL c·ªßa trang s·∫£n ph·∫©m
            
        Returns:
            dict: Th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m
        """
        html = self.get_html_content(product_url)
        if not html:
            return None
        
        # Extract data t·ª´ __INIT_DATA__ tr∆∞·ªõc
        init_data = self.extract_product_init_data(html)
        
        # Fallback to HTML parsing n·∫øu kh√¥ng c√≥ __INIT_DATA__
        soup = BeautifulSoup(html, 'html.parser')
        
        # Kh·ªüi t·∫°o d·ªØ li·ªáu s·∫£n ph·∫©m
        product_data = {
            'url': product_url,
            'product_code': '',
            'product_name': '',
            'full_product_name': '',
            'image_url': '',
            'specifications': {},
            'category': '',
            'series': ''
        }
        
        try:
            # ∆Øu ti√™n data t·ª´ __INIT_DATA__ n·∫øu c√≥
            if init_data:
                model_info = init_data.get('modelVo', {})
                
                # L·∫•y m√£ s·∫£n ph·∫©m
                product_data['product_code'] = model_info.get('modlCode', model_info.get('modelCode', ''))
                
                # T·∫°o t√™n s·∫£n ph·∫©m theo th·ª© t·ª±: Category + Model + Series + Brand
                name_parts = []
                
                # 1. Category
                category_info = init_data.get('categoryVo', {})
                category_name = ''
                if category_info:
                    category_name = category_info.get('ctgryNm', '')
                    if category_name:
                        product_data['category'] = category_name
                
                # 2. Model name
                model_name = model_info.get('modlNm', model_info.get('modelName', product_data['product_code']))
                if model_name:
                    product_data['product_name'] = model_name
                
                # 3. Series
                series_info = init_data.get('seriesVo', {})
                series_name = ''
                if series_info:
                    series_name = series_info.get('seriesNm', '')
                    if series_name:
                        product_data['series'] = series_name
                
                # Gh√©p t√™n theo th·ª© t·ª±: Category + Model + Series + Brand
                if category_name:
                    name_parts.append(category_name)
                if model_name:
                    name_parts.append(model_name)
                if series_name:
                    name_parts.append(series_name)
                name_parts.append('AUTONICS')
                
                product_data['full_product_name'] = ' '.join(name_parts)
                
                # Image URL
                image_url = model_info.get('imageUrl', '')
                if image_url:
                    if image_url.startswith('/'):
                        product_data['image_url'] = f"{self.base_url}/web{image_url}"
                    else:
                        product_data['image_url'] = urljoin(self.base_url, image_url)
                
                # Specifications t·ª´ specList ho·∫∑c data structure kh√°c
                spec_list = init_data.get('specList', [])
                for spec in spec_list:
                    spec_name = spec.get('specNm', spec.get('name', ''))
                    spec_value = spec.get('specValue', spec.get('value', ''))
                    if spec_name and spec_value:
                        product_data['specifications'][spec_name] = spec_value
                
                # Th√™m c√°c specs t·ª´ model info
                if model_info:
                    # C√°c field c√≥ th·ªÉ ch·ª©a specs
                    spec_fields = ['modlSfe', 'modlSfeTwo', 'modlSfeThree', 'modlDc']
                    for field in spec_fields:
                        value = model_info.get(field, '')
                        if value:
                            product_data['specifications'][field.replace('modl', '').replace('Sfe', 'Feature')] = value
                
                logger.info(f"Extracted product details t·ª´ __INIT_DATA__ cho {product_data['product_code']}")
                return product_data
            
            # Fallback: Parse HTML n·∫øu kh√¥ng c√≥ __INIT_DATA__
            logger.info(f"Fallback to HTML parsing cho {product_url}")
            
            # 1. L·∫•y m√£ s·∫£n ph·∫©m
            title_box = soup.find('div', class_='title-box')
            if title_box:
                title_p = title_box.find('p', class_='title')
                if title_p:
                    product_data['product_code'] = title_p.get_text(strip=True)
            
            # 2. L·∫•y c√°c ph·∫ßn t·ª≠ ƒë·ªÉ t·∫°o t√™n s·∫£n ph·∫©m ƒë·∫ßy ƒë·ªß theo th·ª© t·ª±:
            # Category + Model Name + Series + Brand
            
            # 1. L·∫•y category t·ª´ link c√≥ data-categoryon
            category_name = ''
            category_link = soup.find('a', {'data-categoryon': True})
            if category_link:
                category_span = category_link.find('span')
                if category_span:
                    category_name = category_span.get_text(strip=True)
                    product_data['category'] = category_name
            
            # Fallback: t√¨m category link th√¥ng th∆∞·ªùng
            if not category_name:
                category_link = soup.find('a', href=lambda x: x and '/vn/product/category/' in x)
                if category_link:
                    category_span = category_link.find('span')
                    if category_span:
                        category_name = category_span.get_text(strip=True)
                        product_data['category'] = category_name
            
            # 2. L·∫•y model name t·ª´ li.current
            model_name = ''
            current_li = soup.find('li', class_='current')
            if current_li:
                model_name = current_li.get_text(strip=True)
                product_data['product_name'] = model_name
            
            # 3. L·∫•y series name t·ª´ link c√≥ span ch·ª©a "Series"
            series_name = ''
            # T√¨m trong navigation breadcrumb
            nav_items = soup.find_all('li')
            for li in nav_items:
                a_tag = li.find('a')
                if a_tag and a_tag.get('href', '').startswith('javascript:'):
                    span = a_tag.find('span')
                    if span and 'Series' in span.get_text():
                        series_name = span.get_text(strip=True)
                        product_data['series'] = series_name
                        break
            
            # Gh√©p t√™n s·∫£n ph·∫©m theo th·ª© t·ª±: Category + Model + Series + Brand
            name_parts = []
            if category_name:
                name_parts.append(category_name)
            if model_name:
                name_parts.append(model_name)
            if series_name:
                name_parts.append(series_name)
            name_parts.append('AUTONICS')
            
            product_data['full_product_name'] = ' '.join(name_parts)
            
            # 3. L·∫•y ·∫£nh s·∫£n ph·∫©m
            img_box = soup.find('div', class_='img-box')
            if img_box:
                img_tag = img_box.find('img', id='img-chg')
                if img_tag and 'src' in img_tag.attrs:
                    img_src = img_tag['src']
                    product_data['image_url'] = urljoin(self.base_url, img_src)
            
            # 4. L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t
            spec_table = soup.find('table', class_='table')
            if spec_table:
                rows = spec_table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(strip=True)
                        product_data['specifications'][key] = value
            
            return product_data
            
        except Exception as e:
            logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ {product_url}: {str(e)}")
            return None
    
    def extract_product_init_data(self, html):
        """
        Extract product data t·ª´ window.__INIT_DATA__ object
        
        Args:
            html: HTML content
            
        Returns:
            dict: Product data t·ª´ __INIT_DATA__
        """
        try:
            start_marker = 'window.__INIT_DATA__ = '
            start_idx = html.find(start_marker)
            
            if start_idx == -1:
                return None
            
            json_start = start_idx + len(start_marker)
            json_end = html.find(';\n', json_start)
            if json_end == -1:
                json_end = html.find(';', json_start)
            
            if json_end == -1:
                return None
            
            json_str = html[json_start:json_end].strip()
            
            import json
            data = json.loads(json_str)
            
            return data
            
        except Exception as e:
            logger.error(f"L·ªói khi extract product __INIT_DATA__: {str(e)}")
            return None
    
    def add_white_background_to_image(self, image, target_size=(800, 800)):
        """
        Th√™m n·ªÅn tr·∫Øng v√†o ·∫£nh v√† resize v·ªÅ k√≠ch th∆∞·ªõc target
        
        Args:
            image: PIL Image object
            target_size: K√≠ch th∆∞·ªõc m·ª•c ti√™u (width, height)
            
        Returns:
            PIL Image: ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        # T·∫°o ·∫£nh n·ªÅn tr·∫Øng v·ªõi k√≠ch th∆∞·ªõc target
        background = Image.new('RGB', target_size, (255, 255, 255))
        
        # Convert ·∫£nh g·ªëc sang RGBA n·∫øu c·∫ßn
        if image.mode != 'RGBA':
            image = image.convert('RGBA')
        
        # T√≠nh to√°n t·ª∑ l·ªá resize ƒë·ªÉ fit v√†o target size
        img_ratio = min(target_size[0] / image.width, target_size[1] / image.height)
        new_size = (int(image.width * img_ratio), int(image.height * img_ratio))
        
        # Resize ·∫£nh
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        # T√≠nh to√°n v·ªã tr√≠ ƒë·ªÉ paste ·∫£nh v√†o gi·ªØa n·ªÅn tr·∫Øng
        x = (target_size[0] - new_size[0]) // 2
        y = (target_size[1] - new_size[1]) // 2
        
        # Paste ·∫£nh v√†o n·ªÅn tr·∫Øng
        background.paste(image, (x, y), image if image.mode == 'RGBA' else None)
        
        return background
    
    def download_and_process_image(self, image_url, save_path, product_code):
        """
        T·∫£i v√† x·ª≠ l√Ω ·∫£nh s·∫£n ph·∫©m
        
        Args:
            image_url: URL c·ªßa ·∫£nh
            save_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh
            product_code: M√£ s·∫£n ph·∫©m
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            # T·∫£i ·∫£nh
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # M·ªü ·∫£nh t·ª´ bytes
            image = Image.open(BytesIO(response.content))
            
            # Th√™m n·ªÅn tr·∫Øng
            processed_image = self.add_white_background_to_image(image)
            
            # T·∫°o t√™n file
            filename = f"{standardize_filename(product_code)}.webp"
            full_path = os.path.join(save_path, filename)
            
            # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(save_path, exist_ok=True)
            
            # L∆∞u ·∫£nh t·∫°m d∆∞·ªõi d·∫°ng PNG
            temp_path = full_path.replace('.webp', '.png')
            processed_image.save(temp_path, 'PNG', quality=95)
            
            # Chuy·ªÉn ƒë·ªïi sang WebP
            result = WebPConverter.convert_to_webp(
                input_path=temp_path,
                output_path=full_path,
                quality=90,
                lossless=False,
                method=6
            )
            
            # X√≥a file t·∫°m
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            if result['success']:
                self.stats["images_downloaded"] += 1
                return True
            else:
                self.stats["failed_images"] += 1
                return False
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh {image_url}: {str(e)}")
            self.stats["failed_images"] += 1
            return False
    
    def create_excel_with_specifications(self, products_data, output_path):
        """
        T·∫°o file Excel v·ªõi th√¥ng s·ªë k·ªπ thu·∫≠t theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu
        
        Args:
            products_data: Danh s√°ch d·ªØ li·ªáu s·∫£n ph·∫©m
            output_path: ƒê∆∞·ªùng d·∫´n file Excel
        """
        excel_data = []
        
        for product in products_data:
            # T·∫°o b·∫£ng HTML th√¥ng s·ªë k·ªπ thu·∫≠t
            specs_html = '<table id="specifications" border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; font-family: Arial; width: 100%;"><thead><tr style="background-color: #f2f2f2;"><th>Th√¥ng s·ªë</th><th>Gi√° tr·ªã</th></tr></thead><tbody>'
            
            # Th√™m m√£ s·∫£n ph·∫©m v√† t√™n s·∫£n ph·∫©m
            specs_html += f'<tr><td style="font-weight: bold;">M√£ s·∫£n ph·∫©m</td><td>{product.get("product_code", "")}</td></tr>'
            specs_html += f'<tr><td style="font-weight: bold;">T√™n s·∫£n ph·∫©m</td><td>{product.get("full_product_name", "")}</td></tr>'
            
            # Th√™m c√°c th√¥ng s·ªë k·ªπ thu·∫≠t
            for key, value in product.get('specifications', {}).items():
                specs_html += f'<tr><td style="font-weight: bold;">{key}</td><td>{value}</td></tr>'
            
            # Th√™m copyright
            specs_html += '<tr><td style="font-weight: bold;">Copyright</td><td>Haiphongtech.vn</td></tr>'
            specs_html += '</tbody></table>'
            
            # T·∫°o URL s·∫£n ph·∫©m v√† link ·∫£nh
            domain = "https://haiphongtech.vn/product/"
            image_base = "https://haiphongtech.vn/wp-content/uploads/temp-images/"
            
            product_code = product.get("product_code", "")
            clean_code = re.sub(r'[\\/:*?"<>|,=\s]', '-', product_code)
            clean_code = re.sub(r'-+', '-', clean_code).strip('-')
            
            slug = clean_code.lower()
            image_name = clean_code.upper()
            
            row_data = {
                'T√™n s·∫£n ph·∫©m': product.get('full_product_name', ''),
                'M√£ s·∫£n ph·∫©m': product.get('product_code', ''),
                'ƒê∆∞·ªùng d·∫´n s·∫£n ph·∫©m': domain + slug if slug else '',
                'Th√¥ng s·ªë k·ªπ thu·∫≠t': specs_html,
                'Link ·∫£nh ƒë√£ x·ª≠ l√Ω': image_base + image_name + '.webp' if image_name else '',
                'URL g·ªëc': product.get('url', ''),
                'Danh m·ª•c': product.get('category', ''),
                'Series': product.get('series', '')
            }
            
            excel_data.append(row_data)
        
        # T·∫°o DataFrame v√† l∆∞u Excel
        df = pd.DataFrame(excel_data)
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Products')
            
            # ƒêi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
            worksheet = writer.sheets['Products']
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        logger.info(f"ƒê√£ t·∫°o file Excel: {output_path}")
    
    def crawl_category(self, category_url):
        """
        C√†o d·ªØ li·ªáu t·ª´ m·ªôt category URL
        
        Args:
            category_url: URL c·ªßa category
            
        Returns:
            tuple: (products_data, category_name)
        """
        # L·∫•y t√™n category t·ª´ URL
        category_name = category_url.split('/')[-1]
        category_name = sanitize_folder_name(category_name)
        
        self.emit_progress(10, f"B·∫Øt ƒë·∫ßu c√†o category: {category_name}")
        
        # 1. L·∫•y danh s√°ch series
        self.emit_progress(20, "ƒêang thu th·∫≠p danh s√°ch series...")
        series_urls = self.extract_series_from_category(category_url)
        
        if not series_urls:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y series n√†o trong {category_url}")
            return [], category_name
        
        # 2. L·∫•y danh s√°ch s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ series
        self.emit_progress(40, f"ƒêang thu th·∫≠p s·∫£n ph·∫©m t·ª´ {len(series_urls)} series...")
        all_products_data = []
        
        for i, series_url in enumerate(series_urls):
            progress = 40 + (i / len(series_urls)) * 30
            self.emit_progress(progress, f"ƒêang x·ª≠ l√Ω series {i+1}/{len(series_urls)}")
            
            products_data = self.extract_products_from_series(series_url)
            all_products_data.extend(products_data)
        
        # 3. L·∫•y th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m v·ªõi ƒëa lu·ªìng
        self.emit_progress(70, f"ƒêang l·∫•y th√¥ng tin chi ti·∫øt {len(all_products_data)} s·∫£n ph·∫©m...")
        detailed_products = []
        
        def process_product(product_data):
            try:
                details = self.extract_product_details(product_data['url'])
                if details:
                    return details
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω s·∫£n ph·∫©m {product_data['url']}: {str(e)}")
            return None
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_product, product) for product in all_products_data]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                result = future.result()
                if result:
                    detailed_products.append(result)
                    self.stats["products_processed"] += 1
                
                progress = 70 + (i / len(futures)) * 20
                self.emit_progress(progress, f"ƒê√£ x·ª≠ l√Ω {i+1}/{len(futures)} s·∫£n ph·∫©m")
        
        return detailed_products, category_name
    
    def crawl_products(self, urls):
        """
        C√†o d·ªØ li·ªáu t·ª´ danh s√°ch URLs (c√≥ th·ªÉ l√† category URLs ho·∫∑c model URLs)
        
        Args:
            urls: Danh s√°ch URL (categories ho·∫∑c models)
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c k·∫øt qu·∫£
        """
        start_time = time.time()
        
        # T·∫°o th∆∞ m·ª•c k·∫øt qu·∫£ v·ªõi timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = os.path.join(self.output_root, f"AutonicsProduct_{timestamp}")
        os.makedirs(result_dir, exist_ok=True)
        
        # Ph√¢n lo·∫°i URLs theo lo·∫°i
        categorized_urls = {
            'category': [],
            'model': [],
            'series': [],
            'unknown': []
        }
        
        for url in urls:
            url_type = self.detect_url_type(url)
            categorized_urls[url_type].append(url)
        
        # Log th·ªëng k√™ URLs
        total_urls = len(urls)
        logger.info(f"=== PH√ÇN LO·∫†I URLs ===")
        logger.info(f"T·ªïng c·ªông: {total_urls} URLs")
        logger.info(f"Category URLs: {len(categorized_urls['category'])}")
        logger.info(f"Model URLs: {len(categorized_urls['model'])}")
        logger.info(f"Series URLs: {len(categorized_urls['series'])}")
        logger.info(f"Unknown URLs: {len(categorized_urls['unknown'])}")
        
        if categorized_urls['unknown']:
            logger.warning(f"Unknown URLs: {categorized_urls['unknown']}")
        
        self.emit_progress(0, f"B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ {total_urls} URLs")
        
        processed_count = 0
        
        # X·ª≠ l√Ω Category URLs
        for i, category_url in enumerate(categorized_urls['category']):
            try:
                progress = (processed_count / total_urls) * 100
                self.emit_progress(progress, f"ƒêang x·ª≠ l√Ω category {i+1}/{len(categorized_urls['category'])}")
                
                # C√†o d·ªØ li·ªáu category
                products_data, category_name = self.crawl_category(category_url)
                
                if products_data:
                    self._save_products_data(products_data, category_name, result_dir)
                    self.stats["categories_processed"] += 1
                
                processed_count += 1
                
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω category {category_url}: {str(e)}")
                processed_count += 1
        
        # X·ª≠ l√Ω Model URLs
        model_urls = categorized_urls['model']
        
        if len(model_urls) >= 2:
            # X·ª≠ l√Ω nhi·ªÅu model URLs - g·ªôp v√†o 1 folder chung
            try:
                progress = (processed_count / total_urls) * 100
                self.emit_progress(progress, f"ƒêang x·ª≠ l√Ω {len(model_urls)} models (g·ªôp chung)")
                
                # C√†o t·∫•t c·∫£ models v√† g·ªôp v√†o 1 folder
                all_products_data, folder_name = self.crawl_multiple_models(model_urls)
                
                if all_products_data:
                    self._save_products_data(all_products_data, folder_name, result_dir)
                    self.stats["categories_processed"] += 1  # Treat as 1 category for stats
                    logger.info(f"‚úÖ ƒê√£ g·ªôp {len(all_products_data)} models v√†o folder: {folder_name}")
                
                # Update processed count for all model URLs
                processed_count += len(model_urls)
                
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω multiple models: {str(e)}")
                processed_count += len(model_urls)
        
        else:
            # X·ª≠ l√Ω t·ª´ng model URL ri√™ng l·∫ª (logic c≈© cho single model)
            for i, model_url in enumerate(model_urls):
                try:
                    progress = (processed_count / total_urls) * 100
                    self.emit_progress(progress, f"ƒêang x·ª≠ l√Ω model {i+1}/{len(model_urls)}")
                    
                    # C√†o d·ªØ li·ªáu single model
                    products_data, category_name = self.crawl_single_model(model_url)
                    
                    if products_data:
                        self._save_products_data(products_data, category_name, result_dir)
                        self.stats["categories_processed"] += 1  # Treat as a category for stats
                    
                    processed_count += 1
                    
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω model {model_url}: {str(e)}")
                    processed_count += 1
        
        # X·ª≠ l√Ω Series URLs (c√≥ th·ªÉ th√™m logic sau)
        for i, series_url in enumerate(categorized_urls['series']):
            try:
                progress = (processed_count / total_urls) * 100
                self.emit_progress(progress, f"ƒêang x·ª≠ l√Ω series {i+1}/{len(categorized_urls['series'])}")
                
                logger.info(f"Series URL ƒë∆∞·ª£c detect: {series_url}")
                logger.info("Ch∆∞a implement x·ª≠ l√Ω series URL, b·ªè qua...")
                
                processed_count += 1
                
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω series {series_url}: {str(e)}")
                processed_count += 1
        
        # Ho√†n th√†nh
        end_time = time.time()
        duration = end_time - start_time
        
        self.emit_progress(100, f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {processed_count}/{total_urls} URLs")
        
        # Log th·ªëng k√™ cu·ªëi c√πng
        logger.info("=== TH·ªêNG K√ä CRAWLER AUTONICS ===")
        logger.info(f"Th·ªùi gian th·ª±c hi·ªán: {duration:.2f} gi√¢y")
        logger.info(f"T·ªïng URLs ƒë∆∞·ª£c x·ª≠ l√Ω: {processed_count}/{total_urls}")
        logger.info(f"Categories/Models ƒë√£ x·ª≠ l√Ω: {self.stats['categories_processed']}")
        logger.info(f"Series t√¨m th·∫•y: {self.stats['series_found']}")
        logger.info(f"S·∫£n ph·∫©m t√¨m th·∫•y: {self.stats['products_found']}")
        logger.info(f"S·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω: {self.stats['products_processed']}")
        logger.info(f"·∫¢nh ƒë√£ t·∫£i: {self.stats['images_downloaded']}")
        logger.info(f"Request th·∫•t b·∫°i: {self.stats['failed_requests']}")
        logger.info(f"·∫¢nh th·∫•t b·∫°i: {self.stats['failed_images']}")
        
        return result_dir
    
    def _save_products_data(self, products_data, category_name, result_dir):
        """
        L∆∞u d·ªØ li·ªáu s·∫£n ph·∫©m v√†o th∆∞ m·ª•c k·∫øt qu·∫£
        
        Args:
            products_data: Danh s√°ch d·ªØ li·ªáu s·∫£n ph·∫©m
            category_name: T√™n category/folder
            result_dir: Th∆∞ m·ª•c k·∫øt qu·∫£ g·ªëc
        """
        try:
            # T·∫°o th∆∞ m·ª•c cho category/model
            category_dir = os.path.join(result_dir, category_name)
            images_dir = os.path.join(category_dir, "images")
            os.makedirs(category_dir, exist_ok=True)
            os.makedirs(images_dir, exist_ok=True)
            
            # T·∫£i ·∫£nh v·ªõi ƒëa lu·ªìng
            logger.info(f"ƒêang t·∫£i ·∫£nh cho {category_name}...")
            
            def download_image(product):
                if product.get('image_url') and product.get('product_code'):
                    return self.download_and_process_image(
                        product['image_url'],
                        images_dir,
                        product['product_code']
                    )
                return False
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                image_futures = [executor.submit(download_image, product) for product in products_data]
                concurrent.futures.wait(image_futures)
            
            # T·∫°o file Excel
            excel_path = os.path.join(category_dir, f"{category_name}.xlsx")
            self.create_excel_with_specifications(products_data, excel_path)
            
            logger.info(f"ƒê√£ l∆∞u {len(products_data)} s·∫£n ph·∫©m v√†o {category_dir}")
            
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u d·ªØ li·ªáu cho {category_name}: {str(e)}")

if __name__ == "__main__":
    # Test crawler v·ªõi category v√† multiple model URLs
    crawler = AutonicsCrawler()
    
    # Test URLs bao g·ªìm category v√† multiple models
    test_urls = [
        "https://www.autonics.com/vn/product/category/Photoelectric",  # Category URL
        "https://www.autonics.com/vn/model/BYS500-TDT1,2",             # Model URL 1
        "https://www.autonics.com/vn/model/BYS500-TDT3,4"              # Model URL 2 (example)
    ]
    
    print("=== TEST AUTONICS CRAWLER WITH MULTIPLE MODELS ===")
    print(f"Testing v·ªõi {len(test_urls)} URLs:")
    for i, url in enumerate(test_urls, 1):
        print(f"{i}. {url}")
    
    print("\nüìã Expected behavior:")
    print("- Category URL: T·∫°o folder ri√™ng")
    print("- Multiple Model URLs: G·ªôp v√†o 1 folder chung v·ªõi 1 file Excel")
    
    result_dir = crawler.crawl_products(test_urls)
    print(f"\n‚úÖ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {result_dir}")
    print("\n=== HO√ÄN TH√ÄNH TEST ===")